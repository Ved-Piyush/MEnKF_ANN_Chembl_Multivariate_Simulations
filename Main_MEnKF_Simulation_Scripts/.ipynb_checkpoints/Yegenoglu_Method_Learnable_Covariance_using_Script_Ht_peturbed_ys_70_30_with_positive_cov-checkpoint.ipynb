{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "aa53e19a-64db-4b1d-9378-c19c8efacd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import block_diag\n",
    "import warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a6028973-9799-4d8a-b919-a8dd6283faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d459b853-f520-4181-a9a9-310a6dd20de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_with_weights(batch_data, initial_ensembles, size_ens): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    # weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    # h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2a66ee4c-4112-4dca-a9cc-68a60d4e316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(hidden = 32, input_shape = 256, output_shape = 1): \n",
    "    input_layer = tf.keras.layers.Input(shape = (input_shape))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(output_shape, activation = \"relu\")\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e0b9ef03-af61-4ce4-99ff-2f0b5dca639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_ensembles(num_weights, lambda1, size_ens):\n",
    "    mean_vec = np.zeros((num_weights,))\n",
    "    cov_matrix = lambda1*np.identity(num_weights)\n",
    "    mvn_samp = mvn(mean_vec, cov_matrix)\n",
    "    return mvn_samp.rvs(size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fe2ea2b5-13a4-41c4-8257-c6c18708501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expit(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "758ac30a-bcc7-4b5e-9314-7fb85f284f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann =  ann(hidden = 16, input_shape = 32, output_shape = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2548f2de-a4c3-4850-b5cc-439634f5c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ann_1 = samp_ann.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1338dc89-5540-4867-95a6-3d0ea7639cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1  = samp_ann.layers[1].output.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "427b0370-90df-4ab4-85b9-691d082750bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_ann.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "06982568-e5c1-4712-895c-7b3d49a09040",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a71c4646-23a6-45ca-9ccc-aa4c3566d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann_params = samp_ann.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b5f741d7-ba99-4803-8441-5bf2ad3d72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_X_t(data1, data2, size_ens, var_weights = 1.0, var_weight_weights = 4.0, var_L = 1.0, var_D = 1.0):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    \n",
    "    initial_ensembles1 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data1_out1, data1_stack1 = get_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles2 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data1_out2, data1_stack2 = get_targets_with_weights(data1, initial_ensembles2, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles3 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data2_out1, data2_stack1 = get_targets_with_weights(data2, initial_ensembles3, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles4 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data2_out2, data2_stack2 = get_targets_with_weights(data2, initial_ensembles4, size_ens = size_ens)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles_for_weights = generate_initial_ensembles(4, var_weight_weights, size_ens)\n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_L = generate_initial_ensembles(4, var_L, size_ens)\n",
    "    initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)    \n",
    "    \n",
    "    initial_ensembles_for_D1 = generate_initial_ensembles(1, var_D, size_ens).reshape(-1,1)\n",
    "    initial_ensembles_for_D2 = generate_initial_ensembles(1, var_D, size_ens).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D1_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    initial_ensembles_for_D2_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.concatenate((np.expand_dims(initial_ensembles_for_D1,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D1_zero,1), \n",
    "                                                      np.expand_dims(initial_ensembles_for_D2,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D2_zero,1)), axis = 2)\n",
    "    \n",
    "    # print(X_t.shape, initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4))\n",
    "    \n",
    "    return X_t, initial_ensembles, initial_ensembles_for_weights[:,0,:], initial_ensembles_for_L[:,0,:], initial_ensembles_for_D[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6ac49c8c-6da0-4ee2-9561-e1d19365f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_targets_with_weights(batch_data, initial_ensembles, size_ens, weights): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    final_output_1 = final_output_1*weights\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b697b7f9-3d50-4d71-a436-cdc6434909a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_targets = pickle.load( open('..//Data//target_scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9ace8608-8da1-4b4e-b051-a7b49feae636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_t = np.array([[0.02, 0], [0, 0.02]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d156603a-c127-45b6-a74d-2735987d03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var1 = R_t[0,0]\n",
    "# var2 = R_t[1,1]\n",
    "# cov = R_t[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c5a2f9ca-f4b4-445a-ab5f-e0f0557f54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "44ed09c8-f6b0-4b40-86e0-1975541fd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fudging_beta = beta(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9d1a4f5e-7a0d-42b7-ad45-59c708b7b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation(data1, data2, combined_ensembles , size_ens, fudging_beta):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4 + 4):(4*params + 4 + 4 + 4)]\n",
    "    \n",
    "    \n",
    "    softmax_weights = tf.math.softmax(initial_ensembles_for_weights).numpy()\n",
    "    \n",
    "    model_1 = softmax_weights[:, :2].sum(1).reshape(-1,1) +  fudging_beta.rvs(size_ens).reshape(-1,1)\n",
    "    \n",
    "    # model_1 = np.min(model_1 -fudging_factor)\n",
    "    \n",
    "    model_2 = softmax_weights[:, 2:].sum(1).reshape(-1,1) +  fudging_beta.rvs(size_ens).reshape(-1,1)\n",
    "    \n",
    "    \n",
    "    model_1_plus_model_2 = model_1 + model_2\n",
    "    \n",
    "    model_1 = model_1/model_1_plus_model_2\n",
    "    \n",
    "    model_2 = model_2/model_1_plus_model_2\n",
    "    \n",
    "    \n",
    "    # print(np.mean(model_1 + model_2))\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                  weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data1, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                weights=model_1)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data2, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 weights=model_2)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                  weights=model_2)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    weighted_alogp = data1_out1 + data2_out1\n",
    "    \n",
    "    weighted_psa = data1_out2 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles, weighted_alogp, weighted_psa, model_1, model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c6bd2eb3-86b7-41ed-81b4-4dd8e44e106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation_test(data1, data2, combined_ensembles , size_ens):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4 + 4):(4*params + 4 + 4 + 4)]\n",
    "    \n",
    "    \n",
    "    softmax_weights = tf.math.softmax(initial_ensembles_for_weights).numpy()\n",
    "    \n",
    "    model_1 = softmax_weights[:, :2].sum(1).reshape(-1,1) \n",
    "    \n",
    "    # model_1 = np.min(model_1 -fudging_factor)\n",
    "    \n",
    "    model_2 = softmax_weights[:, 2:].sum(1).reshape(-1,1) \n",
    "    \n",
    "    \n",
    "#     model_1_plus_model_2 = model_1 + model_2\n",
    "    \n",
    "#     model_1 = model_1/model_1_plus_model_2\n",
    "    \n",
    "#     model_2 = model_2/model_1_plus_model_2\n",
    "    \n",
    "    \n",
    "    # print(np.mean(model_1 + model_2))\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                  weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data1, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                weights=model_1)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data2, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 weights=model_2)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                  weights=model_2)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    weighted_alogp = data1_out1 + data2_out1\n",
    "    \n",
    "    weighted_psa = data1_out2 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles, weighted_alogp, weighted_psa, model_1, model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e15b0bf7-ba00-40bc-933b-c1c3e062e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_ann =  ann(hidden = 16, input_shape = 32, output_shape = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "af3dd570-7ab3-4eae-bd8e-9a57a333b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = 4*(samp_ann.count_params() + 1 + 1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e820b010-4396-4b7a-8781-a8e6d503aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e5fbaaf7-afae-4008-a26c-e0691ef9b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens = total_weights//reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a5111536-1a97-4390-9658-c9848a136a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3f598992-73d6-4b5a-906d-3184f1b9625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_t = [[1, 0, 1, 0], [0, 1, 0, 1]]\n",
    "G_t = np.array(G_t).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a84cb0e3-2f5e-4207-b092-d2768b52205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data1, data2, initial_ensembles, fudging_beta  =fudging_beta): \n",
    "    _,_, weighted_alogp, weighted_psa, w1, w2 = forward_operation(data1, data2, initial_ensembles, size_ens = size_ens, fudging_beta = fudging_beta)\n",
    "    weighted_alogp = np.expand_dims(weighted_alogp,-1)\n",
    "    weighted_psa = np.expand_dims(weighted_psa,-1)\n",
    "    preds = np.concatenate((weighted_alogp, weighted_psa),-1)\n",
    "    return preds, w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "37b60b37-92b1-4784-85fc-59a865d2398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_test(data1, data2, initial_ensembles): \n",
    "    _,_, weighted_alogp, weighted_psa, w1, w2 = forward_operation_test(data1, data2, initial_ensembles, size_ens = size_ens)\n",
    "    weighted_alogp = np.expand_dims(weighted_alogp,-1)\n",
    "    weighted_psa = np.expand_dims(weighted_psa,-1)\n",
    "    preds = np.concatenate((weighted_alogp, weighted_psa),-1)\n",
    "    return preds, w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3af0a9b9-6202-40e2-911f-3c890213099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mu_bar_G_bar(data1, data2, initial_ensembles, fudging_beta):\n",
    "    H_t = np.hstack((np.identity(data1.shape[0]), np.zeros((data1.shape[0], samp_ann_params + 1 + 1 + 1))))\n",
    "    mu_bar = initial_ensembles.mean(0)\n",
    "    X_t,_, _, _, _, _ = forward_operation(data1, data2, initial_ensembles, size_ens = size_ens, fudging_beta = fudging_beta)\n",
    "    X_t = X_t.transpose((0,2,1))\n",
    "    X_t = X_t.reshape(X_t.shape[0], X_t.shape[1]*X_t.shape[2])\n",
    "    script_H_t = np.kron(G_t.T, H_t)\n",
    "    G_u = (script_H_t@X_t.T)\n",
    "    G_u = G_u.T\n",
    "    G_bar = (G_u.mean(0)).ravel()\n",
    "    return mu_bar.reshape(-1,1), G_bar.reshape(-1,1), G_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7334c80c-cfcd-4d5b-b844-11cfb8c137df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u): \n",
    "    u_j_minus_u_bar = initial_ensembles - mu_bar.reshape(1,-1)\n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    c = np.zeros((total_weights, G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        c += np.kron(u_j_minus_u_bar[i, :].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return c/size_ens, G_u_minus_G_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9148779c-1a3f-455d-a6ad-00fc1bf69c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_D_u( G_bar, G_u): \n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    d = np.zeros((G_bar.shape[0], G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        d += np.kron(G_u_minus_G_bar[i,:].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return d/size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6e8459b0-68c1-4d1f-851e-3d7b99f0ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_ensemble(data1, data2, initial_ensembles, y_train, size_ens = size_ens, inflation_factor = 1.0, fudging_beta = fudging_beta):\n",
    "    mu_bar, G_bar, G_u = calculate_mu_bar_G_bar(data1, data2, initial_ensembles, fudging_beta)\n",
    "    C, G_u_minus_G_bar = calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u)\n",
    "    D = calculate_D_u( G_bar, G_u)\n",
    "    _, R_t = create_cov(data1.shape[0],initial_ensembles)\n",
    "    inflation = np.identity(R_t.shape[0])*inflation_factor\n",
    "    D_plus_cov = D + (R_t *inflation_factor)\n",
    "    D_plus_cov_inv = np.linalg.inv(D_plus_cov)\n",
    "    mid_quant = C@D_plus_cov_inv\n",
    "    noise_vec_mean = np.zeros((R_t.shape[0], ))\n",
    "    noise_mvn = mvn(noise_vec_mean, R_t)\n",
    "    fudging = noise_mvn.rvs(size_ens)\n",
    "    interim = (y_train.T.flatten().reshape(1,-1) + fudging)\n",
    "    right_quant = interim - G_u\n",
    "    mid_times_right = mid_quant@right_quant.T\n",
    "    updated_ensemble = (initial_ensembles + mid_times_right.T)\n",
    "    return updated_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c14b236d-6ebf-40e3-9085-97afd2990e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "346d07bc-5b61-4e75-a816-a6e22ffe9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_D = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "dc3a216a-4dbb-474d-8219-5f6c071250a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(data, idx):\n",
    "    data_cur = data[idx, :, :]\n",
    "    inv_data_cur = std_targets.inverse_transform(data_cur)\n",
    "    return inv_data_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "5abd35fb-ea92-449d-894a-162ed1f4aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "381fc654-8877-41fd-840a-4b35153f923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cov(shape, initial_ensembles):\n",
    "    cov_part = initial_ensembles[:, -8:-4]\n",
    "    cov_part = cov_part.mean(0)\n",
    "    # variances = tf.math.softplus(cov_part[:2]).numpy()\n",
    "    variances = cov_part[:2]\n",
    "    covariances = cov_part[2:]\n",
    "    base_cov = np.identity(target_dim)\n",
    "    base_cov[0,0] = variances[0]\n",
    "    base_cov[1,1] = variances[1]\n",
    "    base_cov[0,1] = covariances[0]\n",
    "    base_cov[1,0] = covariances[1]\n",
    "    \n",
    "    variances1 = tf.math.softplus(initial_ensembles[:, -4:]).numpy()\n",
    "    variances1 = variances1.mean(0)\n",
    "    base_variances = np.identity(target_dim)\n",
    "    base_variances[0,0] = variances1[0]\n",
    "    base_variances[1,1] = variances1[2]\n",
    "    \n",
    "    final = np.linalg.cholesky(base_cov@base_cov.T + base_variances)\n",
    "    cov_mat = final@final.T\n",
    "    cov_mat_final = cov_mat\n",
    "    # cov_mat_final = cov_mat@cov_mat.T\n",
    "    \n",
    "    if is_pos_def(cov_mat_final) != True:\n",
    "        print(\"resulting cov matrix is not positive semi definite\")\n",
    "        pass\n",
    "    \n",
    "    # print(np.linalg.det(cov_mat_final))\n",
    "    \n",
    "    var1 = cov_mat_final[0,0]\n",
    "    var2 = cov_mat_final[1,1]\n",
    "    cov = cov_mat_final[1,0]\n",
    "\n",
    "    n = shape\n",
    "    \n",
    "    ul = var1*np.identity(n)\n",
    "    lr = var2*np.identity(n)\n",
    "    ur = cov*np.identity(n)\n",
    "    ll = ur.T    \n",
    "    \n",
    "    first_row = np.hstack((ul, ur))\n",
    "    second_row = np.hstack((ll, lr))\n",
    "    \n",
    "    R_t = np.vstack((first_row, second_row))\n",
    "    \n",
    "    return cov_mat_final, R_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "30aea5e4-361f-44ae-9ea3-d1d8743e6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ed1f9938-cdc5-400f-8bab-22c180a6d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..//Data//smiles_to_rdkit_70_30_with_cov_positive_0.06_var.pickle\", \"rb\") as f: \n",
    "    catch = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "7c59ea29-6484-40d5-8777-80cf9ca6cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "176445fb-4f30-421a-a710-339955111baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(idx, var_weights = 1.0, var_weight_weights = 4.0, var_L = 1.0, var_D = 1.0): \n",
    "    catch_idx = catch[idx]\n",
    "    x_train, x_valid, y_train, y_valid = catch_idx[0], catch_idx[1], catch_idx[2], catch_idx[3]\n",
    "    y_train_actual, y_train = y_train[:,:2], y_train[:,2:]\n",
    "    y_valid_actual, y_valid = y_valid[:,:2], y_valid[:,2:]\n",
    "    smiles_feats_train = x_train[:, :32]\n",
    "    rdkit_feats_train = x_train[:, 32:]\n",
    "    smiles_feats_valid = x_valid[:, :32]\n",
    "    rdkit_feats_valid = x_valid[:, 32:]\n",
    "\n",
    "    X_t, initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D = get_initial_X_t(smiles_feats_train, rdkit_feats_train, size_ens = size_ens, var_weights = var_weights, var_weight_weights = var_weight_weights, var_L = var_L, var_D = var_D)\n",
    "    initial_ensembles = np.hstack((initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D))\n",
    "    \n",
    "    return smiles_feats_train, rdkit_feats_train, smiles_feats_valid, rdkit_feats_valid, y_train, y_train_actual, y_valid, y_valid_actual, initial_ensembles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8aa85830-62ee-4288-9465-0e86071bbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e5cffacb-8558-45ca-bff9-e803e3fd6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b9769d2d-59c2-43e7-9bdb-44a851d1e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_from_covariance(covariance):\n",
    "    v = np.sqrt(np.diag(covariance))\n",
    "    outer_v = np.outer(v, v)\n",
    "    correlation = covariance / outer_v\n",
    "    correlation[covariance == 0] = 0\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ad80fcc6-72c2-447e-9419-df2684e149e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_r =  correlation_from_covariance(np.array([[ 0.3, -0.06],\n",
    "#          [-0.06,  0.3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "65bf21e9-92c6-4824-93c3-87274b072afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(idx, var_weights = 1.0, var_weight_weights = 1.0, var_L = 1, var_D = 0.01, inflation_factor = 1.6, fudging_beta = beta(1,19)):\n",
    "    # print('var_weights' + str(var_weights))\n",
    "    # print('inflation_factor' + str(inflation_factor))\n",
    "    # print('var_weight_weights' + str(var_weight_weights))\n",
    "    smiles_feats_train, rdkit_feats_train, smiles_feats_valid, rdkit_feats_valid, y_train, y_train_actual, y_valid, y_valid_actual, initial_ensembles  = prepare_data(idx, var_weights = var_weights, var_weight_weights =var_weight_weights, var_L = var_L, var_D = var_D)\n",
    "    # print(R_t.shape)\n",
    "    best_train_width_mean = 100000\n",
    "    # print(catch_covs[idx])\n",
    "    for i in range(0,10000):\n",
    "        # print(i)\n",
    "    \n",
    "        # c = np.zeros((2,2))\n",
    "        initial_ensembles = get_updated_ensemble(smiles_feats_train, rdkit_feats_train, initial_ensembles, y_train, size_ens, inflation_factor = inflation_factor, fudging_beta = fudging_beta)\n",
    "        # print(inflation_factor)\n",
    "        G_u_train, w1, w2 = get_predictions(smiles_feats_train, rdkit_feats_train, initial_ensembles, fudging_beta)\n",
    "\n",
    "        catch = Parallel(n_jobs = 15, verbose = 0)(delayed(inverse_transform)(G_u_train, i)  for i in range(G_u_train.shape[0]))\n",
    "        G_u_train = np.array(catch)\n",
    "    \n",
    "        y_train_cur = std_targets.inverse_transform(y_train_actual)\n",
    "    \n",
    "        li_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[0,:,:]   \n",
    "        ui_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "    \n",
    "        width_train = ui_train - li_train\n",
    "        avg_width_train = width_train.mean(0)\n",
    "    \n",
    "        ind_train = (y_train_cur >= li_train) & (y_train_cur <= ui_train)\n",
    "        coverage_train= ind_train.mean(0)\n",
    "    \n",
    "        averaged_targets_train = G_u_train.mean(0)\n",
    "        rmse_train = np.sqrt(((y_train_cur -averaged_targets_train)**2).mean(0))\n",
    "    # print(rmse_train, coverage_train, avg_width_train)\n",
    "    \n",
    "        G_u_test, _, _ = get_predictions_test(smiles_feats_valid, rdkit_feats_valid, initial_ensembles)\n",
    "    \n",
    "        catch = Parallel(n_jobs = 15, verbose = 0)(delayed(inverse_transform)(G_u_test, i)  for i in range(G_u_test.shape[0]))\n",
    "        G_u_test = np.array(catch)\n",
    "    \n",
    "        y_valid_cur = std_targets.inverse_transform(y_valid_actual)    \n",
    "    \n",
    "        li_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[0,:,:]   \n",
    "        ui_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "    \n",
    "        width_test = ui_test - li_test\n",
    "        avg_width_test = width_test.mean(0)\n",
    "    \n",
    "        ind_test = (y_valid_cur >= li_test) & (y_valid_cur <= ui_test)\n",
    "        coverage_test= ind_test.mean(0)\n",
    "    \n",
    "        averaged_targets_test = G_u_test.mean(0)\n",
    "        rmse_test = np.sqrt(((y_valid_cur -averaged_targets_test)**2).mean(0))    \n",
    "    \n",
    "        # weight_norms = np.array(norm(initial_ensembles, ord = 2, axis = 1))\n",
    "        # weight_norm_mean.append(weight_norms.mean())\n",
    "        # weight_norm_sd.append(weight_norms.std())\n",
    "    \n",
    "        cov_mat_final, _ = create_cov(smiles_feats_train.shape[0],initial_ensembles)\n",
    "        \n",
    "#         for i in range(0, size_ens):\n",
    "#             c+= np.cov(G_u_train[i,:,:].T)\n",
    "        \n",
    "#         c = c/size_ens    \n",
    "        \n",
    "        # print(cov_mat_final)\n",
    "\n",
    "        est_r =  correlation_from_covariance(cov_mat_final)\n",
    "        \n",
    "#         cmd = 1 - ((np.trace(act_r@est_r))/(np.linalg.norm(act_r, \"fro\")*np.linalg.norm(est_r, \"fro\")))\n",
    "        \n",
    "#         print(cmd)\n",
    "        # print(\"standardized_scale_R_t\")\n",
    "        # print(np.diag(cov_mat_final), cov_mat_final[0,1])\n",
    "        \n",
    "        # print(w1.shape)\n",
    "        \n",
    "        li_smiles_weight = np.percentile(w1, axis = 0, q = (2.5, 97.5))[0][0]\n",
    "        \n",
    "        # print(np.percentile(w1, axis = 0, q = (2.5, 97.5)))\n",
    "        \n",
    "        ui_smiles_weight = np.percentile(w1, axis = 0, q = (2.5, 97.5))[1][0]      \n",
    "        \n",
    "        # print(coverage_train.tolist(), avg_width_train.tolist(), rmse_train.tolist())\n",
    "        # print(coverage_test.tolist(), avg_width_test.tolist(), rmse_test.tolist())\n",
    "        # print(w1.mean(), w1.std())\n",
    "        # print(li_smiles_weight, ui_smiles_weight)\n",
    "        # print(avg_width_train.tolist(), coverage_train.tolist(), rmse_train.tolist(), avg_width_test.tolist(), coverage_test.tolist(), rmse_test.tolist(), w1.mean())\n",
    "\n",
    "        if (avg_width_train.mean() < best_train_width_mean) & (coverage_train.mean() > 0.95): \n",
    "            # print(\"went here\")\n",
    "            best_train_width_mean = avg_width_train.mean()\n",
    "            best_train_width = avg_width_train\n",
    "            best_smiles_weight = w1.mean()\n",
    "            best_coverage_train = coverage_train\n",
    "            best_rmse_train = rmse_train\n",
    "        \n",
    "            best_test_width = avg_width_test\n",
    "\n",
    "            best_coverage_test = coverage_test    \n",
    "            best_rmse_test = rmse_test\n",
    "            \n",
    "            best_li_smiles_weight = li_smiles_weight\n",
    "            \n",
    "            best_ui_smiles_weight = ui_smiles_weight\n",
    "            \n",
    "            # best_cmd = cmd\n",
    "    \n",
    "        if coverage_train.mean() < 0.95:\n",
    "            \n",
    "            # print()\n",
    "            # print(best_train_width.tolist(), best_coverage_train.tolist(), best_rmse_train.tolist(), best_test_width.tolist(), best_coverage_test.tolist(), best_rmse_test.tolist(), best_smiles_weight, flush = True)\n",
    "            print(\"done for fold\" + str(idx), flush = True)\n",
    "            print(\"train_width\" + str(best_train_width.tolist()), flush = True)\n",
    "            print(\"test_width\" + str(best_test_width.tolist()), flush = True)\n",
    "            print(\"smiles_weight\" + str(best_smiles_weight), flush = True)\n",
    "            print(\"rmse_train\" + str(best_rmse_train.tolist()), flush = True)\n",
    "            print(\"rmse_test\" + str(best_rmse_test.tolist()), flush = True)\n",
    "            print(\"smiles_weight_ci\" + str([best_li_smiles_weight, best_ui_smiles_weight]), flush = True)\n",
    "            \n",
    "            return [best_train_width.tolist(), best_coverage_train.tolist(), best_rmse_train.tolist(), best_test_width.tolist(), best_coverage_test.tolist(), best_rmse_test.tolist(), best_smiles_weight, best_li_smiles_weight, best_ui_smiles_weight]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "201d8498-7742-4509-94de-4fb1f5d84e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df[results_df[\"indicator\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "14658528-ea1a-4a6f-83bd-f708df0bcd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 5 µs, total: 12 µs\n",
      "Wall time: 20.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get_results(8, var_weights = 1.0, var_weight_weights = 3.0, var_L = 1, var_D = 1,inflation_factor =1.0, fudging_beta = beta(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "3ac41ba9-2b9e-4b10-b6ad-e415030e40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c0033c73-c13a-492a-8caa-fb16a2d51405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for fold10\n",
      "train_width[1.4128884086215527, 27.837551341811984]\n",
      "test_width[1.0071813304246933, 22.16484790057238]\n",
      "smiles_weight0.7723682013091414\n",
      "rmse_train[0.32067513989641816, 6.223502986840081]\n",
      "rmse_test[0.22217164298186193, 4.807003725817127]\n",
      "smiles_weight_ci[0.650933481582132, 0.8634139814007975]\n",
      "done for fold0\n",
      "train_width[1.4532285160520548, 26.14009611756448]\n",
      "test_width[0.9454037959225006, 20.24900379489354]\n",
      "smiles_weight0.80308238584728\n",
      "rmse_train[0.27607533671466256, 6.195405288201823]\n",
      "rmse_test[0.28410976856421816, 5.140542227980579]\n",
      "smiles_weight_ci[0.6744566505766867, 0.8789309840467296]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done   2 tasks      | elapsed:  2.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for fold3\n",
      "train_width[1.3676665683608449, 26.63719788459868]\n",
      "test_width[0.9511805808424341, 19.963684273790587]\n",
      "smiles_weight0.7815073984142801\n",
      "rmse_train[0.2558628286624792, 5.569755879979396]\n",
      "rmse_test[0.21961501147479792, 4.910943506804941]\n",
      "smiles_weight_ci[0.6580540446133515, 0.8604445857743482]\n",
      "done for fold9\n",
      "train_width[1.2027850486594756, 22.646623430196794]\n",
      "test_width[0.752383657246027, 17.51931396826855]\n",
      "smiles_weight0.7877034052125408\n",
      "rmse_train[0.21966444601015228, 5.417120165741091]\n",
      "rmse_test[0.2147634110632464, 4.8012896678043475]\n",
      "smiles_weight_ci[0.6522220039358081, 0.8567070664947736]\n",
      "done for fold6\n",
      "train_width[1.006266948655323, 22.478568961008445]\n",
      "test_width[0.8626857154187574, 18.839089877670194]\n",
      "smiles_weight0.7060039856146246\n",
      "rmse_train[0.2317343028344444, 5.224551866019728]\n",
      "rmse_test[0.2629323884300418, 4.9119654150904735]\n",
      "smiles_weight_ci[0.5958181922160475, 0.791071903359581]\n",
      "done for fold13\n",
      "train_width[1.3465299287975658, 27.61918653744637]\n",
      "test_width[1.0403428737074805, 23.662280879215153]\n",
      "smiles_weight0.7257397741316625\n",
      "rmse_train[0.23118373921244634, 6.289796883673543]\n",
      "rmse_test[0.2632134393074161, 6.348889726692881]\n",
      "smiles_weight_ci[0.6081900895738864, 0.8192625678016666]\n",
      "done for fold2\n",
      "train_width[1.2389660595458944, 26.121050201187156]\n",
      "test_width[0.8883946008726006, 20.196148103176643]\n",
      "smiles_weight0.7544457942315791\n",
      "rmse_train[0.2195919393714984, 4.965634123413664]\n",
      "rmse_test[0.24317446363900558, 5.5607838176283275]\n",
      "smiles_weight_ci[0.6383487733510387, 0.8509261372999342]\n",
      "done for fold7\n",
      "train_width[1.123745812265371, 20.002127611452124]\n",
      "test_width[0.8556420109331984, 16.6017147270044]\n",
      "smiles_weight0.7297842614030601\n",
      "rmse_train[0.24409273976536885, 4.624084053010224]\n",
      "rmse_test[0.2484862011729457, 3.7913515228259675]\n",
      "smiles_weight_ci[0.6074162601807737, 0.8158738214232504]\n",
      "done for fold8\n",
      "train_width[0.975761678287084, 18.49990303458495]\n",
      "test_width[0.752341647262338, 15.015488650759234]\n",
      "smiles_weight0.7668945617304174\n",
      "rmse_train[0.18707596831722717, 4.101613153065709]\n",
      "rmse_test[0.2626601124670024, 3.2111092808401267]\n",
      "smiles_weight_ci[0.6447789702085215, 0.8523101670643459]\n",
      "done for fold11\n",
      "train_width[0.9557331036344682, 21.56092066991478]\n",
      "test_width[0.6484262103409554, 17.477373524381218]\n",
      "smiles_weight0.7707251069317145\n",
      "rmse_train[0.22081018444265077, 3.818980030562306]\n",
      "rmse_test[0.20638625536400532, 3.3511062522196027]\n",
      "smiles_weight_ci[0.6392245644630081, 0.8427050046122894]\n",
      "done for fold1\n",
      "train_width[1.4154393777363128, 21.21600846458746]\n",
      "test_width[0.7003452493561348, 13.839312035226733]\n",
      "smiles_weight0.7990393033486154\n",
      "rmse_train[0.24846596485669148, 4.2934138078609365]\n",
      "rmse_test[0.33323055098226195, 2.8932346917219127]\n",
      "smiles_weight_ci[0.6619736604283633, 0.8692165770377741]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  11 tasks      | elapsed:  3.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for fold12\n",
      "train_width[0.9662663945894953, 19.192217919673542]\n",
      "test_width[0.7776493009781346, 17.57058632018357]\n",
      "smiles_weight0.7721041836988365\n",
      "rmse_train[0.22111087040905425, 4.4777397920834145]\n",
      "rmse_test[0.3098681406688088, 7.3941804747694615]\n",
      "smiles_weight_ci[0.6697497365599361, 0.8399357888550804]\n",
      "done for fold14\n",
      "train_width[1.0453385207715948, 17.146780106958822]\n",
      "test_width[0.608058246721897, 13.700805290938678]\n",
      "smiles_weight0.831155876375866\n",
      "rmse_train[0.19431757860926718, 4.206728414501924]\n",
      "rmse_test[0.3061547544942887, 9.572838611765162]\n",
      "smiles_weight_ci[0.721941535802782, 0.9017753681805732]\n",
      "done for fold4\n",
      "train_width[0.7726396170027985, 17.08725693133859]\n",
      "test_width[0.5952960895171812, 13.520351724746693]\n",
      "smiles_weight0.7179284030771397\n",
      "rmse_train[0.17418950151443163, 4.387329971488003]\n",
      "rmse_test[0.22244522946919668, 3.437424407308623]\n",
      "smiles_weight_ci[0.6167924513322154, 0.7910921250613183]\n",
      "done for fold5\n",
      "train_width[0.8789680495665342, 15.004654879061796]\n",
      "test_width[0.47571241463331915, 10.322889681238605]\n",
      "smiles_weight0.8096070653389381\n",
      "rmse_train[0.1780466589722459, 3.9499846343873397]\n",
      "rmse_test[0.1632823694414723, 4.573726767545997]\n",
      "smiles_weight_ci[0.6857810308350554, 0.8790648018142287]\n",
      "done for fold22\n",
      "train_width[1.2427452921095175, 26.17143898099876]\n",
      "test_width[1.040505245433354, 23.22400863262399]\n",
      "smiles_weight0.7590510491195289\n",
      "rmse_train[0.2501076927779643, 6.214862354033995]\n",
      "rmse_test[0.23728039400878878, 6.296518738222999]\n",
      "smiles_weight_ci[0.6319007338372479, 0.835994840721777]\n",
      "done for fold18\n",
      "train_width[1.3253125009066937, 22.967921042285077]\n",
      "test_width[0.7977376882924374, 17.731541651836388]\n",
      "smiles_weight0.7874123142869603\n",
      "rmse_train[0.22963519692404985, 5.6958106204376895]\n",
      "rmse_test[0.222491672075387, 5.230673481619615]\n",
      "smiles_weight_ci[0.653143289941344, 0.8708510082966093]\n",
      "done for fold23\n",
      "train_width[1.3387784824035847, 25.60211121698401]\n",
      "test_width[1.21215837542081, 24.22140890672476]\n",
      "smiles_weight0.7613077630956702\n",
      "rmse_train[0.3157223532153202, 6.395102925482343]\n",
      "rmse_test[0.3261232459824715, 8.370470442763327]\n",
      "smiles_weight_ci[0.6384667699669103, 0.8386836437601867]\n",
      "done for fold20\n",
      "train_width[1.4462602332086105, 25.16731327389089]\n",
      "test_width[1.0374582420662257, 20.409581654449934]\n",
      "smiles_weight0.7485142019661536\n",
      "rmse_train[0.2308951541966383, 5.989914599090443]\n",
      "rmse_test[0.2401002283118323, 5.190335211050185]\n",
      "smiles_weight_ci[0.6321409878076976, 0.8235270323251985]\n",
      "done for fold21\n",
      "train_width[1.0615902568046152, 24.31786025864945]\n",
      "test_width[0.9109177398405853, 23.001348786146494]\n",
      "smiles_weight0.7325579704291776\n",
      "rmse_train[0.18429746671617056, 5.187808371673138]\n",
      "rmse_test[0.33727733962479367, 5.2297787414285715]\n",
      "smiles_weight_ci[0.5944704306772994, 0.8104592163704629]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:  6.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for fold17\n",
      "train_width[0.9821836568367542, 20.6273158632761]\n",
      "test_width[0.7186233415562734, 16.9225978947265]\n",
      "smiles_weight0.7808249075867947\n",
      "rmse_train[0.1965968979326156, 4.835772598755975]\n",
      "rmse_test[0.24046665211704213, 4.151596789450487]\n",
      "smiles_weight_ci[0.6607468595466934, 0.8602119706867828]\n",
      "done for fold19\n",
      "train_width[1.0398547942805763, 20.670206903615544]\n",
      "test_width[0.7974006705331689, 16.408456943722047]\n",
      "smiles_weight0.7317188259718522\n",
      "rmse_train[0.23233718060618316, 4.9238742625973675]\n",
      "rmse_test[0.2495433310246332, 3.960810694385954]\n",
      "smiles_weight_ci[0.6170535887798448, 0.813481302966538]\n",
      "done for fold24\n",
      "train_width[1.3267196148263336, 23.38873847300111]\n",
      "test_width[0.9595368057617907, 19.606061683569386]\n",
      "smiles_weight0.698459392045011\n",
      "rmse_train[0.2631997682890417, 5.757205682074332]\n",
      "rmse_test[0.2253481368014154, 5.5439081380389155]\n",
      "smiles_weight_ci[0.5774503101746287, 0.788453213580694]\n",
      "done for fold16\n",
      "train_width[0.9712058295785534, 18.974468583509058]\n",
      "test_width[0.8964344659964887, 17.067066830059478]\n",
      "smiles_weight0.7235154129191147\n",
      "rmse_train[0.18241549825829115, 4.743635742586027]\n",
      "rmse_test[0.2926836691954617, 6.435147582184786]\n",
      "smiles_weight_ci[0.6174536772750365, 0.7974283524636763]\n",
      "done for fold15\n",
      "train_width[0.9445117216949372, 17.15952292772197]\n",
      "test_width[0.6122528220453434, 13.899540450657833]\n",
      "smiles_weight0.7754851555212415\n",
      "rmse_train[0.2107116056804487, 4.0479532032302]\n",
      "rmse_test[0.25257173656794624, 3.5793285644151855]\n",
      "smiles_weight_ci[0.6764739815189915, 0.8584464096624546]\n",
      "done for fold27\n",
      "train_width[1.1003982856310874, 23.462193243110377]\n",
      "test_width[0.8024515344146879, 18.712603146579184]\n",
      "smiles_weight0.7378594491452127\n",
      "rmse_train[0.25440291062836873, 5.468789790230712]\n",
      "rmse_test[0.24166878845386958, 4.549739025542808]\n",
      "smiles_weight_ci[0.6250207400329125, 0.8254091023959091]\n",
      "done for fold25\n",
      "train_width[0.9786507137272317, 17.463927907618704]\n",
      "test_width[0.5889710015441604, 12.222272268576905]\n",
      "smiles_weight0.8361541391172072\n",
      "rmse_train[0.2045167495373018, 4.016922141137969]\n",
      "rmse_test[0.2737924494623092, 4.5240816904097425]\n",
      "smiles_weight_ci[0.7215343263183509, 0.905688207551565]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  27 out of  50 | elapsed:  7.2min remaining:  6.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for fold28\n",
      "train_width[1.170264137458923, 22.749207320721524]\n",
      "test_width[0.8566375405591053, 18.39668107426324]\n",
      "smiles_weight0.7327243728899931\n",
      "rmse_train[0.2583725019021255, 5.324673731848623]\n",
      "rmse_test[0.23782732974566617, 3.8634767284038576]\n",
      "smiles_weight_ci[0.6006931038380536, 0.8224873211145233]\n",
      "done for fold26\n",
      "train_width[0.9335337765297228, 19.98494597529938]\n",
      "test_width[0.6565197755224401, 15.131821780589483]\n",
      "smiles_weight0.7757336550085118\n",
      "rmse_train[0.18369099530996, 4.992124236797703]\n",
      "rmse_test[0.1872645129210738, 4.847558944386612]\n",
      "smiles_weight_ci[0.6447417372201568, 0.8622706819607786]\n",
      "done for fold29\n",
      "train_width[1.3647651274577226, 29.03587932647665]\n",
      "test_width[1.1160681948172875, 24.12819753424169]\n",
      "smiles_weight0.7371438164325085\n",
      "rmse_train[0.24045138204754746, 6.5833239772484795]\n",
      "rmse_test[0.24221232335045317, 6.354710590527126]\n",
      "smiles_weight_ci[0.5966392510556324, 0.8219924706182816]\n",
      "done for fold32\n",
      "train_width[1.320713193937156, 23.5855345660038]\n",
      "test_width[0.7099051091239975, 15.742047938346593]\n",
      "smiles_weight0.8227479143504703\n",
      "rmse_train[0.2732573921521806, 5.058726175766376]\n",
      "rmse_test[0.2159008143070002, 4.703977235173361]\n",
      "smiles_weight_ci[0.6851724845280932, 0.9095699364667587]\n",
      "done for fold33\n",
      "train_width[1.2855669854540916, 22.66522476363733]\n",
      "test_width[0.8782189952811025, 19.03105413722083]\n",
      "smiles_weight0.7737146612922964\n",
      "rmse_train[0.24052645849402937, 5.108131995858927]\n",
      "rmse_test[0.6096594503285145, 7.671987794637084]\n",
      "smiles_weight_ci[0.6501514749896784, 0.8597829811010996]\n",
      "done for fold34\n",
      "train_width[1.019289779691092, 21.06627208170289]\n",
      "test_width[0.6611688302452235, 14.997001280572935]\n",
      "smiles_weight0.801792768966461\n",
      "rmse_train[0.1679159132381261, 5.160941830435545]\n",
      "rmse_test[0.21640739034329706, 4.979700737685211]\n",
      "smiles_weight_ci[0.6701131947980603, 0.8754506031086947]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  33 out of  50 | elapsed:  9.5min remaining:  4.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for fold30\n",
      "train_width[1.1426536149964182, 23.045474336667564]\n",
      "test_width[1.0317048759075185, 21.949871999738818]\n",
      "smiles_weight0.7184788784014874\n",
      "rmse_train[0.2443941874422022, 4.7827532726348965]\n",
      "rmse_test[0.28948766487672073, 15.1788815074027]\n",
      "smiles_weight_ci[0.5930247840778489, 0.8096461925868929]\n",
      "done for fold31\n",
      "train_width[0.9150919977393958, 18.621062041472467]\n",
      "test_width[0.7727576332188499, 17.675540174155426]\n",
      "smiles_weight0.7481813234334773\n",
      "rmse_train[0.23010220502317374, 4.322819109384071]\n",
      "rmse_test[0.39727417665460635, 19.072117870687556]\n",
      "smiles_weight_ci[0.6418472836123166, 0.8232894684731517]\n",
      "done for fold36\n",
      "train_width[0.9860998077252707, 20.523309850964562]\n",
      "test_width[0.7376636224479906, 15.978236867509516]\n",
      "smiles_weight0.7396052445155262\n",
      "rmse_train[0.16953840043959797, 5.380549244469488]\n",
      "rmse_test[0.1809413744502919, 5.269944367671658]\n",
      "smiles_weight_ci[0.6280663026966196, 0.8204062137519315]\n",
      "done for fold35\n",
      "train_width[1.0996018556349, 19.75406572949516]\n",
      "test_width[0.7590878309475755, 15.639723393391114]\n",
      "smiles_weight0.7534075376536432\n",
      "rmse_train[0.24203463268218586, 4.488500236101413]\n",
      "rmse_test[0.1649313233212075, 3.674321676417143]\n",
      "smiles_weight_ci[0.6246457491521326, 0.83015674835461]\n",
      "done for fold39\n",
      "train_width[1.2590550940947407, 23.86409326989816]\n",
      "test_width[0.8399387381635673, 18.802238413665748]\n",
      "smiles_weight0.7789238616536229\n",
      "rmse_train[0.2106151414079358, 5.817210725544899]\n",
      "rmse_test[0.28735879889209504, 4.612695556293286]\n",
      "smiles_weight_ci[0.6434342720384548, 0.8612100033073748]\n",
      "done for fold38\n",
      "train_width[1.1264838902012904, 18.306917137217184]\n",
      "test_width[0.7844273268627193, 14.617212246343401]\n",
      "smiles_weight0.773899976600714\n",
      "rmse_train[0.22431564092159703, 4.451879973799943]\n",
      "rmse_test[0.30422610298282443, 4.463399805328738]\n",
      "smiles_weight_ci[0.6361281633689951, 0.8558214077513296]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  39 out of  50 | elapsed: 10.0min remaining:  2.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for fold37\n",
      "train_width[0.9345620520177979, 16.999176718043046]\n",
      "test_width[0.6224211629376236, 13.861044673723454]\n",
      "smiles_weight0.7570752619251323\n",
      "rmse_train[0.1487953677007251, 4.607312682963291]\n",
      "rmse_test[0.18591624769396542, 3.79116739611155]\n",
      "smiles_weight_ci[0.6539992367891165, 0.8313381904176523]\n",
      "done for fold41\n",
      "train_width[1.1812214160778918, 26.986279957075784]\n",
      "test_width[0.8386747801606842, 19.557136571326804]\n",
      "smiles_weight0.8162341936043718\n",
      "rmse_train[0.22485225909872078, 6.542561423759308]\n",
      "rmse_test[0.6131425057345758, 6.375303587463403]\n",
      "smiles_weight_ci[0.6842452268797483, 0.891381974259475]\n",
      "done for fold40\n",
      "train_width[1.2637447100265193, 19.972846680137316]\n",
      "test_width[0.9096678972914709, 17.22595446331942]\n",
      "smiles_weight0.7349572262935554\n",
      "rmse_train[0.25500404126635784, 4.625297326529168]\n",
      "rmse_test[0.2076445114217504, 4.1766804199498635]\n",
      "smiles_weight_ci[0.6128125599108226, 0.818742579391291]\n",
      "done for fold43\n",
      "train_width[1.2193675694855572, 22.913598876270445]\n",
      "test_width[0.8343648098091205, 16.919723669049006]\n",
      "smiles_weight0.7723324744030023\n",
      "rmse_train[0.2432055765276819, 5.847659381889678]\n",
      "rmse_test[0.2253297524492302, 4.732107281013646]\n",
      "smiles_weight_ci[0.6476863293913419, 0.863400994484269]\n",
      "done for fold42\n",
      "train_width[0.9110399008737919, 19.87244808865932]\n",
      "test_width[0.6623865709246927, 16.1065304523327]\n",
      "smiles_weight0.7210687996735533\n",
      "rmse_train[0.18681014562604834, 4.765342521793477]\n",
      "rmse_test[0.15838924316809072, 4.31760274889067]\n",
      "smiles_weight_ci[0.6269720159452518, 0.8019380200905534]\n",
      "done for fold44\n",
      "train_width[0.8707786515514909, 16.7898076169788]\n",
      "test_width[0.6112012687836647, 13.74066574934204]\n",
      "smiles_weight0.7786204275600538\n",
      "rmse_train[0.1714889838422369, 4.730932348938869]\n",
      "rmse_test[0.1402392986812375, 4.050458227473938]\n",
      "smiles_weight_ci[0.6653833573349941, 0.8490574651727036]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  45 out of  50 | elapsed: 10.8min remaining:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for fold46\n",
      "train_width[1.0724187761349642, 26.338130803324983]\n",
      "test_width[0.8853500929772401, 22.80054257072411]\n",
      "smiles_weight0.728377720786616\n",
      "rmse_train[0.2174998035929417, 5.734078529228298]\n",
      "rmse_test[0.22495055963336347, 9.435571750055413]\n",
      "smiles_weight_ci[0.5983975526347963, 0.8170030919122975]\n",
      "done for fold47\n",
      "train_width[1.3150194986411707, 28.585629252042136]\n",
      "test_width[1.0296442266161994, 23.286238490628445]\n",
      "smiles_weight0.7266018645285566\n",
      "rmse_train[0.24989369293413824, 6.130179854414354]\n",
      "rmse_test[0.25889840053014435, 5.058882787666756]\n",
      "smiles_weight_ci[0.6226921736324247, 0.8090574797608754]\n",
      "done for fold48\n",
      "train_width[1.0823181158750605, 21.53016044947108]\n",
      "test_width[0.8470696467687239, 18.412107131453755]\n",
      "smiles_weight0.6847694636704251\n",
      "rmse_train[0.22677800400489645, 5.1747366493733455]\n",
      "rmse_test[0.1926373573278577, 4.405332848249129]\n",
      "smiles_weight_ci[0.5706393973856966, 0.7748064304775465]\n",
      "done for fold45\n",
      "train_width[0.85447379598567, 17.43152733461018]\n",
      "test_width[0.7143379622751821, 16.33420946397096]\n",
      "smiles_weight0.7649149859853338\n",
      "rmse_train[0.18770489560650946, 4.551345398606275]\n",
      "rmse_test[0.6804886381044468, 15.248926523244059]\n",
      "smiles_weight_ci[0.6576055203703266, 0.8384941527310452]\n",
      "done for fold49\n",
      "train_width[0.9548098193984086, 20.720521171419552]\n",
      "test_width[0.7655321130590054, 16.50583021956388]\n",
      "smiles_weight0.7235161547399628\n",
      "rmse_train[0.17164411363002033, 5.411347689871412]\n",
      "rmse_test[0.18440159370812934, 5.655026089632391]\n",
      "smiles_weight_ci[0.5953346674444548, 0.8102901298595364]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  50 out of  50 | elapsed: 12.1min finished\n"
     ]
    }
   ],
   "source": [
    "catch_all = Parallel(n_jobs = 15, verbose = 10)(delayed(get_results)(idx,var_weights = 1.0, var_weight_weights = 2.0, inflation_factor =1.0, fudging_beta = beta(1,11)) for idx in range(0,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1f857358-e5f0-4d43-bf0a-d4d7d034cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3ee2282b-2100-4ed9-972b-ea78eabe2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_all[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "857dc418-a018-4330-868a-47ae36a9f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "1e456d98-c381-450b-964d-fdbdbd1bd45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_catch = []\n",
    "for item in catch_all:\n",
    "    catch_inner = []\n",
    "    for inner in item:\n",
    "        if type(inner) == list:\n",
    "            for inner1 in inner:\n",
    "                catch_inner.append(inner1)\n",
    "        if type(inner) != list:\n",
    "            catch_inner.append(inner)\n",
    "    all_catch.append(catch_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "b17ec0ba-5448-4130-b54f-c8cf20f76128",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "21f8792e-1721-412d-a84e-39bf7ad3c070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 15)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "610306d1-f54e-42be-86c2-70ef6d4fdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6accc813-8aa7-48a0-abae-f89b6e5f5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.iloc[:,-1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "81b223ac-16c3-4547-bb99-9a2e08bbb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"Alop_Train_Width\", \"PSA_Train_Width\", \"Alop_Train_Coverage\", \"PSA_Train_Coverage\", \n",
    "            \"Alop_Train_RMSE\", \"PSA_Train_RMSE\", \"Alop_Test_Width\", \"PSA_Test_Width\", \"Alop_Test_Coverage\", \"PSA_Test_Coverage\", \n",
    "            \"Alop_Test_RMSE\", \"PSA_Test_RMSE\",\"Smiles_Avg_Weight\", \"Lower_Interval_Smiles_Weight\", \"Upper_Interval_Smiles_Weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1124c374-99bf-4282-9e32-1074a5033ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8e52823b-19de-4bed-ae04-519af1d5021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a4fd350f-4f0a-4f9a-af19-52590d875782",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"indicator\"] = (results_df[\"Lower_Interval_Smiles_Weight\"].values < 0.70) & (results_df[\"Upper_Interval_Smiles_Weight\"].values >= 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "5b5283a6-b5f6-4752-a693-9b1dea85ba9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(results_df[\"indicator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b0ebc74a-b0e3-4cb5-8d3f-b2acb6040353",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"width_weight_CI\"] = results_df[\"Upper_Interval_Smiles_Weight\"].values - results_df[\"Lower_Interval_Smiles_Weight\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f36d6343-cfa2-475d-a8dd-01007676f993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alop_Train_Width</td>\n",
       "      <td>1.124066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PSA_Train_Width</td>\n",
       "      <td>21.930106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alop_Train_Coverage</td>\n",
       "      <td>0.967344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PSA_Train_Coverage</td>\n",
       "      <td>0.955021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alop_Train_RMSE</td>\n",
       "      <td>0.222933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PSA_Train_RMSE</td>\n",
       "      <td>5.132314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alop_Test_Width</td>\n",
       "      <td>0.815205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PSA_Test_Width</td>\n",
       "      <td>17.818180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alop_Test_Coverage</td>\n",
       "      <td>0.913167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PSA_Test_Coverage</td>\n",
       "      <td>0.936083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alop_Test_RMSE</td>\n",
       "      <td>0.266147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PSA_Test_RMSE</td>\n",
       "      <td>5.794172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Smiles_Avg_Weight</td>\n",
       "      <td>0.759315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Lower_Interval_Smiles_Weight</td>\n",
       "      <td>0.638439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Upper_Interval_Smiles_Weight</td>\n",
       "      <td>0.839979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>indicator</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>width_weight_CI</td>\n",
       "      <td>0.201540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           index          0\n",
       "0               Alop_Train_Width   1.124066\n",
       "1                PSA_Train_Width  21.930106\n",
       "2            Alop_Train_Coverage   0.967344\n",
       "3             PSA_Train_Coverage   0.955021\n",
       "4                Alop_Train_RMSE   0.222933\n",
       "5                 PSA_Train_RMSE   5.132314\n",
       "6                Alop_Test_Width   0.815205\n",
       "7                 PSA_Test_Width  17.818180\n",
       "8             Alop_Test_Coverage   0.913167\n",
       "9              PSA_Test_Coverage   0.936083\n",
       "10                Alop_Test_RMSE   0.266147\n",
       "11                 PSA_Test_RMSE   5.794172\n",
       "12             Smiles_Avg_Weight   0.759315\n",
       "13  Lower_Interval_Smiles_Weight   0.638439\n",
       "14  Upper_Interval_Smiles_Weight   0.839979\n",
       "15                     indicator   0.960000\n",
       "16               width_weight_CI   0.201540"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "0636ac66-6077-4a54-9423-542338c64072",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"..//Data//smiles_rdkit_70_30__with_cov_positive_0.06_Simulation_added_beat_noise.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "b6a2dba8-e55c-4931-94e5-b50375e7abf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alop_Train_Width</th>\n",
       "      <th>PSA_Train_Width</th>\n",
       "      <th>Alop_Train_Coverage</th>\n",
       "      <th>PSA_Train_Coverage</th>\n",
       "      <th>Alop_Train_RMSE</th>\n",
       "      <th>PSA_Train_RMSE</th>\n",
       "      <th>Alop_Test_Width</th>\n",
       "      <th>PSA_Test_Width</th>\n",
       "      <th>Alop_Test_Coverage</th>\n",
       "      <th>PSA_Test_Coverage</th>\n",
       "      <th>Alop_Test_RMSE</th>\n",
       "      <th>PSA_Test_RMSE</th>\n",
       "      <th>Smiles_Avg_Weight</th>\n",
       "      <th>Lower_Interval_Smiles_Weight</th>\n",
       "      <th>Upper_Interval_Smiles_Weight</th>\n",
       "      <th>indicator</th>\n",
       "      <th>width_weight_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.045339</td>\n",
       "      <td>17.146780</td>\n",
       "      <td>0.961057</td>\n",
       "      <td>0.942976</td>\n",
       "      <td>0.194318</td>\n",
       "      <td>4.206728</td>\n",
       "      <td>0.608058</td>\n",
       "      <td>13.700805</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.306155</td>\n",
       "      <td>9.572839</td>\n",
       "      <td>0.831156</td>\n",
       "      <td>0.721942</td>\n",
       "      <td>0.901775</td>\n",
       "      <td>False</td>\n",
       "      <td>0.179834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.978651</td>\n",
       "      <td>17.463928</td>\n",
       "      <td>0.938804</td>\n",
       "      <td>0.969402</td>\n",
       "      <td>0.204517</td>\n",
       "      <td>4.016922</td>\n",
       "      <td>0.588971</td>\n",
       "      <td>12.222272</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.273792</td>\n",
       "      <td>4.524082</td>\n",
       "      <td>0.836154</td>\n",
       "      <td>0.721534</td>\n",
       "      <td>0.905688</td>\n",
       "      <td>False</td>\n",
       "      <td>0.184154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alop_Train_Width  PSA_Train_Width  Alop_Train_Coverage   \n",
       "14          1.045339        17.146780             0.961057  \\\n",
       "25          0.978651        17.463928             0.938804   \n",
       "\n",
       "    PSA_Train_Coverage  Alop_Train_RMSE  PSA_Train_RMSE  Alop_Test_Width   \n",
       "14            0.942976         0.194318        4.206728         0.608058  \\\n",
       "25            0.969402         0.204517        4.016922         0.588971   \n",
       "\n",
       "    PSA_Test_Width  Alop_Test_Coverage  PSA_Test_Coverage  Alop_Test_RMSE   \n",
       "14       13.700805            0.612500           0.916667        0.306155  \\\n",
       "25       12.222272            0.733333           0.920833        0.273792   \n",
       "\n",
       "    PSA_Test_RMSE  Smiles_Avg_Weight  Lower_Interval_Smiles_Weight   \n",
       "14       9.572839           0.831156                      0.721942  \\\n",
       "25       4.524082           0.836154                      0.721534   \n",
       "\n",
       "    Upper_Interval_Smiles_Weight  indicator  width_weight_CI  \n",
       "14                      0.901775      False         0.179834  \n",
       "25                      0.905688      False         0.184154  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df[\"indicator\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "747d11c8-4f75-4ec9-aacc-930747d74047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5cb7839f-ac6e-4930-85ec-e58da91ec1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
