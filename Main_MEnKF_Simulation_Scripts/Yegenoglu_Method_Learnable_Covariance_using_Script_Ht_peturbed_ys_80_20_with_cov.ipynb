{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa53e19a-64db-4b1d-9378-c19c8efacd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 17:44:07.450909: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 17:44:07.626096: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-14 17:44:07.630024: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-14 17:44:07.630049: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-14 17:44:08.544180: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-14 17:44:08.544273: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-14 17:44:08.544280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import block_diag\n",
    "import warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6028973-9799-4d8a-b919-a8dd6283faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d459b853-f520-4181-a9a9-310a6dd20de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_with_weights(batch_data, initial_ensembles, size_ens): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    # weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    # h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a66ee4c-4112-4dca-a9cc-68a60d4e316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(hidden = 32, input_shape = 256, output_shape = 1): \n",
    "    input_layer = tf.keras.layers.Input(shape = (input_shape))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(output_shape, activation = \"relu\")\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b9ef03-af61-4ce4-99ff-2f0b5dca639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_ensembles(num_weights, lambda1, size_ens):\n",
    "    mean_vec = np.zeros((num_weights,))\n",
    "    cov_matrix = lambda1*np.identity(num_weights)\n",
    "    mvn_samp = mvn(mean_vec, cov_matrix)\n",
    "    return mvn_samp.rvs(size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe2ea2b5-13a4-41c4-8257-c6c18708501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expit(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758ac30a-bcc7-4b5e-9314-7fb85f284f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 23:08:46.403236: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-13 23:08:46.403256: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-13 23:08:46.403275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c2518.crane.hcc.unl.edu): /proc/driver/nvidia/version does not exist\n",
      "2023-06-13 23:08:46.403943: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "samp_ann =  ann(hidden = 16, input_shape = 32, output_shape = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2548f2de-a4c3-4850-b5cc-439634f5c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ann_1 = samp_ann.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1338dc89-5540-4867-95a6-3d0ea7639cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1  = samp_ann.layers[1].output.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "427b0370-90df-4ab4-85b9-691d082750bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_ann.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06982568-e5c1-4712-895c-7b3d49a09040",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a71c4646-23a6-45ca-9ccc-aa4c3566d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann_params = samp_ann.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "790b6dbd-22de-4e94-9a76-953531ed2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_ensembles_for_weights = generate_initial_ensembles(4, 1, 250)\n",
    "# initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce42026-267e-420d-93ef-e88962e8c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_weights = tf.math.softmax(initial_ensembles_for_weights).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9decedfe-1393-4c7c-989a-966ff3312f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "309700fc-dd6c-4fec-a354-cdc7712cd8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8f7aff4-a7de-4003-b49d-a031c0307b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_weights[:2, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6043df79-7cc6-45fd-8160-683e79e2f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_weights[:,:2].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e7fef7e-9046-49ce-b7bd-e3e82b9c1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_weights[:2,:,:2].sum(-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f09cd707-d205-46ce-b6ac-2ab22b93a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_weights[:2,:,2:].sum(-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c40ee77c-55d5-47a0-a43c-638b89372e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_1 = softmax_weights[:,:2].sum(1).reshape(-1,1)\n",
    "    \n",
    "# model_2 = softmax_weights[:,2:].sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c08f62c-d114-4bee-a335-ba4734f6bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_1 + model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12ba54e5-987a-408a-8a1a-21f7bbde8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_ensembles_for_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5f741d7-ba99-4803-8441-5bf2ad3d72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_X_t(data1, data2, size_ens, var_weights = 1.0, var_weight_weights = 4.0):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    \n",
    "    initial_ensembles1 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data1_out1, data1_stack1 = get_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles2 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data1_out2, data1_stack2 = get_targets_with_weights(data1, initial_ensembles2, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles3 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data2_out1, data2_stack1 = get_targets_with_weights(data2, initial_ensembles3, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles4 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data2_out2, data2_stack2 = get_targets_with_weights(data2, initial_ensembles4, size_ens = size_ens)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles_for_weights = generate_initial_ensembles(4, var_weight_weights, size_ens)\n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_L = generate_initial_ensembles(4, var_weights, size_ens)\n",
    "    initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)    \n",
    "    \n",
    "    initial_ensembles_for_D1 = generate_initial_ensembles(1, var_weights, size_ens).reshape(-1,1)\n",
    "    initial_ensembles_for_D2 = generate_initial_ensembles(1, var_weights, size_ens).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D1_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    initial_ensembles_for_D2_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.concatenate((np.expand_dims(initial_ensembles_for_D1,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D1_zero,1), \n",
    "                                                      np.expand_dims(initial_ensembles_for_D2,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D2_zero,1)), axis = 2)\n",
    "    \n",
    "    # print(X_t.shape, initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4))\n",
    "    \n",
    "    return X_t, initial_ensembles, initial_ensembles_for_weights[:,0,:], initial_ensembles_for_L[:,0,:], initial_ensembles_for_D[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ac49c8c-6da0-4ee2-9561-e1d19365f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_targets_with_weights(batch_data, initial_ensembles, size_ens, weights): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    # weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    # h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    final_output_1 = final_output_1*weights\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b697b7f9-3d50-4d71-a436-cdc6434909a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_targets = pickle.load( open('..//Data//target_scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ace8608-8da1-4b4e-b051-a7b49feae636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_t = np.array([[0.02, 0], [0, 0.02]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d156603a-c127-45b6-a74d-2735987d03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var1 = R_t[0,0]\n",
    "# var2 = R_t[1,1]\n",
    "# cov = R_t[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5a2f9ca-f4b4-445a-ab5f-e0f0557f54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44ed09c8-f6b0-4b40-86e0-1975541fd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fudging_beta = beta(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d1a4f5e-7a0d-42b7-ad45-59c708b7b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation(data1, data2, combined_ensembles , size_ens, fudging_beta):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4 + 4):(4*params + 4 + 4 + 4)]\n",
    "    \n",
    "    \n",
    "    softmax_weights = tf.math.softmax(initial_ensembles_for_weights).numpy()\n",
    "    \n",
    "    model_1 = softmax_weights[:, :2].sum(1).reshape(-1,1) +  fudging_beta.rvs(size_ens).reshape(-1,1)\n",
    "    \n",
    "    # model_1 = np.min(model_1 -fudging_factor)\n",
    "    \n",
    "    model_2 = softmax_weights[:, 2:].sum(1).reshape(-1,1) +  fudging_beta.rvs(size_ens).reshape(-1,1)\n",
    "    \n",
    "    \n",
    "    model_1_plus_model_2 = model_1 + model_2\n",
    "    \n",
    "    model_1 = model_1/model_1_plus_model_2\n",
    "    \n",
    "    model_2 = model_2/model_1_plus_model_2\n",
    "    \n",
    "    \n",
    "    # print(np.mean(model_1 + model_2))\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                  weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data1, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                weights=model_1)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data2, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 weights=model_2)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                  weights=model_2)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    weighted_alogp = data1_out1 + data2_out1\n",
    "    \n",
    "    weighted_psa = data1_out2 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles, weighted_alogp, weighted_psa, model_1, model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6bd2eb3-86b7-41ed-81b4-4dd8e44e106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation_test(data1, data2, combined_ensembles , size_ens):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4 + 4):(4*params + 4 + 4 + 4)]\n",
    "    \n",
    "    \n",
    "    softmax_weights = tf.math.softmax(initial_ensembles_for_weights).numpy()\n",
    "    \n",
    "    model_1 = softmax_weights[:, :2].sum(1).reshape(-1,1) \n",
    "    \n",
    "    # model_1 = np.min(model_1 -fudging_factor)\n",
    "    \n",
    "    model_2 = softmax_weights[:, 2:].sum(1).reshape(-1,1) \n",
    "    \n",
    "    \n",
    "#     model_1_plus_model_2 = model_1 + model_2\n",
    "    \n",
    "#     model_1 = model_1/model_1_plus_model_2\n",
    "    \n",
    "#     model_2 = model_2/model_1_plus_model_2\n",
    "    \n",
    "    \n",
    "    # print(np.mean(model_1 + model_2))\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                  weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data1, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                weights=model_1)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data2, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 weights=model_2)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                  weights=model_2)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    weighted_alogp = data1_out1 + data2_out1\n",
    "    \n",
    "    weighted_psa = data1_out2 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles, weighted_alogp, weighted_psa, model_1, model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e15b0bf7-ba00-40bc-933b-c1c3e062e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_ann =  ann(hidden = 16, input_shape = 32, output_shape = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af3dd570-7ab3-4eae-bd8e-9a57a333b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = 4*(samp_ann.count_params() + 1 + 1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e820b010-4396-4b7a-8781-a8e6d503aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5fbaaf7-afae-4008-a26c-e0691ef9b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens = total_weights//reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5111536-1a97-4390-9658-c9848a136a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f598992-73d6-4b5a-906d-3184f1b9625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_t = [[1, 0, 1, 0], [0, 1, 0, 1]]\n",
    "G_t = np.array(G_t).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3073d2ce-5b22-47bb-80a3-9ad86ac3d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a84cb0e3-2f5e-4207-b092-d2768b52205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data1, data2, initial_ensembles, fudging_beta  =fudging_beta): \n",
    "    _,_, weighted_alogp, weighted_psa, w1, w2 = forward_operation(data1, data2, initial_ensembles, size_ens = size_ens, fudging_beta = fudging_beta)\n",
    "    weighted_alogp = np.expand_dims(weighted_alogp,-1)\n",
    "    weighted_psa = np.expand_dims(weighted_psa,-1)\n",
    "    preds = np.concatenate((weighted_alogp, weighted_psa),-1)\n",
    "    return preds, w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37b60b37-92b1-4784-85fc-59a865d2398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_test(data1, data2, initial_ensembles): \n",
    "    _,_, weighted_alogp, weighted_psa, w1, w2 = forward_operation_test(data1, data2, initial_ensembles, size_ens = size_ens)\n",
    "    weighted_alogp = np.expand_dims(weighted_alogp,-1)\n",
    "    weighted_psa = np.expand_dims(weighted_psa,-1)\n",
    "    preds = np.concatenate((weighted_alogp, weighted_psa),-1)\n",
    "    return preds, w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3af0a9b9-6202-40e2-911f-3c890213099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mu_bar_G_bar(data1, data2, initial_ensembles, fudging_beta):\n",
    "    H_t = np.hstack((np.identity(data1.shape[0]), np.zeros((data1.shape[0], samp_ann_params + 1 + 1 + 1))))\n",
    "    mu_bar = initial_ensembles.mean(0)\n",
    "    X_t,_, _, _, _, _ = forward_operation(data1, data2, initial_ensembles, size_ens = size_ens, fudging_beta = fudging_beta)\n",
    "    X_t = X_t.transpose((0,2,1))\n",
    "    X_t = X_t.reshape(X_t.shape[0], X_t.shape[1]*X_t.shape[2])\n",
    "    script_H_t = np.kron(G_t.T, H_t)\n",
    "    G_u = (script_H_t@X_t.T)\n",
    "    G_u = G_u.T\n",
    "    # weighted_alogp = np.expand_dims(weighted_alogp,-1)\n",
    "    # weighted_psa = np.expand_dims(weighted_psa,-1)\n",
    "    # G_u = np.concatenate((weighted_alogp, weighted_psa), axis = -1)\n",
    "    # G_u = G_u.transpose((0,2,1))\n",
    "    # G_u = G_u.reshape(G_u.shape[0], G_u.shape[1]*G_u.shape[2])\n",
    "    # G_u\n",
    "    G_bar = (G_u.mean(0)).ravel()\n",
    "    return mu_bar.reshape(-1,1), G_bar.reshape(-1,1), G_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7334c80c-cfcd-4d5b-b844-11cfb8c137df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u): \n",
    "    u_j_minus_u_bar = initial_ensembles - mu_bar.reshape(1,-1)\n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    c = np.zeros((total_weights, G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        c += np.kron(u_j_minus_u_bar[i, :].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return c/size_ens, G_u_minus_G_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9148779c-1a3f-455d-a6ad-00fc1bf69c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_D_u( G_bar, G_u): \n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    d = np.zeros((G_bar.shape[0], G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        d += np.kron(G_u_minus_G_bar[i,:].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return d/size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e8459b0-68c1-4d1f-851e-3d7b99f0ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_ensemble(data1, data2, initial_ensembles, y_train, size_ens = size_ens, inflation_factor = 1.0, fudging_beta = fudging_beta):\n",
    "    mu_bar, G_bar, G_u = calculate_mu_bar_G_bar(data1, data2, initial_ensembles, fudging_beta)\n",
    "    C, G_u_minus_G_bar = calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u)\n",
    "    D = calculate_D_u( G_bar, G_u)\n",
    "    _, R_t = create_cov(data1.shape[0],initial_ensembles)\n",
    "    inflation = np.identity(R_t.shape[0])*inflation_factor\n",
    "    D_plus_cov = D + (R_t *inflation_factor)\n",
    "    # all_covs = np.array(all_covs)\n",
    "    # D_plus_cov = D + R_t\n",
    "    D_plus_cov_inv = np.linalg.inv(D_plus_cov)\n",
    "    mid_quant = C@D_plus_cov_inv\n",
    "    noise_vec_mean = np.zeros((R_t.shape[0], ))\n",
    "    noise_mvn = mvn(noise_vec_mean, R_t)\n",
    "    fudging = noise_mvn.rvs(size_ens)\n",
    "    interim = (y_train.T.flatten().reshape(1,-1) + fudging)\n",
    "    right_quant = interim - G_u\n",
    "    # print(mid_quant.shape, right_quant.shape)\n",
    "    mid_times_right = mid_quant@right_quant.T\n",
    "    updated_ensemble = (initial_ensembles + mid_times_right.T)\n",
    "    return updated_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c14b236d-6ebf-40e3-9085-97afd2990e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "346d07bc-5b61-4e75-a816-a6e22ffe9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_D = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc3a216a-4dbb-474d-8219-5f6c071250a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(data, idx):\n",
    "    data_cur = data[idx, :, :]\n",
    "    inv_data_cur = std_targets.inverse_transform(data_cur)\n",
    "    return inv_data_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5abd35fb-ea92-449d-894a-162ed1f4aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "381fc654-8877-41fd-840a-4b35153f923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cov(shape, initial_ensembles):\n",
    "    cov_part = initial_ensembles[:, -8:-4]\n",
    "    cov_part = cov_part.mean(0)\n",
    "    # variances = tf.math.softplus(cov_part[:2]).numpy()\n",
    "    variances = cov_part[:2]\n",
    "    covariances = cov_part[2:]\n",
    "    base_cov = np.identity(target_dim)\n",
    "    base_cov[0,0] = variances[0]\n",
    "    base_cov[1,1] = variances[1]\n",
    "    base_cov[0,1] = covariances[0]\n",
    "    base_cov[1,0] = covariances[1]\n",
    "    \n",
    "    variances1 = tf.math.softplus(initial_ensembles[:, -4:]).numpy()\n",
    "    variances1 = variances1.mean(0)\n",
    "    base_variances = np.identity(target_dim)\n",
    "    base_variances[0,0] = variances1[0]\n",
    "    base_variances[1,1] = variances1[2]\n",
    "    \n",
    "    final = np.linalg.cholesky(base_cov@base_cov.T + base_variances)\n",
    "    cov_mat = final@final.T\n",
    "    cov_mat_final = cov_mat\n",
    "    # cov_mat_final = cov_mat@cov_mat.T\n",
    "    \n",
    "    if is_pos_def(cov_mat_final) != True:\n",
    "        print(\"resulting cov matrix is not positive semi definite\")\n",
    "        pass\n",
    "    \n",
    "    # print(np.linalg.det(cov_mat_final))\n",
    "    \n",
    "    var1 = cov_mat_final[0,0]\n",
    "    var2 = cov_mat_final[1,1]\n",
    "    cov = cov_mat_final[1,0]\n",
    "\n",
    "    n = shape\n",
    "    \n",
    "    ul = var1*np.identity(n)\n",
    "    lr = var2*np.identity(n)\n",
    "    ur = cov*np.identity(n)\n",
    "    ll = ur.T    \n",
    "    \n",
    "    first_row = np.hstack((ul, ur))\n",
    "    second_row = np.hstack((ll, lr))\n",
    "    \n",
    "    R_t = np.vstack((first_row, second_row))\n",
    "    \n",
    "    # R_t = block_diag(*([cov_mat_final] * n))\n",
    "    \n",
    "    # R_t = np.linalg.inv(R_t)\n",
    "    \n",
    "    return cov_mat_final, R_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30aea5e4-361f-44ae-9ea3-d1d8743e6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed1f9938-cdc5-400f-8bab-22c180a6d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..//Data//smiles_to_rdkit_80_20_with_cov_minus_0.2_var.pickle\", \"rb\") as f: \n",
    "    catch = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c59ea29-6484-40d5-8777-80cf9ca6cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "176445fb-4f30-421a-a710-339955111baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(idx, var_weights = 1.0, var_weight_weights = 4.0): \n",
    "    catch_idx = catch[idx]\n",
    "    x_train, x_valid, y_train, y_valid = catch_idx[0], catch_idx[1], catch_idx[2], catch_idx[3]\n",
    "    y_train_actual, y_train = y_train[:,:2], y_train[:,2:]\n",
    "    y_valid_actual, y_valid = y_valid[:,:2], y_valid[:,2:]\n",
    "    smiles_feats_train = x_train[:, :32]\n",
    "    rdkit_feats_train = x_train[:, 32:]\n",
    "    smiles_feats_valid = x_valid[:, :32]\n",
    "    rdkit_feats_valid = x_valid[:, 32:]\n",
    "\n",
    "    X_t, initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D = get_initial_X_t(smiles_feats_train, rdkit_feats_train, size_ens = size_ens, var_weights = var_weights, var_weight_weights = var_weight_weights)\n",
    "    initial_ensembles = np.hstack((initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D))\n",
    "    \n",
    "    return smiles_feats_train, rdkit_feats_train, smiles_feats_valid, rdkit_feats_valid, y_train, y_train_actual, y_valid, y_valid_actual, initial_ensembles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82d67093-cbd5-4a2b-bdbc-aa33a082e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smiles_feats_train, rdkit_feats_train, smiles_feats_valid, rdkit_feats_valid, y_train, y_train_actual, y_valid, y_valid_actual, initial_ensembles  = prepare_data(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa640128-df71-44ec-b20b-6922bdb5b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc0c894d-db8c-4d88-bd51-ef23ca80fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5cffacb-8558-45ca-bff9-e803e3fd6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a2e8d41-0775-457c-97a2-4060fd72634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta(1,19).rvs(size_ens).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65bf21e9-92c6-4824-93c3-87274b072afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(idx, var_weights = 1.0, var_weight_weights = 1.0, inflation_factor = 1.2, fudging_beta = beta(1,19)):\n",
    "    # print('var_weights' + str(var_weights))\n",
    "    # print('inflation_factor' + str(inflation_factor))\n",
    "    # print('var_weight_weights' + str(var_weight_weights))\n",
    "    smiles_feats_train, rdkit_feats_train, smiles_feats_valid, rdkit_feats_valid, y_train, y_train_actual, y_valid, y_valid_actual, initial_ensembles  = prepare_data(idx, var_weights = var_weights, var_weight_weights =var_weight_weights)\n",
    "    # print(R_t.shape)\n",
    "    best_train_width_mean = 100000\n",
    "    \n",
    "    for i in range(0,10000):\n",
    "        # print(i)\n",
    "    \n",
    "        c = np.zeros((2,2))\n",
    "        initial_ensembles = get_updated_ensemble(smiles_feats_train, rdkit_feats_train, initial_ensembles, y_train, size_ens, inflation_factor = inflation_factor, fudging_beta = fudging_beta)\n",
    "        # print(inflation_factor)\n",
    "        G_u_train, w1, w2 = get_predictions(smiles_feats_train, rdkit_feats_train, initial_ensembles, fudging_beta)\n",
    "\n",
    "        catch = Parallel(n_jobs = 15, verbose = 0)(delayed(inverse_transform)(G_u_train, i)  for i in range(G_u_train.shape[0]))\n",
    "        G_u_train = np.array(catch)\n",
    "    \n",
    "        y_train_cur = std_targets.inverse_transform(y_train_actual)\n",
    "    \n",
    "        li_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[0,:,:]   \n",
    "        ui_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "    \n",
    "        width_train = ui_train - li_train\n",
    "        avg_width_train = width_train.mean(0)\n",
    "    \n",
    "        ind_train = (y_train_cur >= li_train) & (y_train_cur <= ui_train)\n",
    "        coverage_train= ind_train.mean(0)\n",
    "    \n",
    "        averaged_targets_train = G_u_train.mean(0)\n",
    "        rmse_train = np.sqrt(((y_train_cur -averaged_targets_train)**2).mean(0))\n",
    "    # print(rmse_train, coverage_train, avg_width_train)\n",
    "    \n",
    "        G_u_test, _, _ = get_predictions_test(smiles_feats_valid, rdkit_feats_valid, initial_ensembles)\n",
    "    \n",
    "        catch = Parallel(n_jobs = 15, verbose = 0)(delayed(inverse_transform)(G_u_test, i)  for i in range(G_u_test.shape[0]))\n",
    "        G_u_test = np.array(catch)\n",
    "    \n",
    "        y_valid_cur = std_targets.inverse_transform(y_valid_actual)    \n",
    "    \n",
    "        li_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[0,:,:]   \n",
    "        ui_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "    \n",
    "        width_test = ui_test - li_test\n",
    "        avg_width_test = width_test.mean(0)\n",
    "    \n",
    "        ind_test = (y_valid_cur >= li_test) & (y_valid_cur <= ui_test)\n",
    "        coverage_test= ind_test.mean(0)\n",
    "    \n",
    "        averaged_targets_test = G_u_test.mean(0)\n",
    "        rmse_test = np.sqrt(((y_valid_cur -averaged_targets_test)**2).mean(0))    \n",
    "    \n",
    "        # weight_norms = np.array(norm(initial_ensembles, ord = 2, axis = 1))\n",
    "        # weight_norm_mean.append(weight_norms.mean())\n",
    "        # weight_norm_sd.append(weight_norms.std())\n",
    "    \n",
    "        cov_mat_final, _ = create_cov(smiles_feats_train.shape[0],initial_ensembles)\n",
    "        \n",
    "        # print(\"standardized_scale_R_t\")\n",
    "        # print(np.diag(cov_mat_final), cov_mat_final[0,1])\n",
    "        \n",
    "        # print(w1.shape)\n",
    "        \n",
    "        li_smiles_weight = np.percentile(w1, axis = 0, q = (2.5, 97.5))[0][0]\n",
    "        \n",
    "        # print(np.percentile(w1, axis = 0, q = (2.5, 97.5)))\n",
    "        \n",
    "        ui_smiles_weight = np.percentile(w1, axis = 0, q = (2.5, 97.5))[1][0]      \n",
    "        \n",
    "        # print(coverage_train.tolist(), avg_width_train.tolist(), rmse_train.tolist())\n",
    "        # print(coverage_test.tolist(), avg_width_test.tolist(), rmse_test.tolist())\n",
    "        # print(w1.mean())\n",
    "        # print(li_smiles_weight, ui_smiles_weight)\n",
    "        # print(avg_width_train.tolist(), coverage_train.tolist(), rmse_train.tolist(), avg_width_test.tolist(), coverage_test.tolist(), rmse_test.tolist(), w1.mean())\n",
    "\n",
    "        if (avg_width_train.mean() < best_train_width_mean) & (coverage_train.mean() > 0.95): \n",
    "            # print(\"went here\")\n",
    "            best_train_width_mean = avg_width_train.mean()\n",
    "            best_train_width = avg_width_train\n",
    "            best_smiles_weight = w1.mean()\n",
    "            best_coverage_train = coverage_train\n",
    "            best_rmse_train = rmse_train\n",
    "        \n",
    "            best_test_width = avg_width_test\n",
    "\n",
    "            best_coverage_test = coverage_test    \n",
    "            best_rmse_test = rmse_test\n",
    "            \n",
    "            best_li_smiles_weight = li_smiles_weight\n",
    "            \n",
    "            best_ui_smiles_weight = ui_smiles_weight\n",
    "    \n",
    "        if coverage_train.mean() < 0.95:\n",
    "            \n",
    "            # print()\n",
    "            # print(best_train_width.tolist(), best_coverage_train.tolist(), best_rmse_train.tolist(), best_test_width.tolist(), best_coverage_test.tolist(), best_rmse_test.tolist(), best_smiles_weight, flush = True)\n",
    "            print(\"done for fold\" + str(idx), flush = True)\n",
    "            print(\"train_width\" + str(best_train_width.tolist()), flush = True)\n",
    "            print(\"test_width\" + str(best_test_width.tolist()), flush = True)\n",
    "            print(\"smiles_weight\" + str(best_smiles_weight), flush = True)\n",
    "            print(\"rmse_train\" + str(best_rmse_train.tolist()), flush = True)\n",
    "            print(\"rmse_test\" + str(best_rmse_test.tolist()), flush = True)\n",
    "            print(\"smiles_weight_ci\" + str([best_li_smiles_weight, best_ui_smiles_weight]), flush = True)\n",
    "            \n",
    "            return [best_train_width.tolist(), best_coverage_train.tolist(), best_rmse_train.tolist(), best_test_width.tolist(), best_coverage_test.tolist(), best_rmse_test.tolist(), best_smiles_weight, best_li_smiles_weight, best_ui_smiles_weight]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "201d8498-7742-4509-94de-4fb1f5d84e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14658528-ea1a-4a6f-83bd-f708df0bcd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 3 µs, total: 6 µs\n",
      "Wall time: 14.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get_results(43, var_weights = 1.0, var_weight_weights = 3.0, inflation_factor =1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ac41ba9-2b9e-4b10-b6ad-e415030e40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0033c73-c13a-492a-8caa-fb16a2d51405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold8\n",
      "train_width[1.0847291789892317, 19.20892224040216]\n",
      "test_width[0.8361406117550596, 16.483142109034002]\n",
      "smiles_weight0.8414032994999122\n",
      "rmse_train[0.24552270613384744, 4.530572365940998]\n",
      "rmse_test[0.21718087658023108, 4.677358487734904]\n",
      "smiles_weight_ci[0.7401994465371133, 0.9078455549069083]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold11\n",
      "train_width[0.967993272210303, 20.704152472140464]\n",
      "test_width[0.7482144231519666, 17.295827605570956]\n",
      "smiles_weight0.8542654665657387\n",
      "rmse_train[0.1979292069163449, 4.7662727307101065]\n",
      "rmse_test[0.22212147089763598, 4.728060661659346]\n",
      "smiles_weight_ci[0.7556661641957848, 0.91772071397014]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done   2 tasks      | elapsed:  5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold1\n",
      "train_width[0.828372183024853, 17.13720592220191]\n",
      "test_width[0.6966773199338674, 16.342220673399265]\n",
      "smiles_weight0.7673864004496154\n",
      "rmse_train[0.19691802196864908, 4.6654230923707045]\n",
      "rmse_test[0.1683771308280604, 5.335614353009048]\n",
      "smiles_weight_ci[0.680958972813426, 0.8344938608343536]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold12\n",
      "train_width[0.9334365849378621, 17.790361271937268]\n",
      "test_width[0.6358937089643601, 14.367446186958931]\n",
      "smiles_weight0.8528459402104711\n",
      "rmse_train[0.2221822307810798, 4.2904242403861765]\n",
      "rmse_test[0.18973549970058276, 4.458247533418008]\n",
      "smiles_weight_ci[0.7511302651582746, 0.9076813203091443]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold2\n",
      "train_width[0.7649075432628195, 17.584666724634012]\n",
      "test_width[0.7464124450295009, 17.805106729272946]\n",
      "smiles_weight0.7655850358770828\n",
      "rmse_train[0.1613522740793943, 4.15318430910195]\n",
      "rmse_test[0.5280498325089035, 5.054568635132486]\n",
      "smiles_weight_ci[0.6715867404470699, 0.8354980109827715]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold7\n",
      "train_width[0.8593754716232443, 18.09203171154736]\n",
      "test_width[0.6105741433543598, 13.979191115314705]\n",
      "smiles_weight0.8703982467504314\n",
      "rmse_train[0.17624664416600266, 4.519852501824234]\n",
      "rmse_test[0.22831561703429207, 5.046898006850518]\n",
      "smiles_weight_ci[0.7917535847651523, 0.9181112040482926]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold0\n",
      "train_width[0.8886377905260842, 15.866021499336169]\n",
      "test_width[0.5865116272545414, 12.27739468237283]\n",
      "smiles_weight0.8514442892168919\n",
      "rmse_train[0.18228238243234426, 4.157536464082681]\n",
      "rmse_test[0.1674768164770348, 3.6733609594871526]\n",
      "smiles_weight_ci[0.7631304473207355, 0.9114398229701446]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold6\n",
      "train_width[0.8946231236872879, 13.774863043027903]\n",
      "test_width[0.6637696590451612, 11.413408302495005]\n",
      "smiles_weight0.8616670605014869\n",
      "rmse_train[0.17200617892828066, 3.9189313344019685]\n",
      "rmse_test[0.22603690860310488, 4.849702895839349]\n",
      "smiles_weight_ci[0.7585684931414568, 0.9152157467281253]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold13\n",
      "train_width[0.8170205231546649, 15.053118450832816]\n",
      "test_width[0.5880506374808027, 12.410703588072659]\n",
      "smiles_weight0.8678624903685387\n",
      "rmse_train[0.17577646547548717, 4.072610690604766]\n",
      "rmse_test[0.18260928397886753, 8.32327075921508]\n",
      "smiles_weight_ci[0.7733948175130833, 0.9242685398427225]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold5\n",
      "train_width[0.8606243580625356, 15.831953148533628]\n",
      "test_width[0.5880458911672227, 13.056286662933575]\n",
      "smiles_weight0.8474806140570831\n",
      "rmse_train[0.1704976963788597, 4.034434803329428]\n",
      "rmse_test[0.18517641232517928, 3.243712429017654]\n",
      "smiles_weight_ci[0.7442409536265648, 0.9084001390350449]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold3\n",
      "train_width[0.7247956704354911, 14.029596680722541]\n",
      "test_width[0.5159207970953855, 11.487299983232703]\n",
      "smiles_weight0.8601788492275896\n",
      "rmse_train[0.1381967118788963, 3.465563014466859]\n",
      "rmse_test[0.21003557325597905, 5.375834619142292]\n",
      "smiles_weight_ci[0.7622453339548075, 0.9107184212397336]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  11 tasks      | elapsed:  6.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold14\n",
      "train_width[0.6397610862737674, 14.690685284702502]\n",
      "test_width[0.5118994199067971, 12.53305105906338]\n",
      "smiles_weight0.8194102681969893\n",
      "rmse_train[0.14757305068489882, 3.9707375648254035]\n",
      "rmse_test[0.18494114178476537, 3.350197991555213]\n",
      "smiles_weight_ci[0.7280979446426022, 0.880227409779344]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold4\n",
      "train_width[0.6737289250047898, 12.861163463056963]\n",
      "test_width[0.6269059387232502, 12.570533990118056]\n",
      "smiles_weight0.8240250992563624\n",
      "rmse_train[0.1726558690517328, 3.1685259160547985]\n",
      "rmse_test[0.4917915751852353, 10.10719664093624]\n",
      "smiles_weight_ci[0.7343119182663327, 0.8839466914505272]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold10\n",
      "train_width[0.7224286826713032, 13.050656103811129]\n",
      "test_width[0.4714264125525039, 10.050854356298084]\n",
      "smiles_weight0.849881503091805\n",
      "rmse_train[0.15192663464800008, 3.597065893342206]\n",
      "rmse_test[0.11829511499985312, 2.6248720725083152]\n",
      "smiles_weight_ci[0.7705232231380729, 0.9023097262364684]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold9\n",
      "train_width[0.7237898909516451, 11.872002861175348]\n",
      "test_width[0.44420929526607517, 8.658313396115748]\n",
      "smiles_weight0.87235497776455\n",
      "rmse_train[0.14714859146285617, 2.9692925806070294]\n",
      "rmse_test[0.14333606820332614, 2.2050661838562027]\n",
      "smiles_weight_ci[0.7847040459652989, 0.9276593907285337]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold17\n",
      "train_width[1.0607040809274244, 18.447585866945154]\n",
      "test_width[0.7185484786069217, 15.022029920848954]\n",
      "smiles_weight0.8816524586367077\n",
      "rmse_train[0.23332092119858666, 4.8409267371146365]\n",
      "rmse_test[0.20288950363497893, 5.0374877819213975]\n",
      "smiles_weight_ci[0.7895925859754008, 0.9372433825730262]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold18\n",
      "train_width[0.7376434637495317, 16.293393947557433]\n",
      "test_width[0.6295225790114197, 14.630702367737056]\n",
      "smiles_weight0.7974762314076603\n",
      "rmse_train[0.1904020424229587, 3.3539951101943903]\n",
      "rmse_test[0.1850561185995883, 3.4369260912545188]\n",
      "smiles_weight_ci[0.7222748134248204, 0.8545037268464457]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold20\n",
      "train_width[0.8166009866304282, 18.92577298009921]\n",
      "test_width[0.5767498333768079, 15.048097666058707]\n",
      "smiles_weight0.8422232433134251\n",
      "rmse_train[0.17893817166717108, 4.507785833552631]\n",
      "rmse_test[0.14624606109722652, 3.606291403120346]\n",
      "smiles_weight_ci[0.7603688775058207, 0.899382078469845]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold24\n",
      "train_width[1.1583541147397622, 24.540211569006367]\n",
      "test_width[0.9566568852805063, 20.530413099158046]\n",
      "smiles_weight0.8298373511575129\n",
      "rmse_train[0.2646352392598796, 5.393663831745349]\n",
      "rmse_test[0.21502625823067187, 4.6034152103766415]\n",
      "smiles_weight_ci[0.7357385183008144, 0.8904931013462064]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold15\n",
      "train_width[0.7887088970208, 14.11007190322796]\n",
      "test_width[0.577650054351682, 11.627037823790964]\n",
      "smiles_weight0.827199320197722\n",
      "rmse_train[0.1895276666726171, 3.8766265216325158]\n",
      "rmse_test[0.1729244687519516, 2.948495057484626]\n",
      "smiles_weight_ci[0.7253941757875425, 0.8869814781041416]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed: 10.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold26\n",
      "train_width[1.0300697847821467, 19.747485673435833]\n",
      "test_width[0.743693384212594, 16.14453539604701]\n",
      "smiles_weight0.8740045141511443\n",
      "rmse_train[0.21955107430555315, 4.749024225180553]\n",
      "rmse_test[0.2521891271403592, 4.901459365968348]\n",
      "smiles_weight_ci[0.78299406408533, 0.9264717124067839]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold19\n",
      "train_width[0.8254220282684788, 18.563919797252527]\n",
      "test_width[0.6360916635823697, 14.98116342673131]\n",
      "smiles_weight0.8573064774515692\n",
      "rmse_train[0.16946626770356762, 4.421477780063604]\n",
      "rmse_test[0.19448737152116782, 3.736622347320916]\n",
      "smiles_weight_ci[0.7661367254390437, 0.9162669790036967]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold16\n",
      "train_width[0.8037169987705329, 15.272999982836696]\n",
      "test_width[0.6570801405615053, 13.075599432951595]\n",
      "smiles_weight0.7925361334905857\n",
      "rmse_train[0.18767310751156874, 4.0176608742511855]\n",
      "rmse_test[0.19529326096622016, 4.138720380432466]\n",
      "smiles_weight_ci[0.7043020733563319, 0.8470066970542702]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold23\n",
      "train_width[0.8191542552242439, 14.14319270348327]\n",
      "test_width[0.5103526787546796, 10.638600362462013]\n",
      "smiles_weight0.870167279180274\n",
      "rmse_train[0.16864047199799345, 3.571584125731666]\n",
      "rmse_test[0.16868977690212675, 3.0083816317486956]\n",
      "smiles_weight_ci[0.7745933466969114, 0.9241235351946656]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold22\n",
      "train_width[0.8164719866499237, 12.186140394200903]\n",
      "test_width[0.48222585918985655, 9.308374881598372]\n",
      "smiles_weight0.893032644800216\n",
      "rmse_train[0.16089775170830597, 3.397838149410993]\n",
      "rmse_test[0.18919595212107548, 3.915349673610813]\n",
      "smiles_weight_ci[0.8022616203147699, 0.9377207242635931]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold25\n",
      "train_width[0.7161096222621004, 15.164873876651102]\n",
      "test_width[0.4881474622904213, 12.021806050716423]\n",
      "smiles_weight0.8475019096440857\n",
      "rmse_train[0.15684954218002062, 3.3922172209713612]\n",
      "rmse_test[0.183439152947288, 3.3816353554823446]\n",
      "smiles_weight_ci[0.7436039482756893, 0.902302356613289]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold21\n",
      "train_width[0.670454313066918, 11.330986863945757]\n",
      "test_width[0.43501571608493755, 8.519611427241275]\n",
      "smiles_weight0.8471009283822286\n",
      "rmse_train[0.17007613653145717, 2.836316823506577]\n",
      "rmse_test[0.14581423752041786, 2.7821770214632395]\n",
      "smiles_weight_ci[0.7662048689434537, 0.9027781817940623]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  27 out of  50 | elapsed: 12.4min remaining: 10.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold29\n",
      "train_width[0.8888021342183637, 15.09178136738787]\n",
      "test_width[0.5391709247823603, 10.809177487956724]\n",
      "smiles_weight0.8629299565154608\n",
      "rmse_train[0.16534786799203208, 3.910074088497758]\n",
      "rmse_test[0.18492589084906597, 3.181206658774211]\n",
      "smiles_weight_ci[0.7572862487007467, 0.9204912152538623]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold28\n",
      "train_width[0.766574628657621, 13.729585666022615]\n",
      "test_width[0.620110992075228, 13.323705950062145]\n",
      "smiles_weight0.8207072773949284\n",
      "rmse_train[0.1551060722482931, 3.2142184884570155]\n",
      "rmse_test[0.40086312874650276, 8.774045634967543]\n",
      "smiles_weight_ci[0.7472853616462473, 0.8801331229958689]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold27\n",
      "train_width[0.6932571444783845, 12.993897416776635]\n",
      "test_width[0.45727021475337654, 10.333055614805712]\n",
      "smiles_weight0.8421260094574358\n",
      "rmse_train[0.1596347749840766, 2.9702077540341323]\n",
      "rmse_test[0.18080787577870536, 2.6480782095597424]\n",
      "smiles_weight_ci[0.7612497294185546, 0.8972015096432503]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold32\n",
      "train_width[0.8932414828720118, 18.486868414812914]\n",
      "test_width[0.7135309875911726, 16.35172223810112]\n",
      "smiles_weight0.8316654667537347\n",
      "rmse_train[0.20377598211144154, 4.201300975107476]\n",
      "rmse_test[0.2485177073395324, 3.9579046804375153]\n",
      "smiles_weight_ci[0.729003755088898, 0.8934191238924052]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold34\n",
      "train_width[0.8066287624497744, 15.585878608696495]\n",
      "test_width[0.6510082644977674, 13.99076242660726]\n",
      "smiles_weight0.8706377245296651\n",
      "rmse_train[0.12884725931964996, 4.4299074376649195]\n",
      "rmse_test[0.44837462190908006, 12.926764113701537]\n",
      "smiles_weight_ci[0.7852207784434818, 0.9300996142811507]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold31\n",
      "train_width[0.7888717157674527, 17.565084437065074]\n",
      "test_width[0.5899950232396028, 14.635929973812162]\n",
      "smiles_weight0.8215925610255556\n",
      "rmse_train[0.17338537363169812, 3.7520244572876513]\n",
      "rmse_test[0.1371513098188575, 3.2421690940053605]\n",
      "smiles_weight_ci[0.7343792046994363, 0.8826388729915685]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  33 out of  50 | elapsed: 15.6min remaining:  8.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold37\n",
      "train_width[0.7988330341184232, 15.471196827159503]\n",
      "test_width[0.6372495434052652, 13.943499503369123]\n",
      "smiles_weight0.8440798931702334\n",
      "rmse_train[0.1731300580228695, 4.313176215275204]\n",
      "rmse_test[0.13103129900257435, 7.840198809043015]\n",
      "smiles_weight_ci[0.7534843324934721, 0.9036235555675257]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold38\n",
      "train_width[0.968374171829624, 19.90765002060761]\n",
      "test_width[0.6843374712312311, 15.863170837539817]\n",
      "smiles_weight0.8706386972740414\n",
      "rmse_train[0.1964699157243643, 4.496273933504809]\n",
      "rmse_test[0.19543439775701577, 4.127716003410078]\n",
      "smiles_weight_ci[0.7895301506547355, 0.9263745582227693]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold39\n",
      "train_width[0.858510717916715, 19.998063484237125]\n",
      "test_width[0.8761062417807061, 21.148541637165287]\n",
      "smiles_weight0.7286157255085988\n",
      "rmse_train[0.1921110362502478, 4.119531804939691]\n",
      "rmse_test[0.4945256846479508, 17.257073812858117]\n",
      "smiles_weight_ci[0.6521026685710688, 0.8043413321041939]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold33\n",
      "train_width[0.6779391235112647, 13.822171706198978]\n",
      "test_width[0.5129501066137023, 11.081397530657375]\n",
      "smiles_weight0.8328509909296935\n",
      "rmse_train[0.157738370888903, 3.590794455576119]\n",
      "rmse_test[0.16457360600159493, 2.9122940665308534]\n",
      "smiles_weight_ci[0.7475887398100204, 0.8881884910487804]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold30\n",
      "train_width[0.782405809193615, 14.249960481540633]\n",
      "test_width[0.5500536712270084, 11.7386083685497]\n",
      "smiles_weight0.8645927393740159\n",
      "rmse_train[0.16163729771560406, 3.4496543890754365]\n",
      "rmse_test[0.20216885784682018, 3.2628653035140203]\n",
      "smiles_weight_ci[0.77280130683344, 0.9212261210568979]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold35\n",
      "train_width[0.7549426940481581, 13.513382331694562]\n",
      "test_width[0.5092212610928327, 10.51128565618153]\n",
      "smiles_weight0.8351981142233394\n",
      "rmse_train[0.17469441453708384, 3.338323166071091]\n",
      "rmse_test[0.1477972940845975, 3.2503517417836565]\n",
      "smiles_weight_ci[0.7508242653017221, 0.8890295443715819]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  39 out of  50 | elapsed: 17.7min remaining:  5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold44\n",
      "train_width[0.9020314782180112, 20.18042846233878]\n",
      "test_width[0.7193264281449496, 17.446271289530998]\n",
      "smiles_weight0.7868127864749257\n",
      "rmse_train[0.21006811581957602, 5.152065860362461]\n",
      "rmse_test[0.16145169773372134, 4.517223163082472]\n",
      "smiles_weight_ci[0.6975669902407126, 0.8493017309137704]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold36\n",
      "train_width[0.8821038529278802, 10.782495003743573]\n",
      "test_width[0.46821620149427823, 8.49275182463412]\n",
      "smiles_weight0.8597125405452083\n",
      "rmse_train[0.14809822615294727, 3.110678566230732]\n",
      "rmse_test[0.22319082415252076, 3.0385337742166505]\n",
      "smiles_weight_ci[0.7628225597653066, 0.912699776550275]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold43\n",
      "train_width[0.9090653978375145, 16.893003819344713]\n",
      "test_width[0.8050917070838822, 16.231096639718004]\n",
      "smiles_weight0.7915711542559956\n",
      "rmse_train[0.21169148695831033, 4.164443707671385]\n",
      "rmse_test[0.22051710603575456, 5.01996971928973]\n",
      "smiles_weight_ci[0.7100054671038096, 0.8485828040235349]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold40\n",
      "train_width[0.6836202665522279, 14.1254841413488]\n",
      "test_width[0.5373553120530725, 12.445837505996597]\n",
      "smiles_weight0.8278338966908011\n",
      "rmse_train[0.16570432834762186, 2.894637325938011]\n",
      "rmse_test[0.171205250322446, 2.7118095182494195]\n",
      "smiles_weight_ci[0.7432144583581644, 0.8863241531392343]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold42\n",
      "train_width[0.747289227543253, 16.331685038575635]\n",
      "test_width[0.619547895108217, 14.274144996842775]\n",
      "smiles_weight0.796858728729563\n",
      "rmse_train[0.16536308233281682, 4.1517958858416675]\n",
      "rmse_test[0.16455446485628428, 3.9720188514899735]\n",
      "smiles_weight_ci[0.7168669074072997, 0.8582737075742726]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold41\n",
      "train_width[0.6442955698288649, 12.909311860749106]\n",
      "test_width[0.49865082343036715, 11.461047577480878]\n",
      "smiles_weight0.8040115271976701\n",
      "rmse_train[0.1287540917421895, 3.7621480818784363]\n",
      "rmse_test[0.16260955696255983, 3.8871356550845024]\n",
      "smiles_weight_ci[0.7196406983835165, 0.8626005542470497]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  45 out of  50 | elapsed: 19.0min remaining:  2.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold46\n",
      "train_width[0.9551689694646703, 21.035760670295222]\n",
      "test_width[0.9600954727237968, 21.544872129340018]\n",
      "smiles_weight0.6884467967216282\n",
      "rmse_train[0.21304763759993764, 4.383174611609016]\n",
      "rmse_test[0.2647954123456012, 4.174520424506186]\n",
      "smiles_weight_ci[0.599303955808443, 0.7684889835785843]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold47\n",
      "train_width[0.8761950675297266, 20.57735717257833]\n",
      "test_width[0.7345166508107792, 18.251628691668817]\n",
      "smiles_weight0.807574912349228\n",
      "rmse_train[0.21733673826712813, 4.1571765840676935]\n",
      "rmse_test[0.2339842125616461, 3.8100281071012416]\n",
      "smiles_weight_ci[0.7144078553977549, 0.8692198496754077]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold45\n",
      "train_width[0.7337318165461738, 12.735470063709466]\n",
      "test_width[0.5218110610857153, 10.687959619571645]\n",
      "smiles_weight0.8364981681876309\n",
      "rmse_train[0.13966961558593874, 3.232566553292188]\n",
      "rmse_test[0.15961377610678548, 3.0162722895199785]\n",
      "smiles_weight_ci[0.75404127243267, 0.8888189798274898]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold49\n",
      "train_width[0.8749426173834045, 17.91731919864391]\n",
      "test_width[0.7260888594055313, 15.241759291743708]\n",
      "smiles_weight0.7902612567624846\n",
      "rmse_train[0.2040866525631402, 4.366213893114214]\n",
      "rmse_test[0.17721314929339138, 3.8291731950886616]\n",
      "smiles_weight_ci[0.7100903707028392, 0.862338428507467]\n",
      "var_weights1.0\n",
      "inflation_factor1.0\n",
      "var_weight_weights3.0\n",
      "done for fold48\n",
      "train_width[0.7457043525918287, 14.655647767660883]\n",
      "test_width[0.5087005236531824, 10.919073676247685]\n",
      "smiles_weight0.839529567166601\n",
      "rmse_train[0.15363549301202084, 3.9338891605437865]\n",
      "rmse_test[0.18717502164886185, 3.462528048290817]\n",
      "smiles_weight_ci[0.7484475690901553, 0.8927110694968168]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  50 out of  50 | elapsed: 20.6min finished\n"
     ]
    }
   ],
   "source": [
    "catch_all = Parallel(n_jobs = 15, verbose = 10)(delayed(get_results)(idx,var_weights = 1.0, var_weight_weights = 3.0, inflation_factor =1.0) for idx in range(0,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f857358-e5f0-4d43-bf0a-d4d7d034cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ee2282b-2100-4ed9-972b-ea78eabe2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_all[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "857dc418-a018-4330-868a-47ae36a9f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1e456d98-c381-450b-964d-fdbdbd1bd45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_catch = []\n",
    "for item in catch_all:\n",
    "    catch_inner = []\n",
    "    for inner in item:\n",
    "        if type(inner) == list:\n",
    "            for inner1 in inner:\n",
    "                catch_inner.append(inner1)\n",
    "        if type(inner) != list:\n",
    "            catch_inner.append(inner)\n",
    "    all_catch.append(catch_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b17ec0ba-5448-4130-b54f-c8cf20f76128",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "21f8792e-1721-412d-a84e-39bf7ad3c070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 15)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "610306d1-f54e-42be-86c2-70ef6d4fdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6accc813-8aa7-48a0-abae-f89b6e5f5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.iloc[:,-1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81b223ac-16c3-4547-bb99-9a2e08bbb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"Alop_Train_Width\", \"PSA_Train_Width\", \"Alop_Train_Coverage\", \"PSA_Train_Coverage\", \n",
    "            \"Alop_Train_RMSE\", \"PSA_Train_RMSE\", \"Alop_Test_Width\", \"PSA_Test_Width\", \"Alop_Test_Coverage\", \"PSA_Test_Coverage\", \n",
    "            \"Alop_Test_RMSE\", \"PSA_Test_RMSE\", \"Smiles_Avg_Weight\", \"Lower_Interval_Smiles_Weight\", \"Upper_Interval_Smiles_Weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1124c374-99bf-4282-9e32-1074a5033ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e52823b-19de-4bed-ae04-519af1d5021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a4fd350f-4f0a-4f9a-af19-52590d875782",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"indicator\"] = (results_df[\"Lower_Interval_Smiles_Weight\"].values < 0.80) & (results_df[\"Upper_Interval_Smiles_Weight\"].values >= 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5b5283a6-b5f6-4752-a693-9b1dea85ba9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(results_df[\"indicator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc59d51b-355c-47af-af15-34da30579a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_csv(\"..//Data//smiles_rdkit_80_20__with_cov_minus_0.2_Simulation_added_beat_noise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "272ba30f-cb1b-4356-9b74-38683acfef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"width_weight_CI\"] = results_df[\"Upper_Interval_Smiles_Weight\"].values - results_df[\"Lower_Interval_Smiles_Weight\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033f4b53-811b-4faa-8331-be8978b24454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alop_Train_Width</th>\n",
       "      <th>PSA_Train_Width</th>\n",
       "      <th>Alop_Train_Coverage</th>\n",
       "      <th>PSA_Train_Coverage</th>\n",
       "      <th>Alop_Train_RMSE</th>\n",
       "      <th>PSA_Train_RMSE</th>\n",
       "      <th>Alop_Test_Width</th>\n",
       "      <th>PSA_Test_Width</th>\n",
       "      <th>Alop_Test_Coverage</th>\n",
       "      <th>PSA_Test_Coverage</th>\n",
       "      <th>Alop_Test_RMSE</th>\n",
       "      <th>PSA_Test_RMSE</th>\n",
       "      <th>Smiles_Avg_Weight</th>\n",
       "      <th>Lower_Interval_Smiles_Weight</th>\n",
       "      <th>Upper_Interval_Smiles_Weight</th>\n",
       "      <th>indicator</th>\n",
       "      <th>width_weight_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.888638</td>\n",
       "      <td>15.866021</td>\n",
       "      <td>0.966620</td>\n",
       "      <td>0.942976</td>\n",
       "      <td>0.182282</td>\n",
       "      <td>4.157536</td>\n",
       "      <td>0.586512</td>\n",
       "      <td>12.277395</td>\n",
       "      <td>0.970833</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.167477</td>\n",
       "      <td>3.673361</td>\n",
       "      <td>0.851444</td>\n",
       "      <td>0.763130</td>\n",
       "      <td>0.911440</td>\n",
       "      <td>True</td>\n",
       "      <td>0.148309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.828372</td>\n",
       "      <td>17.137206</td>\n",
       "      <td>0.965229</td>\n",
       "      <td>0.952712</td>\n",
       "      <td>0.196918</td>\n",
       "      <td>4.665423</td>\n",
       "      <td>0.696677</td>\n",
       "      <td>16.342221</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.168377</td>\n",
       "      <td>5.335614</td>\n",
       "      <td>0.767386</td>\n",
       "      <td>0.680959</td>\n",
       "      <td>0.834494</td>\n",
       "      <td>True</td>\n",
       "      <td>0.153535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.764908</td>\n",
       "      <td>17.584667</td>\n",
       "      <td>0.962448</td>\n",
       "      <td>0.966620</td>\n",
       "      <td>0.161352</td>\n",
       "      <td>4.153184</td>\n",
       "      <td>0.746412</td>\n",
       "      <td>17.805107</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.528050</td>\n",
       "      <td>5.054569</td>\n",
       "      <td>0.765585</td>\n",
       "      <td>0.671587</td>\n",
       "      <td>0.835498</td>\n",
       "      <td>True</td>\n",
       "      <td>0.163911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.724796</td>\n",
       "      <td>14.029597</td>\n",
       "      <td>0.954103</td>\n",
       "      <td>0.952712</td>\n",
       "      <td>0.138197</td>\n",
       "      <td>3.465563</td>\n",
       "      <td>0.515921</td>\n",
       "      <td>11.487300</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.210036</td>\n",
       "      <td>5.375835</td>\n",
       "      <td>0.860179</td>\n",
       "      <td>0.762245</td>\n",
       "      <td>0.910718</td>\n",
       "      <td>True</td>\n",
       "      <td>0.148473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.673729</td>\n",
       "      <td>12.861163</td>\n",
       "      <td>0.959666</td>\n",
       "      <td>0.956885</td>\n",
       "      <td>0.172656</td>\n",
       "      <td>3.168526</td>\n",
       "      <td>0.626906</td>\n",
       "      <td>12.570534</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.491792</td>\n",
       "      <td>10.107197</td>\n",
       "      <td>0.824025</td>\n",
       "      <td>0.734312</td>\n",
       "      <td>0.883947</td>\n",
       "      <td>True</td>\n",
       "      <td>0.149635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alop_Train_Width  PSA_Train_Width  Alop_Train_Coverage  PSA_Train_Coverage  \\\n",
       "0          0.888638        15.866021             0.966620            0.942976   \n",
       "1          0.828372        17.137206             0.965229            0.952712   \n",
       "2          0.764908        17.584667             0.962448            0.966620   \n",
       "3          0.724796        14.029597             0.954103            0.952712   \n",
       "4          0.673729        12.861163             0.959666            0.956885   \n",
       "\n",
       "   Alop_Train_RMSE  PSA_Train_RMSE  Alop_Test_Width  PSA_Test_Width  \\\n",
       "0         0.182282        4.157536         0.586512       12.277395   \n",
       "1         0.196918        4.665423         0.696677       16.342221   \n",
       "2         0.161352        4.153184         0.746412       17.805107   \n",
       "3         0.138197        3.465563         0.515921       11.487300   \n",
       "4         0.172656        3.168526         0.626906       12.570534   \n",
       "\n",
       "   Alop_Test_Coverage  PSA_Test_Coverage  Alop_Test_RMSE  PSA_Test_RMSE  \\\n",
       "0            0.970833           0.933333        0.167477       3.673361   \n",
       "1            0.920833           0.941667        0.168377       5.335614   \n",
       "2            0.950000           0.966667        0.528050       5.054569   \n",
       "3            0.904167           0.975000        0.210036       5.375835   \n",
       "4            0.862500           0.908333        0.491792      10.107197   \n",
       "\n",
       "   Smiles_Avg_Weight  Lower_Interval_Smiles_Weight  \\\n",
       "0           0.851444                      0.763130   \n",
       "1           0.767386                      0.680959   \n",
       "2           0.765585                      0.671587   \n",
       "3           0.860179                      0.762245   \n",
       "4           0.824025                      0.734312   \n",
       "\n",
       "   Upper_Interval_Smiles_Weight  indicator  width_weight_CI  \n",
       "0                      0.911440       True         0.148309  \n",
       "1                      0.834494       True         0.153535  \n",
       "2                      0.835498       True         0.163911  \n",
       "3                      0.910718       True         0.148473  \n",
       "4                      0.883947       True         0.149635  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f36d6343-cfa2-475d-a8dd-01007676f993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alop_Train_Width</td>\n",
       "      <td>0.825203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PSA_Train_Width</td>\n",
       "      <td>16.096670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alop_Train_Coverage</td>\n",
       "      <td>0.962698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PSA_Train_Coverage</td>\n",
       "      <td>0.952045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alop_Train_RMSE</td>\n",
       "      <td>0.178951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PSA_Train_RMSE</td>\n",
       "      <td>3.954676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alop_Test_Width</td>\n",
       "      <td>0.622456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PSA_Test_Width</td>\n",
       "      <td>13.660121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alop_Test_Coverage</td>\n",
       "      <td>0.919500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PSA_Test_Coverage</td>\n",
       "      <td>0.941250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alop_Test_RMSE</td>\n",
       "      <td>0.215544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PSA_Test_RMSE</td>\n",
       "      <td>4.667416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Smiles_Avg_Weight</td>\n",
       "      <td>0.832419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Lower_Interval_Smiles_Weight</td>\n",
       "      <td>0.742823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Upper_Interval_Smiles_Weight</td>\n",
       "      <td>0.890939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>indicator</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>width_weight_CI</td>\n",
       "      <td>0.148116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           index          0\n",
       "0               Alop_Train_Width   0.825203\n",
       "1                PSA_Train_Width  16.096670\n",
       "2            Alop_Train_Coverage   0.962698\n",
       "3             PSA_Train_Coverage   0.952045\n",
       "4                Alop_Train_RMSE   0.178951\n",
       "5                 PSA_Train_RMSE   3.954676\n",
       "6                Alop_Test_Width   0.622456\n",
       "7                 PSA_Test_Width  13.660121\n",
       "8             Alop_Test_Coverage   0.919500\n",
       "9              PSA_Test_Coverage   0.941250\n",
       "10                Alop_Test_RMSE   0.215544\n",
       "11                 PSA_Test_RMSE   4.667416\n",
       "12             Smiles_Avg_Weight   0.832419\n",
       "13  Lower_Interval_Smiles_Weight   0.742823\n",
       "14  Upper_Interval_Smiles_Weight   0.890939\n",
       "15                     indicator   0.960000\n",
       "16               width_weight_CI   0.148116"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0636ac66-6077-4a54-9423-542338c64072",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"..//Data//smiles_rdkit_80_20__with_cov_minus_0.2_Simulation_added_beat_noise.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d11c8-4f75-4ec9-aacc-930747d74047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7839f-ac6e-4930-85ec-e58da91ec1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
