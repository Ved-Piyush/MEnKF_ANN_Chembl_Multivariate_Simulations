{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa53e19a-64db-4b1d-9378-c19c8efacd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 18:22:23.613290: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 18:22:23.721565: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-12 18:22:23.725526: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-12 18:22:23.725539: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-12 18:22:24.692375: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-12 18:22:24.693087: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-12 18:22:24.693097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import block_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d459b853-f520-4181-a9a9-310a6dd20de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_with_weights(batch_data, initial_ensembles, size_ens, ann): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a66ee4c-4112-4dca-a9cc-68a60d4e316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(hidden = 32, input_shape = 256, output_shape = 1): \n",
    "    input_layer = tf.keras.layers.Input(shape = (input_shape))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(output_shape, activation = \"relu\")\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b9ef03-af61-4ce4-99ff-2f0b5dca639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_ensembles(num_weights, lambda1, size_ens):\n",
    "    mean_vec = np.zeros((num_weights,))\n",
    "    cov_matrix = lambda1*np.identity(num_weights)\n",
    "    mvn_samp = mvn(mean_vec, cov_matrix)\n",
    "    return mvn_samp.rvs(size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2ea2b5-13a4-41c4-8257-c6c18708501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expit(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0623822d-59a8-44f0-948a-e23964559343",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f741d7-ba99-4803-8441-5bf2ad3d72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_X_t(data1, data2, size_ens, var_weights = 1.0):\n",
    "    samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    \n",
    "    initial_ensembles1 = generate_initial_ensembles(samp_ann.count_params(), var_weights, size_ens)\n",
    "    data1_out1, data1_stack1 = get_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens, ann = samp_ann)\n",
    "    \n",
    "    initial_ensembles2 = generate_initial_ensembles(samp_ann.count_params(), var_weights, size_ens)\n",
    "    data1_out2, data1_stack2 = get_targets_with_weights(data1, initial_ensembles2, size_ens = size_ens, ann = samp_ann)\n",
    "    \n",
    "    initial_ensembles3 = generate_initial_ensembles(samp_ann.count_params(), var_weights, size_ens)\n",
    "    data2_out1, data2_stack1 = get_targets_with_weights(data2, initial_ensembles3, size_ens = size_ens, ann = samp_ann)\n",
    "    \n",
    "    initial_ensembles4 = generate_initial_ensembles(samp_ann.count_params(), var_weights, size_ens)\n",
    "    data2_out2, data2_stack2 = get_targets_with_weights(data2, initial_ensembles4, size_ens = size_ens, ann = samp_ann)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles_for_weights = generate_initial_ensembles(4, var_weights, size_ens)\n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_L = generate_initial_ensembles(4, var_weights, size_ens)\n",
    "    initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)    \n",
    "    \n",
    "    initial_ensembles_for_D1 = generate_initial_ensembles(1, var_weights, size_ens).reshape(-1,1)\n",
    "    initial_ensembles_for_D2 = generate_initial_ensembles(1, var_weights, size_ens).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D1_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    initial_ensembles_for_D2_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.concatenate((np.expand_dims(initial_ensembles_for_D1,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D1_zero,1), \n",
    "                                                      np.expand_dims(initial_ensembles_for_D2,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D2_zero,1)), axis = 2)\n",
    "    \n",
    "    # print(X_t.shape, initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4))\n",
    "    \n",
    "    return X_t, initial_ensembles, initial_ensembles_for_weights[:,0,:], initial_ensembles_for_L[:,0,:], initial_ensembles_for_D[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac49c8c-6da0-4ee2-9561-e1d19365f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_targets_with_weights(batch_data, initial_ensembles, size_ens, ann, weights): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    final_output_1 = final_output_1*weights\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f382d3e-e4dc-4e19-b19a-0eb954236a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "alogp_bottleneck = np.load(\"..//Data/small_mol_phase_3_features_for_both.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e5e3f5-e4c6-40f8-ac40-b487543e3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = np.load('..//Data/smiles_0.7_rdkit_0.3_outputs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b697b7f9-3d50-4d71-a436-cdc6434909a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/statgrads/vpiyush2/.conda/envs/enkf/lib/python3.10/site-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator StandardScaler from version 1.1.2 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "std_targets = pickle.load( open('..//Data//target_scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7f85a16-abcf-4c61-8635-b47b6b1881a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_valid = pd.read_csv(\"..//Data/smiles_with_rdkit_with_small_phase_3_outputs.csv\")\n",
    "# y_train = y_valid.values[:,1:]\n",
    "# y_train = std_targets.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ad7f00f-1bd3-40bc-b50b-de0406c72655",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_valid\n",
    "# y_train = y_train = std_targets.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e49a5313-fa38-44e9-bb29-07b461c12162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.cov(y_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3fe10a7-fcc3-430f-8ade-356a7f7b85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = std_targets.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dfffa08-081b-4b5f-8181-ae94a920c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f03896e4-b7c8-4237-a0a8-7af9ed6b1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(alogp_bottleneck, y_train, test_size = 0.25, shuffle = True, \n",
    "                                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2ed02ca-e8a1-4767-8152-4fc17b1afe61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(719, 64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b98c57b-e85d-46f9-92b4-09f2a1b179a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_feats_train = x_train[:, :32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b91f4be-60b4-44ee-b992-80da2cbe5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdkit_feats_train = x_train[:, 32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfe0b04f-b7c9-4f72-ac4b-4ba7e50d128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_feats_valid = x_valid[:, :32]\n",
    "rdkit_feats_valid = x_valid[:, 32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5932b6c7-8781-4514-8b67-1e760609c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_ens = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d1a4f5e-7a0d-42b7-ad45-59c708b7b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation(data1, data2, combined_ensembles , size_ens ):\n",
    "    samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann.count_params()\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4 + 4):(4*params + 4 + 4 + 4)]\n",
    "    \n",
    "    \n",
    "    softmax_weights = tf.math.softmax(initial_ensembles_for_weights).numpy()\n",
    "    \n",
    "    model_1 = softmax_weights[:,:2].sum(1).reshape(-1,1)\n",
    "    \n",
    "    model_2 = softmax_weights[:,2:].sum(1).reshape(-1,1)\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                 ann = samp_ann, weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data1, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                 ann = samp_ann, weights=model_1)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data2, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 ann = samp_ann, weights=model_2)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                 ann = samp_ann, weights=model_2)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    weighted_alogp = data1_out1 + data2_out1\n",
    "    \n",
    "    weighted_psa = data1_out2 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles, weighted_alogp, weighted_psa, model_1, model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e15b0bf7-ba00-40bc-933b-c1c3e062e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 18:22:26.355988: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-12 18:22:26.356009: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-12 18:22:26.356022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c2518.crane.hcc.unl.edu): /proc/driver/nvidia/version does not exist\n",
      "2023-06-12 18:22:26.356203: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af3dd570-7ab3-4eae-bd8e-9a57a333b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = 4*(samp_ann.count_params() + 1 + 1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fcac026-31e1-4d16-8bea-f9cd8a99c51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2192"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e820b010-4396-4b7a-8781-a8e6d503aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5fbaaf7-afae-4008-a26c-e0691ef9b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens = total_weights//reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5111536-1a97-4390-9658-c9848a136a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a59f4669-65d3-41df-8cd9-79ebdecb9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D = get_initial_X_t(smiles_feats_train, rdkit_feats_train, size_ens = size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c1bbad1-127a-4c27-8fb9-23783b072a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_ensembles = np.hstack((initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecfb3b01-63af-4194-8bb8-8d7dfa1a9edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 2192)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_ensembles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f598992-73d6-4b5a-906d-3184f1b9625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_t = [[1, 0, 1, 0], [0, 1, 0, 1]]\n",
    "G_t = np.array(G_t).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3073d2ce-5b22-47bb-80a3-9ad86ac3d6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a84cb0e3-2f5e-4207-b092-d2768b52205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data1, data2, initial_ensembles): \n",
    "    _,_, weighted_alogp, weighted_psa, w1, w2 = forward_operation(data1, data2, initial_ensembles, size_ens = size_ens)\n",
    "    weighted_alogp = np.expand_dims(weighted_alogp,-1)\n",
    "    weighted_psa = np.expand_dims(weighted_psa,-1)\n",
    "    preds = np.concatenate((weighted_alogp, weighted_psa),-1)\n",
    "    return preds, w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3af0a9b9-6202-40e2-911f-3c890213099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mu_bar_G_bar(data1, data2, initial_ensembles):\n",
    "    H_t = np.hstack((np.identity(data1.shape[0]), np.zeros((data1.shape[0], samp_ann.count_params() + 1 + 1 + 1))))\n",
    "    mu_bar = initial_ensembles.mean(0)\n",
    "    X_t,_, _, _, _, _ = forward_operation(data1, data2, initial_ensembles, size_ens = size_ens)\n",
    "    X_t = X_t.transpose((0,2,1))\n",
    "    X_t = X_t.reshape(X_t.shape[0], X_t.shape[1]*X_t.shape[2])\n",
    "    script_H_t = np.kron(G_t.T, H_t)\n",
    "    G_u = (script_H_t@X_t.T)\n",
    "    G_u = G_u.T\n",
    "    # weighted_alogp = np.expand_dims(weighted_alogp,-1)\n",
    "    # weighted_psa = np.expand_dims(weighted_psa,-1)\n",
    "    # G_u = np.concatenate((weighted_alogp, weighted_psa), axis = -1)\n",
    "    # G_u = G_u.transpose((0,2,1))\n",
    "    # G_u = G_u.reshape(G_u.shape[0], G_u.shape[1]*G_u.shape[2])\n",
    "    # G_u\n",
    "    G_bar = (G_u.mean(0)).ravel()\n",
    "    return mu_bar.reshape(-1,1), G_bar.reshape(-1,1), G_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7334c80c-cfcd-4d5b-b844-11cfb8c137df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u): \n",
    "    u_j_minus_u_bar = initial_ensembles - mu_bar.reshape(1,-1)\n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    c = np.zeros((total_weights, G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        c += np.kron(u_j_minus_u_bar[i, :].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return c/size_ens, G_u_minus_G_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9148779c-1a3f-455d-a6ad-00fc1bf69c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_D_u( G_bar, G_u): \n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    d = np.zeros((G_bar.shape[0], G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        d += np.kron(G_u_minus_G_bar[i,:].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return d/size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e8459b0-68c1-4d1f-851e-3d7b99f0ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_ensemble(data1, data2, initial_ensembles):\n",
    "    mu_bar, G_bar, G_u = calculate_mu_bar_G_bar(data1, data2, initial_ensembles)\n",
    "    C, G_u_minus_G_bar = calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u)\n",
    "    D = calculate_D_u( G_bar, G_u)\n",
    "    _, R_t = create_cov(data1.shape[0],initial_ensembles)\n",
    "    # all_covs = np.array(all_covs)\n",
    "    D_plus_cov = D + R_t\n",
    "    D_plus_cov_inv = np.linalg.inv(D_plus_cov)\n",
    "    mid_quant = C@D_plus_cov_inv\n",
    "    right_quant = y_train.T.flatten().reshape(-1,1) - G_u.T\n",
    "    mid_times_right = mid_quant@right_quant\n",
    "    updated_ensemble = (initial_ensembles.T + mid_times_right)\n",
    "    return updated_ensemble.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c14b236d-6ebf-40e3-9085-97afd2990e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1283df8-c281-48fa-82db-86888e6b6a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cov(shape, initial_ensembles):\n",
    "    cov_part = initial_ensembles[:, -8:-4]\n",
    "    cov_part = cov_part.mean(0)\n",
    "    # variances = tf.math.softplus(cov_part[:2]).numpy()\n",
    "    variances = cov_part[:2]\n",
    "    covariances = cov_part[2:]\n",
    "    base_cov = np.identity(target_dim)\n",
    "    base_cov[0,0] = variances[0]\n",
    "    base_cov[1,1] = variances[1]\n",
    "    base_cov[0,1] = covariances[0]\n",
    "    base_cov[1,0] = covariances[1]\n",
    "    \n",
    "    variances1 = tf.math.softplus(initial_ensembles[:, -4:]).numpy()\n",
    "    variances1 = variances1.mean(0)\n",
    "    base_variances = np.identity(target_dim)\n",
    "    base_variances[0,0] = variances1[0]\n",
    "    base_variances[1,1] = variances1[2]\n",
    "    \n",
    "    final = np.linalg.cholesky(base_cov@base_cov.T + base_variances)\n",
    "    cov_mat = final@final.T\n",
    "    cov_mat_final = cov_mat\n",
    "    # cov_mat_final = cov_mat@cov_mat.T\n",
    "    \n",
    "    if is_pos_def(cov_mat_final) != True:\n",
    "        print(\"resulting cov matrix is not positive semi definite\")\n",
    "        pass\n",
    "    \n",
    "    # print(np.linalg.det(cov_mat_final))\n",
    "    \n",
    "    var1 = cov_mat_final[0,0]\n",
    "    var2 = cov_mat_final[1,1]\n",
    "    cov = cov_mat_final[1,0]\n",
    "\n",
    "    n = shape\n",
    "    \n",
    "    ul = var1*np.identity(n)\n",
    "    lr = var2*np.identity(n)\n",
    "    ur = cov*np.identity(n)\n",
    "    ll = ur.T    \n",
    "    \n",
    "    first_row = np.hstack((ul, ur))\n",
    "    second_row = np.hstack((ll, lr))\n",
    "    \n",
    "    R_t = np.vstack((first_row, second_row))\n",
    "    \n",
    "    # R_t = block_diag(*([cov_mat_final] * n))\n",
    "    \n",
    "    # R_t = np.linalg.inv(R_t)\n",
    "    \n",
    "    return cov_mat_final, R_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af50f314-7841-4bc7-9764-fcf33c9b8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "242c1e9e-3745-4dc2-bab8-6fec04ea2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat_final, _ = create_cov(smiles_feats_train.shape[0],initial_ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0848e471-31ad-415d-adb4-64853aca2a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.06469552, 0.00883376],\n",
       "       [0.00883376, 1.11163292]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_mat_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc3a216a-4dbb-474d-8219-5f6c071250a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(data, idx):\n",
    "    data_cur = data[idx, :, :]\n",
    "    inv_data_cur = std_targets.inverse_transform(data_cur)\n",
    "    return inv_data_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5abd35fb-ea92-449d-894a-162ed1f4aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58e18b92-bba1-4a96-9abf-866b8594d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66cdaaa0-cd22-42b1-87ee-968e96aa3ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 2192)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_ensembles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9148940e-835a-41e2-89d6-1daf1afa3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_chk_full = pd.read_csv(\"..//Data/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfa93801-4360-4403-a813-9a3f1867bf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1694445, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_chk_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd92965c-9cf5-4ee7-9769-a52a0de67784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   3.56194622,  -24.68021802],\n",
       "       [ -24.68021802, 1862.41330966]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(y_chk_full.values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65bf21e9-92c6-4824-93c3-87274b072afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Averaging Weight\n",
      "[0.5330401433717615, 0.3006803200361828]\n",
      "Weight Norms\n",
      "[79.04672621890816, 9.334254761426253]\n",
      "standardized_scale_R_t\n",
      "[1.0579286  1.05357299] 0.019182155932196188\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[  6.75457942 220.51759985] [1. 1.] [  466.12010813 10303.2790537 ]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[  5.28709392 184.86675561] [1. 1.] [  459.7612382  10241.37450755]\n",
      "data_scale_R_t\n",
      "[[ 1.73528858e+04 -5.91001557e+03]\n",
      " [-5.91001557e+03  8.45076726e+06]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Averaging Weight\n",
      "[0.5907936174033759, 0.26871812991873045]\n",
      "Weight Norms\n",
      "[70.736539145113, 11.077137102521407]\n",
      "standardized_scale_R_t\n",
      "[1.0367309  1.00734303] 0.021040953638752656\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[  6.44682244 124.5233939 ] [1. 1.] [ 307.15833875 6789.3015779 ]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[  5.06265739 125.24241125] [1. 1.] [ 304.0128685  6677.28431341]\n",
      "data_scale_R_t\n",
      "[[7.83303812e+03 1.02295922e+03]\n",
      " [1.02295922e+03 3.66657661e+06]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Averaging Weight\n",
      "[0.612829406862826, 0.24862833998745726]\n",
      "Weight Norms\n",
      "[64.85869342838002, 11.611635604130425]\n",
      "standardized_scale_R_t\n",
      "[1.06426422 1.0137545 ] 0.03111864491773524\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[  6.97058586 108.15090099] [1. 1.] [ 233.8358551  5042.70164993]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 5.29526081 92.62213135] [1. 1.] [ 231.88335877 4995.46520045]\n",
      "data_scale_R_t\n",
      "[[ 4.46633572e+03 -9.51173253e+01]\n",
      " [-9.51173253e+01  2.04740390e+06]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Averaging Weight\n",
      "[0.6566675683194909, 0.21947285802526142]\n",
      "Weight Norms\n",
      "[60.14011637963599, 11.968907953932908]\n",
      "standardized_scale_R_t\n",
      "[1.01381078 0.99587305] 0.051562610114421606\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 3.78469305 72.94719914] [1. 1.] [ 172.71443049 3921.48650807]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 3.15106139 69.60951897] [1. 1.] [ 170.70157041 3893.06435235]\n",
      "data_scale_R_t\n",
      "[[ 2.43999690e+03 -8.37333512e+02]\n",
      " [-8.37333512e+02  1.19830922e+06]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "Averaging Weight\n",
      "[0.6632589350814525, 0.20584274247162238]\n",
      "Weight Norms\n",
      "[56.02010356903928, 12.18088301291356]\n",
      "standardized_scale_R_t\n",
      "[1.02245438 0.95694188] 0.03824925826957302\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.85697134 96.74823306] [1. 1.] [ 136.42106297 3002.30294053]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.86381031 88.79239413] [1. 1.] [ 135.10363304 2968.57347194]\n",
      "data_scale_R_t\n",
      "[[1.49992419e+03 1.12471477e+02]\n",
      " [1.12471477e+02 7.18693405e+05]]\n",
      "patience\n",
      "1\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "Averaging Weight\n",
      "[0.6893043044001004, 0.18445914896365548]\n",
      "Weight Norms\n",
      "[52.741454866053246, 12.011494133461797]\n",
      "standardized_scale_R_t\n",
      "[1.030576   0.87014249] 0.02556207358810041\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.38052098 75.53537666] [1. 1.] [ 111.32000811 2486.39939638]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.40303763 68.07896768] [1. 1.] [ 110.41549924 2455.97240003]\n",
      "data_scale_R_t\n",
      "[[   963.55577854    752.78448991]\n",
      " [   752.78448991 453863.737467  ]]\n",
      "patience\n",
      "2\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "Averaging Weight\n",
      "[0.7060348205647858, 0.170876763091748]\n",
      "Weight Norms\n",
      "[49.955597837672165, 11.649527720145237]\n",
      "standardized_scale_R_t\n",
      "[0.99915556 0.80685468] 0.018285311105201082\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.16073341 59.95703978] [1. 1.] [  94.67363311 2024.70725886]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.17462945 55.70107297] [1. 1.] [  93.56049891 2013.29577423]\n",
      "data_scale_R_t\n",
      "[[   702.13166373    584.29628622]\n",
      " [   584.29628622 327291.28027508]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "Averaging Weight\n",
      "[0.7026734831794462, 0.16418421031654545]\n",
      "Weight Norms\n",
      "[47.495028898820806, 11.066750840313677]\n",
      "standardized_scale_R_t\n",
      "[1.00494082 0.78173213] 0.0025065971802340097\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.12445784 61.76558932] [1. 1.] [  82.47000387 1787.40103226]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.09664832 57.6741721 ] [1. 1.] [  81.25294856 1775.63484597]\n",
      "data_scale_R_t\n",
      "[[   533.47742932   1404.15066751]\n",
      " [  1404.15066751 253682.10091656]]\n",
      "patience\n",
      "1\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "Averaging Weight\n",
      "[0.7073552341366471, 0.15644139758958142]\n",
      "Weight Norms\n",
      "[45.185464393702496, 10.626458573616821]\n",
      "standardized_scale_R_t\n",
      "[0.95657545 0.76628772] 0.022718249114637126\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.17347815 64.78448346] [1. 1.] [  72.28156961 1543.77528972]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.29616718 61.34344067] [1. 1.] [  70.83104034 1521.70000476]\n",
      "data_scale_R_t\n",
      "[[   415.27937447    621.48150212]\n",
      " [   621.48150212 186312.57001157]]\n",
      "patience\n",
      "2\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "Averaging Weight\n",
      "[0.7171790342853765, 0.14795778945495558]\n",
      "Weight Norms\n",
      "[43.1746903959544, 9.905566933565694]\n",
      "standardized_scale_R_t\n",
      "[1.00212833 0.76094912] 0.028523710888621163\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.04920117 60.44196536] [1. 1.] [  62.6999573  1429.72761595]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.24954926 55.57399153] [1. 1.] [  61.87188663 1428.34651011]\n",
      "data_scale_R_t\n",
      "[[   302.34255833    270.63584346]\n",
      " [   270.63584346 155683.58428162]]\n",
      "patience\n",
      "3\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "Averaging Weight\n",
      "[0.7217913207257607, 0.1392849960223023]\n",
      "Weight Norms\n",
      "[41.16066959277394, 9.371077927369633]\n",
      "standardized_scale_R_t\n",
      "[0.96633092 0.76373425] 0.014935040711219701\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.02730572 60.97733389] [1. 1.] [  54.92845525 1252.62561071]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.19117501 58.22691878] [1. 1.] [  54.22827213 1245.90022753]\n",
      "data_scale_R_t\n",
      "[[   237.09635769   -201.51962577]\n",
      " [  -201.51962577 119279.3488587 ]]\n",
      "patience\n",
      "4\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "Averaging Weight\n",
      "[0.718907661400205, 0.13319507605159542]\n",
      "Weight Norms\n",
      "[39.26184574401516, 8.57737920039323]\n",
      "standardized_scale_R_t\n",
      "[0.97779156 0.78096512] 0.04336477359758091\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.3090517  67.31855324] [1. 1.] [  50.18443619 1126.45080958]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.51545394 63.61539694] [1. 1.] [  49.21423396 1119.81396878]\n",
      "data_scale_R_t\n",
      "[[  194.65704264  -224.00849695]\n",
      " [ -224.00849695 95065.91960605]]\n",
      "patience\n",
      "5\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "Averaging Weight\n",
      "[0.728216886986526, 0.12506324842315747]\n",
      "Weight Norms\n",
      "[37.49252574229878, 7.855905450937834]\n",
      "standardized_scale_R_t\n",
      "[0.99278085 0.78625354] 0.03523096254839196\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.70120236 64.61381199] [1. 1.] [ 42.76501422 980.79216864]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.85623378 60.49401103] [1. 1.] [ 42.16481656 972.34362973]\n",
      "data_scale_R_t\n",
      "[[  143.71343552  -172.50745544]\n",
      " [ -172.50745544 73550.75352125]]\n",
      "patience\n",
      "6\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "Averaging Weight\n",
      "[0.733608128272094, 0.11374860939906478]\n",
      "Weight Norms\n",
      "[35.52190427980282, 7.366724878572596]\n",
      "standardized_scale_R_t\n",
      "[1.01074259 0.7680788 ] 0.035400949408702316\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.48509418 73.14134489] [1. 1.] [ 38.83769143 880.64606495]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.72258315 67.74910758] [1. 1.] [ 37.9451095  868.05545903]\n",
      "data_scale_R_t\n",
      "[[  116.93828307  -182.38693011]\n",
      " [ -182.38693011 60320.86603384]]\n",
      "patience\n",
      "7\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "Averaging Weight\n",
      "[0.7548522867346641, 0.10173968400409972]\n",
      "Weight Norms\n",
      "[33.61803976673383, 6.502439507747465]\n",
      "standardized_scale_R_t\n",
      "[1.1076828  0.79565005] 0.06458325237208552\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.21427289 73.33737083] [1. 1.] [ 32.22296736 703.30265501]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.43966189 69.08347813] [1. 1.] [ 31.67817364 696.7850813 ]\n",
      "data_scale_R_t\n",
      "[[ 7.94589710e+01 -2.80029099e+01]\n",
      " [-2.80029099e+01  3.92587686e+04]]\n",
      "patience\n",
      "8\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "Averaging Weight\n",
      "[0.7664222958098088, 0.08860390534664131]\n",
      "Weight Norms\n",
      "[31.47276006765652, 5.866786237283139]\n",
      "standardized_scale_R_t\n",
      "[1.08886385 0.76596064] 0.041747885213254565\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.11004146 64.6733776 ] [1. 1.] [ 24.52124917 555.58754975]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.37574044 62.67223171] [1. 1.] [ 24.12782179 555.29261546]\n",
      "data_scale_R_t\n",
      "[[4.85058954e+01 1.44308608e+00]\n",
      " [1.44308608e+00 2.61463638e+04]]\n",
      "patience\n",
      "9\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "Averaging Weight\n",
      "[0.7832364117240584, 0.07883401299702236]\n",
      "Weight Norms\n",
      "[29.35340742991006, 4.982133638377142]\n",
      "standardized_scale_R_t\n",
      "[1.14121479 0.74775673] 0.050496860573951835\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 1.93968824 59.53936601] [0.99582754 1.        ] [ 19.42467741 439.60343311]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.17289982 57.88795557] [1. 1.] [ 19.14874489 437.87018084]\n",
      "data_scale_R_t\n",
      "[[   29.52682354    38.4214141 ]\n",
      " [   38.4214141  15259.32587709]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m10000\u001b[39m):\n\u001b[1;32m     11\u001b[0m     c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m     initial_ensembles \u001b[38;5;241m=\u001b[39m \u001b[43mget_updated_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles_feats_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrdkit_feats_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_ensembles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     G_u_train, w1, w2 \u001b[38;5;241m=\u001b[39m get_predictions(smiles_feats_train, rdkit_feats_train, initial_ensembles)\n\u001b[1;32m     15\u001b[0m     w1_catch\u001b[38;5;241m.\u001b[39mappend(w1\u001b[38;5;241m.\u001b[39mmean())\n",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m, in \u001b[0;36mget_updated_ensemble\u001b[0;34m(data1, data2, initial_ensembles)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_updated_ensemble\u001b[39m(data1, data2, initial_ensembles):\n\u001b[1;32m      2\u001b[0m     mu_bar, G_bar, G_u \u001b[38;5;241m=\u001b[39m calculate_mu_bar_G_bar(data1, data2, initial_ensembles)\n\u001b[0;32m----> 3\u001b[0m     C, G_u_minus_G_bar \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_C_u\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_ensembles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_u\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     D \u001b[38;5;241m=\u001b[39m calculate_D_u( G_bar, G_u)\n\u001b[1;32m      5\u001b[0m     _, R_t \u001b[38;5;241m=\u001b[39m create_cov(data1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],initial_ensembles)\n",
      "Cell \u001b[0;32mIn[37], line 6\u001b[0m, in \u001b[0;36mcalculate_C_u\u001b[0;34m(initial_ensembles, mu_bar, G_bar, G_u)\u001b[0m\n\u001b[1;32m      4\u001b[0m c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((total_weights, G_bar\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, size_ens): \n\u001b[0;32m----> 6\u001b[0m     c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mkron(u_j_minus_u_bar[i, :]\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), G_u_minus_G_bar[i,:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\u001b[38;5;241m/\u001b[39msize_ens, G_u_minus_G_bar\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w1_catch = []\n",
    "w2_catch = []\n",
    "w1_sd_catch = []\n",
    "w2_sd_catch = []\n",
    "weight_norm_mean = []\n",
    "weight_norm_sd = []\n",
    "\n",
    "best_rmse = 10000\n",
    "patience = 0\n",
    "for i in range(0,10000):\n",
    "    c = np.zeros((2,2))\n",
    "    initial_ensembles = get_updated_ensemble(smiles_feats_train, rdkit_feats_train, initial_ensembles)\n",
    "    G_u_train, w1, w2 = get_predictions(smiles_feats_train, rdkit_feats_train, initial_ensembles)\n",
    "    \n",
    "    w1_catch.append(w1.mean())\n",
    "    w1_sd_catch.append(w1.std())\n",
    "    \n",
    "    w2_catch.append(w2.mean())\n",
    "    w2_sd_catch.append(w2.std())  \n",
    "    \n",
    "    print(\"Epoch \" + str(i+1))\n",
    "    \n",
    "    \n",
    "    print(\"Averaging Weight\")\n",
    "    print([w1.mean(), w1.std()])\n",
    "    # print(w2.mean(), w2.std())\n",
    "    \n",
    "    # G_u_train = get_targets_with_weights(smiles_feats_train, rdkit_feats_train, initial_ensembles, size_ens = size_ens)\n",
    "    catch = Parallel(n_jobs = 15, verbose = 0)(delayed(inverse_transform)(G_u_train, i)  for i in range(G_u_train.shape[0]))\n",
    "    G_u_train = np.array(catch)\n",
    "    \n",
    "    y_train_cur = std_targets.inverse_transform(y_train)\n",
    "    \n",
    "    li_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[0,:,:]   \n",
    "    ui_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "    \n",
    "    width_train = ui_train - li_train\n",
    "    avg_width_train = width_train.mean(0)\n",
    "    \n",
    "    ind_train = (y_train_cur >= li_train) & (y_train_cur <= ui_train)\n",
    "    coverage_train= ind_train.mean(0)\n",
    "    \n",
    "    averaged_targets_train = G_u_train.mean(0)\n",
    "    rmse_train = np.sqrt(((y_train_cur -averaged_targets_train)**2).mean(0))\n",
    "    # print(rmse_train, coverage_train, avg_width_train)\n",
    "    \n",
    "    G_u_test, _, _ = get_predictions(smiles_feats_valid, rdkit_feats_valid, initial_ensembles)\n",
    "    \n",
    "    catch = Parallel(n_jobs = 15, verbose = 0)(delayed(inverse_transform)(G_u_test, i)  for i in range(G_u_test.shape[0]))\n",
    "    G_u_test = np.array(catch)\n",
    "    \n",
    "    y_valid_cur = std_targets.inverse_transform(y_valid)    \n",
    "    \n",
    "    li_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[0,:,:]   \n",
    "    ui_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "    \n",
    "    width_test = ui_test - li_test\n",
    "    avg_width_test = width_test.mean(0)\n",
    "    \n",
    "    ind_test = (y_valid_cur >= li_test) & (y_valid_cur <= ui_test)\n",
    "    coverage_test= ind_test.mean(0)\n",
    "    \n",
    "    averaged_targets_test = G_u_test.mean(0)\n",
    "    rmse_test = np.sqrt(((y_valid_cur -averaged_targets_test)**2).mean(0))    \n",
    "    \n",
    "    weight_norms = np.array(norm(initial_ensembles, ord = 2, axis = 1))\n",
    "    weight_norm_mean.append(weight_norms.mean())\n",
    "    weight_norm_sd.append(weight_norms.std())\n",
    "    \n",
    "   \n",
    "    print(\"Weight Norms\")\n",
    "    print([weight_norms.mean(), weight_norms.std()])\n",
    "    \n",
    "    \n",
    "    # if np.mean(coverage_train < 0.95) == 1:\n",
    "    #     break\n",
    "        \n",
    "    cov_mat_final, _ = create_cov(smiles_feats_train.shape[0],initial_ensembles)\n",
    "    \n",
    "    print(\"standardized_scale_R_t\")\n",
    "    print(np.diag(cov_mat_final), cov_mat_final[0,1])\n",
    "    \n",
    "    print(\"Train RMSEs, Coverages, and Widths\")\n",
    "    print(rmse_train, coverage_train, avg_width_train)\n",
    "    \n",
    "    print(\"Test RMSEs, Coverages, and Widths\")\n",
    "    print(rmse_test, coverage_test, avg_width_test)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    value = rmse_train.mean()\n",
    "    \n",
    "    for i in range(0, size_ens):\n",
    "        c+= np.cov(G_u_train[i,:,:].T)\n",
    "        \n",
    "    c = c/size_ens    \n",
    "    \n",
    "    if value < best_rmse: \n",
    "        best_rmse = value\n",
    "        best_coverage = coverage_train\n",
    "        patience = 0\n",
    "        best_rmse_test = rmse_test\n",
    "        best_coverage_test = coverage_test\n",
    "        best_R_t = c\n",
    "        # print(['i', 'best_rmse', 'best_coverage', 'best_rmse_test', 'best_coverage_test', 'best_R_t'])\n",
    "        # print([i, best_rmse, best_coverage, best_rmse_test, best_coverage_test, best_R_t])\n",
    "    else:\n",
    "        patience +=1 \n",
    "    \n",
    "    print(\"data_scale_R_t\")\n",
    "    print(c)\n",
    "        \n",
    "    print('patience')\n",
    "    print(patience)\n",
    "    \n",
    "    print(\"\\n\")    \n",
    "        \n",
    "    if patience >= 20:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd0232-864e-422d-810b-ada022f08acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81eb6c3-e084-4e43-aa5e-3f9b0f5ee016",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_u_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8df43-9ae3-47f6-9c69-ac77c38a6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47207771-df91-48f0-9963-9a78989bcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.sample(range(y_valid_cur.shape[0]), k = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f17ea-a47c-421d-bd32-7a0dc0f1af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8, 2,figsize=(15, 15))\n",
    "# axs = axs.ravel()\n",
    "# counter = 0\n",
    "for idx, i in enumerate(random_idx):\n",
    "    # print(counter)\n",
    "    truth = y_valid_cur[i,:]\n",
    "    preds = G_u_test[:, i,:]\n",
    "    percts = np.percentile(preds, axis = 0, q = (2.5, 97.5))\n",
    "    lis = percts[0,:]\n",
    "    uis = percts[1,:]\n",
    "    \n",
    "    \n",
    "    axs[idx, 0].hist(preds[:,0])\n",
    "    axs[idx, 0].axvline(truth[0], color='green', linewidth=2)\n",
    "    axs[idx, 0].axvline(lis[0], color='red', linewidth=2)\n",
    "    axs[idx, 0].axvline(uis[0], color='red', linewidth=2)\n",
    "    \n",
    "    axs[idx, 1].hist(preds[:,1])\n",
    "    axs[idx, 1].axvline(truth[1], color='green', linewidth=2)\n",
    "    axs[idx, 1].axvline(lis[1], color='red', linewidth=2)\n",
    "    axs[idx, 1].axvline(uis[1], color='red', linewidth=2)\n",
    "    \n",
    "    # counter+=2\n",
    "    # print(counter)\n",
    "    \n",
    "    # plt.show()\n",
    "plt.savefig('prediction_intervals.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230a62b-f36f-4ec0-9c7d-0ba75c9de369",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_valid_cur[:, 0], averaged_targets_test[:,0])\n",
    "plt.axline((0,0), slope = 1, c= \"black\")\n",
    "plt.show()\n",
    "plt.scatter(y_valid_cur[:,1], averaged_targets_test[:, 1])\n",
    "plt.axline((0,0), slope = 1, c= \"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8d1f1-5b52-48eb-997c-0a760a8729a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w1_catch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa904e3-6452-4443-93ef-3c54377ff059",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w1_sd_catch)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a36715c-20fa-406e-9ac6-55a56152d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(range(0, len(w1_catch)), w1_catch, w1_sd_catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a18b9e-6388-4b45-b46b-e7ecd3ba8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(range(0, len(weight_norm_mean)), weight_norm_mean, weight_norm_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a7783b-b135-41e1-8ae7-6a346965daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat_final, _ = create_cov(smiles_feats_train.shape[0],initial_ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d891012-f14b-40fa-b8e4-8bf97bac5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd1fd9-89cb-4a66-896c-62f8805c0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(y_chk_full.values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d0666-8d71-4b66-bc92-9d03cf19d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_R_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
