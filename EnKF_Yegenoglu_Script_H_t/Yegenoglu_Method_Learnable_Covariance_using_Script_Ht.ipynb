{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa53e19a-64db-4b1d-9378-c19c8efacd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 18:17:16.351340: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 18:17:16.480259: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-12 18:17:16.484063: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-12 18:17:16.484080: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-12 18:17:17.466218: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-12 18:17:17.466708: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-12 18:17:17.466717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import block_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d459b853-f520-4181-a9a9-310a6dd20de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_with_weights(batch_data, initial_ensembles, size_ens, ann): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a66ee4c-4112-4dca-a9cc-68a60d4e316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(hidden = 32, input_shape = 256, output_shape = 1): \n",
    "    input_layer = tf.keras.layers.Input(shape = (input_shape))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(output_shape, activation = \"relu\")\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b9ef03-af61-4ce4-99ff-2f0b5dca639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_ensembles(num_weights, lambda1, size_ens):\n",
    "    mean_vec = np.zeros((num_weights,))\n",
    "    cov_matrix = lambda1*np.identity(num_weights)\n",
    "    mvn_samp = mvn(mean_vec, cov_matrix)\n",
    "    return mvn_samp.rvs(size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2ea2b5-13a4-41c4-8257-c6c18708501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expit(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0623822d-59a8-44f0-948a-e23964559343",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f741d7-ba99-4803-8441-5bf2ad3d72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_X_t(data1, data2, size_ens, var_weights = 8.0):\n",
    "    samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    \n",
    "    initial_ensembles1 = generate_initial_ensembles(samp_ann.count_params(), var_weights, size_ens)\n",
    "    data1_out1, data1_stack1 = get_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens, ann = samp_ann)\n",
    "    \n",
    "    initial_ensembles2 = generate_initial_ensembles(samp_ann.count_params(), var_weights, size_ens)\n",
    "    data1_out2, data1_stack2 = get_targets_with_weights(data1, initial_ensembles2, size_ens = size_ens, ann = samp_ann)\n",
    "    \n",
    "    initial_ensembles3 = generate_initial_ensembles(samp_ann.count_params(), var_weights, size_ens)\n",
    "    data2_out1, data2_stack1 = get_targets_with_weights(data2, initial_ensembles3, size_ens = size_ens, ann = samp_ann)\n",
    "    \n",
    "    initial_ensembles4 = generate_initial_ensembles(samp_ann.count_params(), var_weights, size_ens)\n",
    "    data2_out2, data2_stack2 = get_targets_with_weights(data2, initial_ensembles4, size_ens = size_ens, ann = samp_ann)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles_for_weights = generate_initial_ensembles(4, var_weights, size_ens)\n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_L = generate_initial_ensembles(4, var_weights, size_ens)\n",
    "    initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)    \n",
    "    \n",
    "    initial_ensembles_for_D1 = generate_initial_ensembles(1, var_weights, size_ens).reshape(-1,1)\n",
    "    initial_ensembles_for_D2 = generate_initial_ensembles(1, var_weights, size_ens).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D1_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    initial_ensembles_for_D2_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.concatenate((np.expand_dims(initial_ensembles_for_D1,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D1_zero,1), \n",
    "                                                      np.expand_dims(initial_ensembles_for_D2,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D2_zero,1)), axis = 2)\n",
    "    \n",
    "    # print(X_t.shape, initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4))\n",
    "    \n",
    "    return X_t, initial_ensembles, initial_ensembles_for_weights[:,0,:], initial_ensembles_for_L[:,0,:], initial_ensembles_for_D[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac49c8c-6da0-4ee2-9561-e1d19365f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_targets_with_weights(batch_data, initial_ensembles, size_ens, ann, weights): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    final_output_1 = final_output_1*weights\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f382d3e-e4dc-4e19-b19a-0eb954236a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "alogp_bottleneck = np.load(\"..//Data/small_mol_phase_3_features_for_both.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e5e3f5-e4c6-40f8-ac40-b487543e3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = pd.read_csv(\"..//Data/smiles_with_rdkit_with_small_phase_3_outputs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b697b7f9-3d50-4d71-a436-cdc6434909a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/statgrads/vpiyush2/.conda/envs/enkf/lib/python3.10/site-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator StandardScaler from version 1.1.2 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "std_targets = pickle.load( open('..//Data//target_scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ad7f00f-1bd3-40bc-b50b-de0406c72655",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_valid.values[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3fe10a7-fcc3-430f-8ade-356a7f7b85b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/statgrads/vpiyush2/.conda/envs/enkf/lib/python3.10/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_train = std_targets.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dfffa08-081b-4b5f-8181-ae94a920c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f03896e4-b7c8-4237-a0a8-7af9ed6b1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(alogp_bottleneck, y_train, test_size = 0.25, shuffle = True, \n",
    "                                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2ed02ca-e8a1-4767-8152-4fc17b1afe61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(719, 64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b98c57b-e85d-46f9-92b4-09f2a1b179a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_feats_train = x_train[:, :32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b91f4be-60b4-44ee-b992-80da2cbe5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdkit_feats_train = x_train[:, 32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfe0b04f-b7c9-4f72-ac4b-4ba7e50d128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_feats_valid = x_valid[:, :32]\n",
    "rdkit_feats_valid = x_valid[:, 32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5932b6c7-8781-4514-8b67-1e760609c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_ens = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d1a4f5e-7a0d-42b7-ad45-59c708b7b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation(data1, data2, combined_ensembles , size_ens ):\n",
    "    samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann.count_params()\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4 + 4):(4*params + 4 + 4 + 4)]\n",
    "    \n",
    "    \n",
    "    softmax_weights = tf.math.softmax(initial_ensembles_for_weights).numpy()\n",
    "    \n",
    "    model_1 = softmax_weights[:,:2].sum(1).reshape(-1,1)\n",
    "    \n",
    "    model_2 = softmax_weights[:,2:].sum(1).reshape(-1,1)\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                 ann = samp_ann, weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data1, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                 ann = samp_ann, weights=model_1)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data2, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 ann = samp_ann, weights=model_2)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                 ann = samp_ann, weights=model_2)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    weighted_alogp = data1_out1 + data2_out1\n",
    "    \n",
    "    weighted_psa = data1_out2 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles, weighted_alogp, weighted_psa, model_1, model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e15b0bf7-ba00-40bc-933b-c1c3e062e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 18:17:19.233713: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-12 18:17:19.233738: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-12 18:17:19.233752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c2518.crane.hcc.unl.edu): /proc/driver/nvidia/version does not exist\n",
      "2023-06-12 18:17:19.233950: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af3dd570-7ab3-4eae-bd8e-9a57a333b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = 4*(samp_ann.count_params() + 1 + 1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fcac026-31e1-4d16-8bea-f9cd8a99c51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2192"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e820b010-4396-4b7a-8781-a8e6d503aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5fbaaf7-afae-4008-a26c-e0691ef9b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens = total_weights//reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5111536-1a97-4390-9658-c9848a136a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a59f4669-65d3-41df-8cd9-79ebdecb9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D = get_initial_X_t(smiles_feats_train, rdkit_feats_train, size_ens = size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dad93ee9-a527-4663-9e18-d72039909c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_t, initial_ensembles, initial_ensembles_for_weights = get_initial_X_t(smiles_feats_train, rdkit_feats_train, size_ens = size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c1bbad1-127a-4c27-8fb9-23783b072a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_ensembles = np.hstack((initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecfb3b01-63af-4194-8bb8-8d7dfa1a9edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 2192)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_ensembles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f598992-73d6-4b5a-906d-3184f1b9625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_t = [[1, 0, 1, 0], [0, 1, 0, 1]]\n",
    "G_t = np.array(G_t).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3073d2ce-5b22-47bb-80a3-9ad86ac3d6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a84cb0e3-2f5e-4207-b092-d2768b52205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data1, data2, initial_ensembles): \n",
    "    _,_, weighted_alogp, weighted_psa, w1, w2 = forward_operation(data1, data2, initial_ensembles, size_ens = size_ens)\n",
    "    weighted_alogp = np.expand_dims(weighted_alogp,-1)\n",
    "    weighted_psa = np.expand_dims(weighted_psa,-1)\n",
    "    preds = np.concatenate((weighted_alogp, weighted_psa),-1)\n",
    "    return preds, w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abb66e39-541a-4d16-b18c-c5eafdfeaea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_t = np.hstack((np.identity(smiles_feats_train.shape[0]), np.zeros((smiles_feats_train.shape[0], samp_ann.count_params() + 1 + 1 + 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e3cd1ba-a09f-4d84-9add-6a3e1a1ad01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3af0a9b9-6202-40e2-911f-3c890213099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mu_bar_G_bar(data1, data2, initial_ensembles):\n",
    "    H_t = np.hstack((np.identity(data1.shape[0]), np.zeros((data1.shape[0], samp_ann.count_params() + 1 + 1 + 1))))\n",
    "    mu_bar = initial_ensembles.mean(0)\n",
    "    X_t,_, _, _, _, _ = forward_operation(data1, data2, initial_ensembles, size_ens = size_ens)\n",
    "    X_t = X_t.transpose((0,2,1))\n",
    "    X_t = X_t.reshape(X_t.shape[0], X_t.shape[1]*X_t.shape[2])\n",
    "    script_H_t = np.kron(G_t.T, H_t)\n",
    "    G_u = (script_H_t@X_t.T)\n",
    "    G_u = G_u.T\n",
    "    # weighted_alogp = np.expand_dims(weighted_alogp,-1)\n",
    "    # weighted_psa = np.expand_dims(weighted_psa,-1)\n",
    "    # G_u = np.concatenate((weighted_alogp, weighted_psa), axis = -1)\n",
    "    # G_u = G_u.transpose((0,2,1))\n",
    "    # G_u = G_u.reshape(G_u.shape[0], G_u.shape[1]*G_u.shape[2])\n",
    "    # G_u\n",
    "    G_bar = (G_u.mean(0)).ravel()\n",
    "    return mu_bar.reshape(-1,1), G_bar.reshape(-1,1), G_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7334c80c-cfcd-4d5b-b844-11cfb8c137df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u): \n",
    "    u_j_minus_u_bar = initial_ensembles - mu_bar.reshape(1,-1)\n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    c = np.zeros((total_weights, G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        c += np.kron(u_j_minus_u_bar[i, :].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return c/size_ens, G_u_minus_G_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9148779c-1a3f-455d-a6ad-00fc1bf69c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_D_u( G_bar, G_u): \n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    d = np.zeros((G_bar.shape[0], G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        d += np.kron(G_u_minus_G_bar[i,:].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return d/size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e8459b0-68c1-4d1f-851e-3d7b99f0ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_ensemble(data1, data2, initial_ensembles):\n",
    "    mu_bar, G_bar, G_u = calculate_mu_bar_G_bar(data1, data2, initial_ensembles)\n",
    "    C, G_u_minus_G_bar = calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u)\n",
    "    D = calculate_D_u( G_bar, G_u)\n",
    "    _, R_t = create_cov(data1.shape[0],initial_ensembles)\n",
    "    # all_covs = np.array(all_covs)\n",
    "    D_plus_cov = D + R_t\n",
    "    D_plus_cov_inv = np.linalg.inv(D_plus_cov)\n",
    "    mid_quant = C@D_plus_cov_inv\n",
    "    right_quant = y_train.T.flatten().reshape(-1,1) - G_u.T\n",
    "    mid_times_right = mid_quant@right_quant\n",
    "    updated_ensemble = (initial_ensembles.T + mid_times_right)\n",
    "    return updated_ensemble.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87593eef-d706-40af-aab9-68e51ac6207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_ensembles[:, -8:-4].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c14b236d-6ebf-40e3-9085-97afd2990e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0745b46a-e832-4caf-ae08-4d15d9fbe7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_ensembles[:, -4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8e8c958-572e-4430-86ca-aa89819c9363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.math.softplus(np.array([0.]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1283df8-c281-48fa-82db-86888e6b6a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cov(shape, initial_ensembles):\n",
    "    cov_part = initial_ensembles[:, -8:-4]\n",
    "    cov_part = cov_part.mean(0)\n",
    "    # variances = tf.math.softplus(cov_part[:2]).numpy()\n",
    "    variances = cov_part[:2]\n",
    "    covariances = cov_part[2:]\n",
    "    base_cov = np.identity(target_dim)\n",
    "    base_cov[0,0] = variances[0]\n",
    "    base_cov[1,1] = variances[1]\n",
    "    base_cov[0,1] = covariances[0]\n",
    "    base_cov[1,0] = covariances[1]\n",
    "    \n",
    "    variances1 = tf.math.softplus(initial_ensembles[:, -4:]).numpy()\n",
    "    variances1 = variances1.mean(0)\n",
    "    base_variances = np.identity(target_dim)\n",
    "    base_variances[0,0] = variances1[0]\n",
    "    base_variances[1,1] = variances1[2]\n",
    "    \n",
    "    final = np.linalg.cholesky(base_cov@base_cov.T + base_variances)\n",
    "    cov_mat = final@final.T\n",
    "    cov_mat_final = cov_mat\n",
    "    # cov_mat_final = cov_mat@cov_mat.T\n",
    "    \n",
    "    if is_pos_def(cov_mat_final) != True:\n",
    "        print(\"resulting cov matrix is not positive semi definite\")\n",
    "        pass\n",
    "    \n",
    "    # print(np.linalg.det(cov_mat_final))\n",
    "    \n",
    "    var1 = cov_mat_final[0,0]\n",
    "    var2 = cov_mat_final[1,1]\n",
    "    cov = cov_mat_final[1,0]\n",
    "\n",
    "    n = shape\n",
    "    \n",
    "    ul = var1*np.identity(n)\n",
    "    lr = var2*np.identity(n)\n",
    "    ur = cov*np.identity(n)\n",
    "    ll = ur.T    \n",
    "    \n",
    "    first_row = np.hstack((ul, ur))\n",
    "    second_row = np.hstack((ll, lr))\n",
    "    \n",
    "    R_t = np.vstack((first_row, second_row))\n",
    "    \n",
    "    # R_t = block_diag(*([cov_mat_final] * n))\n",
    "    \n",
    "    # R_t = np.linalg.inv(R_t)\n",
    "    \n",
    "    return cov_mat_final, R_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af50f314-7841-4bc7-9764-fcf33c9b8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "242c1e9e-3745-4dc2-bab8-6fec04ea2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat_final, _ = create_cov(smiles_feats_train.shape[0],initial_ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0848e471-31ad-415d-adb4-64853aca2a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.42074659, 0.09038997],\n",
       "       [0.09038997, 1.38821502]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_mat_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc3a216a-4dbb-474d-8219-5f6c071250a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(data, idx):\n",
    "    data_cur = data[idx, :, :]\n",
    "    inv_data_cur = std_targets.inverse_transform(data_cur)\n",
    "    return inv_data_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5abd35fb-ea92-449d-894a-162ed1f4aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58e18b92-bba1-4a96-9abf-866b8594d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66cdaaa0-cd22-42b1-87ee-968e96aa3ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 2192)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_ensembles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "417c025e-9f73-48ec-ac9f-664b42306c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65bf21e9-92c6-4824-93c3-87274b072afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n",
      "Averaging Weight\n",
      "[0.549219903546435, 0.34326577083336923]\n",
      "Weight Norms\n",
      "[111.64530723625504, 14.981043217043508]\n",
      "about_R_t\n",
      "[1.21568509 1.24421355] 0.034819328004890765\n",
      "RMSEs, Coverages, and Widths\n",
      "[  9.01639689 246.26607802] [1. 1.] [  843.38509996 19958.81609396]\n",
      "\n",
      "\n",
      "Epoch2\n",
      "Averaging Weight\n",
      "[0.6099697772135939, 0.32105007474357256]\n",
      "Weight Norms\n",
      "[99.42697300189623, 18.23701478712934]\n",
      "about_R_t\n",
      "[1.12294782 1.16359865] -0.016642519243115078\n",
      "RMSEs, Coverages, and Widths\n",
      "[  8.76744944 119.97758038] [1. 1.] [  545.80165657 12647.58356481]\n",
      "\n",
      "\n",
      "Epoch3\n",
      "Averaging Weight\n",
      "[0.6817865525498755, 0.2778043215404494]\n",
      "Weight Norms\n",
      "[90.23178388673004, 21.66886731816607]\n",
      "about_R_t\n",
      "[1.12566927 1.08179497] -0.03733912134093871\n",
      "RMSEs, Coverages, and Widths\n",
      "[  5.79981686 148.38552213] [1. 1.] [ 342.30282768 8269.3180137 ]\n",
      "\n",
      "\n",
      "Epoch4\n",
      "Averaging Weight\n",
      "[0.7471220657916111, 0.21757004285930448]\n",
      "Weight Norms\n",
      "[82.46793521765831, 23.547734039686667]\n",
      "about_R_t\n",
      "[1.0844056  1.02215054] -0.0409773713590497\n",
      "RMSEs, Coverages, and Widths\n",
      "[  4.21003544 118.97605977] [1. 1.] [ 226.90292373 5466.31395313]\n",
      "\n",
      "\n",
      "Epoch5\n",
      "Averaging Weight\n",
      "[0.8251059765045429, 0.15464471099849278]\n",
      "Weight Norms\n",
      "[75.8475034851562, 23.760230420735184]\n",
      "about_R_t\n",
      "[1.11372761 1.00653676] -0.040154311949638734\n",
      "RMSEs, Coverages, and Widths\n",
      "[  3.81942437 118.84756083] [1. 1.] [ 160.82606827 3745.85899567]\n",
      "\n",
      "\n",
      "Epoch6\n",
      "Averaging Weight\n",
      "[0.8698828659836975, 0.11392856297970175]\n",
      "Weight Norms\n",
      "[69.178011494817, 22.780984604381235]\n",
      "about_R_t\n",
      "[1.02997267 0.90142755] -0.012678042044796629\n",
      "RMSEs, Coverages, and Widths\n",
      "[  3.43513857 127.52323416] [1. 1.] [ 118.1170606  2800.36121531]\n",
      "\n",
      "\n",
      "Epoch7\n",
      "Averaging Weight\n",
      "[0.9086565343619379, 0.0713652694913678]\n",
      "Weight Norms\n",
      "[63.3307007626023, 21.801310013894767]\n",
      "about_R_t\n",
      "[1.0414788  0.94933105] 0.01715425561327963\n",
      "RMSEs, Coverages, and Widths\n",
      "[  2.68383246 114.17757633] [1. 1.] [  89.01121925 2218.81277278]\n",
      "\n",
      "\n",
      "Epoch8\n",
      "Averaging Weight\n",
      "[0.9256414698017265, 0.051496725069765596]\n",
      "Weight Norms\n",
      "[57.974384811611344, 19.160692716902943]\n",
      "about_R_t\n",
      "[1.08876558 1.02143487] 0.06362353469742485\n",
      "RMSEs, Coverages, and Widths\n",
      "[  2.72850482 110.4838056 ] [1. 1.] [  65.21561566 1737.22444141]\n",
      "\n",
      "\n",
      "Epoch9\n",
      "Averaging Weight\n",
      "[0.9303707352180859, 0.04341160938004556]\n",
      "Weight Norms\n",
      "[53.46518296983198, 16.365330788162662]\n",
      "about_R_t\n",
      "[1.12454484 1.10640497] 0.04530014884141377\n",
      "RMSEs, Coverages, and Widths\n",
      "[ 2.46339403 96.4533514 ] [1. 1.] [  50.92304884 1296.05782254]\n",
      "\n",
      "\n",
      "Epoch10\n",
      "Averaging Weight\n",
      "[0.9373729652649678, 0.036375674371400886]\n",
      "Weight Norms\n",
      "[49.43203180228627, 14.813206786257163]\n",
      "about_R_t\n",
      "[1.06126933 1.01613119] 0.025280376051690766\n",
      "RMSEs, Coverages, and Widths\n",
      "[  1.84583365 106.44180948] [1. 1.] [  40.30871685 1005.52460462]\n",
      "\n",
      "\n",
      "Epoch11\n",
      "Averaging Weight\n",
      "[0.9452462145872802, 0.030373881307493276]\n",
      "Weight Norms\n",
      "[46.187524283083015, 13.05487223726535]\n",
      "about_R_t\n",
      "[1.04765191 1.05717771] -0.03940104612389133\n",
      "RMSEs, Coverages, and Widths\n",
      "[ 1.86568363 74.6551305 ] [1. 1.] [ 32.52559789 792.26595215]\n",
      "\n",
      "\n",
      "Epoch12\n",
      "Averaging Weight\n",
      "[0.9432629744783672, 0.028748281295459265]\n",
      "Weight Norms\n",
      "[43.639857122278535, 10.739521087849038]\n",
      "about_R_t\n",
      "[1.10530926 1.21154573] -0.12146453572412136\n",
      "RMSEs, Coverages, and Widths\n",
      "[ 1.47750142 60.83342059] [1. 1.] [ 26.53776424 632.89726791]\n",
      "\n",
      "\n",
      "Epoch13\n",
      "Averaging Weight\n",
      "[0.9454520653529385, 0.024819772839764505]\n",
      "Weight Norms\n",
      "[40.82866440229501, 9.070996537740653]\n",
      "about_R_t\n",
      "[0.91190151 1.15928742] -0.001200081960811646\n",
      "RMSEs, Coverages, and Widths\n",
      "[ 0.97037931 40.45457082] [1. 1.] [ 20.70344938 530.18930151]\n",
      "\n",
      "\n",
      "Epoch14\n",
      "Averaging Weight\n",
      "[0.9397404861622844, 0.024728487319110648]\n",
      "Weight Norms\n",
      "[38.03878148500042, 7.20592640270716]\n",
      "about_R_t\n",
      "[0.88121746 1.29635702] -0.00928145669983007\n",
      "RMSEs, Coverages, and Widths\n",
      "[ 0.93192901 37.9638689 ] [1. 1.] [ 17.23172398 411.93557686]\n",
      "\n",
      "\n",
      "Epoch15\n",
      "Averaging Weight\n",
      "[0.9403109081444982, 0.020957654093266973]\n",
      "Weight Norms\n",
      "[35.43668293625179, 5.870888805665418]\n",
      "about_R_t\n",
      "[0.90763304 1.45544663] 0.017755685874226997\n",
      "RMSEs, Coverages, and Widths\n",
      "[ 0.69433355 32.71922311] [1. 1.] [ 13.31051018 280.91526256]\n",
      "\n",
      "\n",
      "Epoch16\n",
      "Averaging Weight\n",
      "[0.9421325218295122, 0.018183890885742973]\n",
      "Weight Norms\n",
      "[33.415098591584005, 4.768455022239175]\n",
      "about_R_t\n",
      "[0.93216957 1.47219315] 0.01833825474414047\n",
      "RMSEs, Coverages, and Widths\n",
      "[ 0.64176239 24.41619036] [1. 1.] [ 10.15367956 196.00748605]\n",
      "\n",
      "\n",
      "Epoch17\n",
      "Averaging Weight\n",
      "[0.9435691611555629, 0.016622097142084485]\n",
      "Weight Norms\n",
      "[31.943017205042405, 4.045748538138901]\n",
      "about_R_t\n",
      "[0.98654086 1.53780918] -0.03267939163171547\n",
      "RMSEs, Coverages, and Widths\n",
      "[ 0.64258389 20.27977688] [1. 1.] [  7.90452308 144.82868584]\n",
      "\n",
      "\n",
      "Epoch18\n",
      "Averaging Weight\n",
      "[0.9418980097648132, 0.01541229321679651]\n",
      "Weight Norms\n",
      "[30.905544985421503, 3.581711660818759]\n",
      "about_R_t\n",
      "[1.0465928  1.59551509] -0.09178533247339941\n",
      "RMSEs, Coverages, and Widths\n",
      "[ 0.32079332 15.4877087 ] [1. 1.] [  5.90705003 107.34047872]\n",
      "\n",
      "\n",
      "Epoch19\n",
      "Averaging Weight\n",
      "[0.9400191008654283, 0.014516903748353372]\n",
      "Weight Norms\n",
      "[30.018379775187118, 3.2899448331766763]\n",
      "about_R_t\n",
      "[1.09593927 1.54529051] -0.10745284748925354\n",
      "RMSEs, Coverages, and Widths\n",
      "[ 0.33236837 13.65194671] [1. 1.] [ 4.60956964 84.01304722]\n",
      "\n",
      "\n",
      "Epoch20\n",
      "Averaging Weight\n",
      "[0.9380146723654141, 0.014049970955067723]\n",
      "Weight Norms\n",
      "[29.22546137681137, 2.9534914722650396]\n",
      "about_R_t\n",
      "[1.09394621 1.48922557] -0.10963042033620315\n",
      "RMSEs, Coverages, and Widths\n",
      "[ 0.37088685 12.48167894] [1. 1.] [ 3.81722266 67.53907766]\n",
      "\n",
      "\n",
      "Epoch21\n",
      "Averaging Weight\n",
      "[0.9352608039369249, 0.013817006523976716]\n",
      "Weight Norms\n",
      "[28.534113047618366, 2.714261686240304]\n",
      "about_R_t\n",
      "[1.08319038 1.47519344] -0.11861118669527643\n",
      "RMSEs, Coverages, and Widths\n",
      "[0.37035924 8.59963361] [1. 1.] [ 3.01271883 56.16066131]\n",
      "\n",
      "\n",
      "Epoch22\n",
      "Averaging Weight\n",
      "[0.9337641611217975, 0.013408403337581505]\n",
      "Weight Norms\n",
      "[27.911596894226534, 2.4882233602901205]\n",
      "about_R_t\n",
      "[1.10181816 1.47595562] -0.08187269907257065\n",
      "RMSEs, Coverages, and Widths\n",
      "[0.32384718 6.33765137] [1. 1.] [ 2.49175448 47.98794709]\n",
      "\n",
      "\n",
      "Epoch23\n",
      "Averaging Weight\n",
      "[0.9323782535012286, 0.01296467556702912]\n",
      "Weight Norms\n",
      "[27.348624038333373, 2.284995501850851]\n",
      "about_R_t\n",
      "[1.12686562 1.44898112] -0.0950474873906647\n",
      "RMSEs, Coverages, and Widths\n",
      "[0.17943046 4.85014674] [1. 1.] [ 2.02724864 42.23611493]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m weight_norm_sd \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     initial_ensembles \u001b[38;5;241m=\u001b[39m \u001b[43mget_updated_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles_feats_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrdkit_feats_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_ensembles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     G_u_train, w1, w2 \u001b[38;5;241m=\u001b[39m get_predictions(smiles_feats_train, rdkit_feats_train, initial_ensembles)\n\u001b[1;32m     12\u001b[0m     w1_catch\u001b[38;5;241m.\u001b[39mappend(w1\u001b[38;5;241m.\u001b[39mmean())\n",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m, in \u001b[0;36mget_updated_ensemble\u001b[0;34m(data1, data2, initial_ensembles)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_updated_ensemble\u001b[39m(data1, data2, initial_ensembles):\n\u001b[1;32m      2\u001b[0m     mu_bar, G_bar, G_u \u001b[38;5;241m=\u001b[39m calculate_mu_bar_G_bar(data1, data2, initial_ensembles)\n\u001b[0;32m----> 3\u001b[0m     C, G_u_minus_G_bar \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_C_u\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_ensembles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_u\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     D \u001b[38;5;241m=\u001b[39m calculate_D_u( G_bar, G_u)\n\u001b[1;32m      5\u001b[0m     _, R_t \u001b[38;5;241m=\u001b[39m create_cov(data1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],initial_ensembles)\n",
      "Cell \u001b[0;32mIn[38], line 6\u001b[0m, in \u001b[0;36mcalculate_C_u\u001b[0;34m(initial_ensembles, mu_bar, G_bar, G_u)\u001b[0m\n\u001b[1;32m      4\u001b[0m c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((total_weights, G_bar\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, size_ens): \n\u001b[0;32m----> 6\u001b[0m     c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_j_minus_u_bar\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_u_minus_G_bar\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\u001b[38;5;241m/\u001b[39msize_ens, G_u_minus_G_bar\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mkron\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/enkf/lib/python3.10/site-packages/numpy/lib/shape_base.py:1157\u001b[0m, in \u001b[0;36mkron\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1155\u001b[0m axis \u001b[38;5;241m=\u001b[39m nd\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nd):\n\u001b[0;32m-> 1157\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m wrapper \u001b[38;5;241m=\u001b[39m get_array_prepare(a, b)\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w1_catch = []\n",
    "w2_catch = []\n",
    "w1_sd_catch = []\n",
    "w2_sd_catch = []\n",
    "weight_norm_mean = []\n",
    "weight_norm_sd = []\n",
    "for i in range(0,10000):\n",
    "    \n",
    "    initial_ensembles = get_updated_ensemble(smiles_feats_train, rdkit_feats_train, initial_ensembles)\n",
    "    G_u_train, w1, w2 = get_predictions(smiles_feats_train, rdkit_feats_train, initial_ensembles)\n",
    "    \n",
    "    w1_catch.append(w1.mean())\n",
    "    w1_sd_catch.append(w1.std())\n",
    "    \n",
    "    w2_catch.append(w2.mean())\n",
    "    w2_sd_catch.append(w2.std())  \n",
    "    \n",
    "    print(\"Epoch\" + str(i+1))\n",
    "    \n",
    "    \n",
    "    print(\"Averaging Weight\")\n",
    "    print([w1.mean(), w1.std()])\n",
    "    # print(w2.mean(), w2.std())\n",
    "    \n",
    "    # G_u_train = get_targets_with_weights(smiles_feats_train, rdkit_feats_train, initial_ensembles, size_ens = size_ens)\n",
    "    catch = Parallel(n_jobs = 15, verbose = 0)(delayed(inverse_transform)(G_u_train, i)  for i in range(G_u_train.shape[0]))\n",
    "    G_u_train = np.array(catch)\n",
    "    \n",
    "    y_train_cur = std_targets.inverse_transform(y_train)\n",
    "    \n",
    "    li_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[0,:,:]   \n",
    "    ui_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "    \n",
    "    width_train = ui_train - li_train\n",
    "    avg_width_train = width_train.mean(0)\n",
    "    \n",
    "    ind_train = (y_train_cur >= li_train) & (y_train_cur <= ui_train)\n",
    "    coverage_train= ind_train.mean(0)\n",
    "    \n",
    "    averaged_targets_train = G_u_train.mean(0)\n",
    "    rmse_train = np.sqrt(((y_train_cur -averaged_targets_train)**2).mean(0))\n",
    "    # print(rmse_train, coverage_train, avg_width_train)\n",
    "    \n",
    "    G_u_test, _, _ = get_predictions(smiles_feats_valid, rdkit_feats_valid, initial_ensembles)\n",
    "    \n",
    "    catch = Parallel(n_jobs = 15, verbose = 0)(delayed(inverse_transform)(G_u_test, i)  for i in range(G_u_test.shape[0]))\n",
    "    G_u_test = np.array(catch)\n",
    "    \n",
    "    y_valid_cur = std_targets.inverse_transform(y_valid)    \n",
    "    \n",
    "    li_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[0,:,:]   \n",
    "    ui_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "    \n",
    "    width_test = ui_test - li_test\n",
    "    avg_width_test = width_test.mean(0)\n",
    "    \n",
    "    ind_test = (y_valid_cur >= li_test) & (y_valid_cur <= ui_test)\n",
    "    coverage_test= ind_test.mean(0)\n",
    "    \n",
    "    averaged_targets_test = G_u_test.mean(0)\n",
    "    rmse_test = np.sqrt(((y_valid_cur -averaged_targets_test)**2).mean(0))    \n",
    "    \n",
    "    weight_norms = np.array(norm(initial_ensembles, ord = 2, axis = 1))\n",
    "    weight_norm_mean.append(weight_norms.mean())\n",
    "    weight_norm_sd.append(weight_norms.std())\n",
    "    \n",
    "   \n",
    "    print(\"Weight Norms\")\n",
    "    print([weight_norms.mean(), weight_norms.std()])\n",
    "    \n",
    "    \n",
    "    if np.mean(coverage_train < 0.95) == 1:\n",
    "        break\n",
    "        \n",
    "    cov_mat_final, _ = create_cov(smiles_feats_train.shape[0],initial_ensembles)\n",
    "    \n",
    "    print(\"about_R_t\")\n",
    "    print(np.diag(cov_mat_final), cov_mat_final[0,1])\n",
    "    \n",
    "    print(\"RMSEs, Coverages, and Widths\")\n",
    "    print(rmse_test, coverage_test, avg_width_test)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81eb6c3-e084-4e43-aa5e-3f9b0f5ee016",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_u_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8df43-9ae3-47f6-9c69-ac77c38a6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47207771-df91-48f0-9963-9a78989bcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.sample(range(y_valid_cur.shape[0]), k = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f17ea-a47c-421d-bd32-7a0dc0f1af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8, 2,figsize=(15, 15))\n",
    "# axs = axs.ravel()\n",
    "# counter = 0\n",
    "for idx, i in enumerate(random_idx):\n",
    "    # print(counter)\n",
    "    truth = y_valid_cur[i,:]\n",
    "    preds = G_u_test[:, i,:]\n",
    "    percts = np.percentile(preds, axis = 0, q = (2.5, 97.5))\n",
    "    lis = percts[0,:]\n",
    "    uis = percts[1,:]\n",
    "    \n",
    "    \n",
    "    axs[idx, 0].hist(preds[:,0])\n",
    "    axs[idx, 0].axvline(truth[0], color='green', linewidth=2)\n",
    "    axs[idx, 0].axvline(lis[0], color='red', linewidth=2)\n",
    "    axs[idx, 0].axvline(uis[0], color='red', linewidth=2)\n",
    "    \n",
    "    axs[idx, 1].hist(preds[:,1])\n",
    "    axs[idx, 1].axvline(truth[1], color='green', linewidth=2)\n",
    "    axs[idx, 1].axvline(lis[1], color='red', linewidth=2)\n",
    "    axs[idx, 1].axvline(uis[1], color='red', linewidth=2)\n",
    "    \n",
    "    # counter+=2\n",
    "    # print(counter)\n",
    "    \n",
    "    # plt.show()\n",
    "plt.savefig('prediction_intervals.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230a62b-f36f-4ec0-9c7d-0ba75c9de369",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_valid_cur[:, 0], averaged_targets_test[:,0])\n",
    "plt.axline((0,0), slope = 1, c= \"black\")\n",
    "plt.show()\n",
    "plt.scatter(y_valid_cur[:,1], averaged_targets_test[:, 1])\n",
    "plt.axline((0,0), slope = 1, c= \"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8d1f1-5b52-48eb-997c-0a760a8729a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w1_catch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa904e3-6452-4443-93ef-3c54377ff059",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w1_sd_catch)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a36715c-20fa-406e-9ac6-55a56152d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(range(0, len(w1_catch)), w1_catch, w1_sd_catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a18b9e-6388-4b45-b46b-e7ecd3ba8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(range(0, len(weight_norm_mean)), weight_norm_mean, weight_norm_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a7783b-b135-41e1-8ae7-6a346965daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat_final, _ = create_cov(smiles_feats_train.shape[0],initial_ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d891012-f14b-40fa-b8e4-8bf97bac5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace7c78-5947-4803-922b-9848a6c46bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd1fd9-89cb-4a66-896c-62f8805c0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(y_train.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
