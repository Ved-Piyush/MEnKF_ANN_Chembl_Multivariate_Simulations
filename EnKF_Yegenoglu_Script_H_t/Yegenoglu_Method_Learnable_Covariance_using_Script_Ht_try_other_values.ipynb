{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa53e19a-64db-4b1d-9378-c19c8efacd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 18:25:39.877473: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 18:25:39.980934: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-12 18:25:39.985214: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-12 18:25:39.985226: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-12 18:25:40.919594: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-12 18:25:40.921542: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-12 18:25:40.921551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import block_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d459b853-f520-4181-a9a9-310a6dd20de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_with_weights(batch_data, initial_ensembles, size_ens, ann): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a66ee4c-4112-4dca-a9cc-68a60d4e316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(hidden = 32, input_shape = 256, output_shape = 1): \n",
    "    input_layer = tf.keras.layers.Input(shape = (input_shape))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(output_shape, activation = \"relu\")\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b9ef03-af61-4ce4-99ff-2f0b5dca639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_ensembles(num_weights, lambda1, size_ens):\n",
    "    mean_vec = np.zeros((num_weights,))\n",
    "    cov_matrix = lambda1*np.identity(num_weights)\n",
    "    mvn_samp = mvn(mean_vec, cov_matrix)\n",
    "    return mvn_samp.rvs(size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2ea2b5-13a4-41c4-8257-c6c18708501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expit(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0623822d-59a8-44f0-948a-e23964559343",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f741d7-ba99-4803-8441-5bf2ad3d72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_X_t(data1, data2, size_ens, var_weights = 1.0):\n",
    "    samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    \n",
    "    initial_ensembles1 = generate_initial_ensembles(samp_ann.count_params(), var_weights, size_ens)\n",
    "    data1_out1, data1_stack1 = get_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens, ann = samp_ann)\n",
    "    \n",
    "    initial_ensembles2 = generate_initial_ensembles(samp_ann.count_params(), var_weights, size_ens)\n",
    "    data1_out2, data1_stack2 = get_targets_with_weights(data1, initial_ensembles2, size_ens = size_ens, ann = samp_ann)\n",
    "    \n",
    "    initial_ensembles3 = generate_initial_ensembles(samp_ann.count_params(), var_weights, size_ens)\n",
    "    data2_out1, data2_stack1 = get_targets_with_weights(data2, initial_ensembles3, size_ens = size_ens, ann = samp_ann)\n",
    "    \n",
    "    initial_ensembles4 = generate_initial_ensembles(samp_ann.count_params(), var_weights, size_ens)\n",
    "    data2_out2, data2_stack2 = get_targets_with_weights(data2, initial_ensembles4, size_ens = size_ens, ann = samp_ann)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles_for_weights = generate_initial_ensembles(4, var_weights, size_ens)\n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_L = generate_initial_ensembles(4, var_weights, size_ens)\n",
    "    initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)    \n",
    "    \n",
    "    initial_ensembles_for_D1 = generate_initial_ensembles(1, var_weights, size_ens).reshape(-1,1)\n",
    "    initial_ensembles_for_D2 = generate_initial_ensembles(1, var_weights, size_ens).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D1_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    initial_ensembles_for_D2_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.concatenate((np.expand_dims(initial_ensembles_for_D1,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D1_zero,1), \n",
    "                                                      np.expand_dims(initial_ensembles_for_D2,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D2_zero,1)), axis = 2)\n",
    "    \n",
    "    # print(X_t.shape, initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4))\n",
    "    \n",
    "    return X_t, initial_ensembles, initial_ensembles_for_weights[:,0,:], initial_ensembles_for_L[:,0,:], initial_ensembles_for_D[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac49c8c-6da0-4ee2-9561-e1d19365f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_targets_with_weights(batch_data, initial_ensembles, size_ens, ann, weights): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    final_output_1 = final_output_1*weights\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f382d3e-e4dc-4e19-b19a-0eb954236a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "alogp_bottleneck = np.load(\"..//Data/small_mol_phase_3_features_for_both.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e5e3f5-e4c6-40f8-ac40-b487543e3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = np.load('..//Data/smiles_0.7_rdkit_0.3_outputs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b697b7f9-3d50-4d71-a436-cdc6434909a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/statgrads/vpiyush2/.conda/envs/enkf/lib/python3.10/site-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator StandardScaler from version 1.1.2 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "std_targets = pickle.load( open('..//Data//target_scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7f85a16-abcf-4c61-8635-b47b6b1881a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_valid = pd.read_csv(\"..//Data/smiles_with_rdkit_with_small_phase_3_outputs.csv\")\n",
    "# y_train = y_valid.values[:,1:]\n",
    "# y_train = std_targets.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ad7f00f-1bd3-40bc-b50b-de0406c72655",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_valid\n",
    "# y_train = y_train = std_targets.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e49a5313-fa38-44e9-bb29-07b461c12162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.cov(y_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3fe10a7-fcc3-430f-8ade-356a7f7b85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = std_targets.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dfffa08-081b-4b5f-8181-ae94a920c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f03896e4-b7c8-4237-a0a8-7af9ed6b1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(alogp_bottleneck, y_train, test_size = 0.25, shuffle = True, \n",
    "                                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2ed02ca-e8a1-4767-8152-4fc17b1afe61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(719, 64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b98c57b-e85d-46f9-92b4-09f2a1b179a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_feats_train = x_train[:, :32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b91f4be-60b4-44ee-b992-80da2cbe5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdkit_feats_train = x_train[:, 32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfe0b04f-b7c9-4f72-ac4b-4ba7e50d128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_feats_valid = x_valid[:, :32]\n",
    "rdkit_feats_valid = x_valid[:, 32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5932b6c7-8781-4514-8b67-1e760609c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_ens = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d1a4f5e-7a0d-42b7-ad45-59c708b7b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation(data1, data2, combined_ensembles , size_ens ):\n",
    "    samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann.count_params()\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4 + 4):(4*params + 4 + 4 + 4)]\n",
    "    \n",
    "    \n",
    "    softmax_weights = tf.math.softmax(initial_ensembles_for_weights).numpy()\n",
    "    \n",
    "    model_1 = softmax_weights[:,:2].sum(1).reshape(-1,1)\n",
    "    \n",
    "    model_2 = softmax_weights[:,2:].sum(1).reshape(-1,1)\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                 ann = samp_ann, weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data1, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                 ann = samp_ann, weights=model_1)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data2, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 ann = samp_ann, weights=model_2)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                 ann = samp_ann, weights=model_2)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    weighted_alogp = data1_out1 + data2_out1\n",
    "    \n",
    "    weighted_psa = data1_out2 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles, weighted_alogp, weighted_psa, model_1, model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e15b0bf7-ba00-40bc-933b-c1c3e062e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 18:25:42.513506: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\n",
      "2023-06-12 18:25:42.513529: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-12 18:25:42.513542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c2518.crane.hcc.unl.edu): /proc/driver/nvidia/version does not exist\n",
      "2023-06-12 18:25:42.513725: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af3dd570-7ab3-4eae-bd8e-9a57a333b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = 4*(samp_ann.count_params() + 1 + 1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fcac026-31e1-4d16-8bea-f9cd8a99c51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2192"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e820b010-4396-4b7a-8781-a8e6d503aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5fbaaf7-afae-4008-a26c-e0691ef9b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens = total_weights//reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5111536-1a97-4390-9658-c9848a136a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a59f4669-65d3-41df-8cd9-79ebdecb9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D = get_initial_X_t(smiles_feats_train, rdkit_feats_train, size_ens = size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c1bbad1-127a-4c27-8fb9-23783b072a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_ensembles = np.hstack((initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_L, initial_ensembles_for_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecfb3b01-63af-4194-8bb8-8d7dfa1a9edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 2192)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_ensembles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f598992-73d6-4b5a-906d-3184f1b9625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_t = [[1, 0, 1, 0], [0, 1, 0, 1]]\n",
    "G_t = np.array(G_t).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3073d2ce-5b22-47bb-80a3-9ad86ac3d6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a84cb0e3-2f5e-4207-b092-d2768b52205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data1, data2, initial_ensembles): \n",
    "    _,_, weighted_alogp, weighted_psa, w1, w2 = forward_operation(data1, data2, initial_ensembles, size_ens = size_ens)\n",
    "    weighted_alogp = np.expand_dims(weighted_alogp,-1)\n",
    "    weighted_psa = np.expand_dims(weighted_psa,-1)\n",
    "    preds = np.concatenate((weighted_alogp, weighted_psa),-1)\n",
    "    return preds, w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3af0a9b9-6202-40e2-911f-3c890213099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mu_bar_G_bar(data1, data2, initial_ensembles):\n",
    "    H_t = np.hstack((np.identity(data1.shape[0]), np.zeros((data1.shape[0], samp_ann.count_params() + 1 + 1 + 1))))\n",
    "    mu_bar = initial_ensembles.mean(0)\n",
    "    X_t,_, _, _, _, _ = forward_operation(data1, data2, initial_ensembles, size_ens = size_ens)\n",
    "    X_t = X_t.transpose((0,2,1))\n",
    "    X_t = X_t.reshape(X_t.shape[0], X_t.shape[1]*X_t.shape[2])\n",
    "    script_H_t = np.kron(G_t.T, H_t)\n",
    "    G_u = (script_H_t@X_t.T)\n",
    "    G_u = G_u.T\n",
    "    # weighted_alogp = np.expand_dims(weighted_alogp,-1)\n",
    "    # weighted_psa = np.expand_dims(weighted_psa,-1)\n",
    "    # G_u = np.concatenate((weighted_alogp, weighted_psa), axis = -1)\n",
    "    # G_u = G_u.transpose((0,2,1))\n",
    "    # G_u = G_u.reshape(G_u.shape[0], G_u.shape[1]*G_u.shape[2])\n",
    "    # G_u\n",
    "    G_bar = (G_u.mean(0)).ravel()\n",
    "    return mu_bar.reshape(-1,1), G_bar.reshape(-1,1), G_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7334c80c-cfcd-4d5b-b844-11cfb8c137df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u): \n",
    "    u_j_minus_u_bar = initial_ensembles - mu_bar.reshape(1,-1)\n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    c = np.zeros((total_weights, G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        c += np.kron(u_j_minus_u_bar[i, :].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return c/size_ens, G_u_minus_G_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9148779c-1a3f-455d-a6ad-00fc1bf69c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_D_u( G_bar, G_u): \n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    d = np.zeros((G_bar.shape[0], G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        d += np.kron(G_u_minus_G_bar[i,:].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return d/size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e8459b0-68c1-4d1f-851e-3d7b99f0ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_ensemble(data1, data2, initial_ensembles):\n",
    "    mu_bar, G_bar, G_u = calculate_mu_bar_G_bar(data1, data2, initial_ensembles)\n",
    "    C, G_u_minus_G_bar = calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u)\n",
    "    D = calculate_D_u( G_bar, G_u)\n",
    "    _, R_t = create_cov(data1.shape[0],initial_ensembles)\n",
    "    # all_covs = np.array(all_covs)\n",
    "    D_plus_cov = D + R_t\n",
    "    D_plus_cov_inv = np.linalg.inv(D_plus_cov)\n",
    "    mid_quant = C@D_plus_cov_inv\n",
    "    right_quant = y_train.T.flatten().reshape(-1,1) - G_u.T\n",
    "    mid_times_right = mid_quant@right_quant\n",
    "    updated_ensemble = (initial_ensembles.T + mid_times_right)\n",
    "    return updated_ensemble.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c14b236d-6ebf-40e3-9085-97afd2990e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1283df8-c281-48fa-82db-86888e6b6a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cov(shape, initial_ensembles):\n",
    "    cov_part = initial_ensembles[:, -8:-4]\n",
    "    cov_part = cov_part.mean(0)\n",
    "    # variances = tf.math.softplus(cov_part[:2]).numpy()\n",
    "    variances = cov_part[:2]\n",
    "    covariances = cov_part[2:]\n",
    "    base_cov = np.identity(target_dim)\n",
    "    base_cov[0,0] = variances[0]\n",
    "    base_cov[1,1] = variances[1]\n",
    "    base_cov[0,1] = covariances[0]\n",
    "    base_cov[1,0] = covariances[1]\n",
    "    \n",
    "    variances1 = tf.math.softplus(initial_ensembles[:, -4:]).numpy()\n",
    "    variances1 = variances1.mean(0)\n",
    "    base_variances = np.identity(target_dim)\n",
    "    base_variances[0,0] = variances1[0]\n",
    "    base_variances[1,1] = variances1[2]\n",
    "    \n",
    "    final = np.linalg.cholesky(base_cov@base_cov.T + base_variances)\n",
    "    cov_mat = final@final.T\n",
    "    cov_mat_final = cov_mat\n",
    "    # cov_mat_final = cov_mat@cov_mat.T\n",
    "    \n",
    "    if is_pos_def(cov_mat_final) != True:\n",
    "        print(\"resulting cov matrix is not positive semi definite\")\n",
    "        pass\n",
    "    \n",
    "    # print(np.linalg.det(cov_mat_final))\n",
    "    \n",
    "    var1 = cov_mat_final[0,0]\n",
    "    var2 = cov_mat_final[1,1]\n",
    "    cov = cov_mat_final[1,0]\n",
    "\n",
    "    n = shape\n",
    "    \n",
    "    ul = var1*np.identity(n)\n",
    "    lr = var2*np.identity(n)\n",
    "    ur = cov*np.identity(n)\n",
    "    ll = ur.T    \n",
    "    \n",
    "    first_row = np.hstack((ul, ur))\n",
    "    second_row = np.hstack((ll, lr))\n",
    "    \n",
    "    R_t = np.vstack((first_row, second_row))\n",
    "    \n",
    "    # R_t = block_diag(*([cov_mat_final] * n))\n",
    "    \n",
    "    # R_t = np.linalg.inv(R_t)\n",
    "    \n",
    "    return cov_mat_final, R_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af50f314-7841-4bc7-9764-fcf33c9b8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "242c1e9e-3745-4dc2-bab8-6fec04ea2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat_final, _ = create_cov(smiles_feats_train.shape[0],initial_ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0848e471-31ad-415d-adb4-64853aca2a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.79902867e-01, 4.17835077e-04],\n",
       "       [4.17835077e-04, 7.41302057e-01]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_mat_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc3a216a-4dbb-474d-8219-5f6c071250a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(data, idx):\n",
    "    data_cur = data[idx, :, :]\n",
    "    inv_data_cur = std_targets.inverse_transform(data_cur)\n",
    "    return inv_data_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5abd35fb-ea92-449d-894a-162ed1f4aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58e18b92-bba1-4a96-9abf-866b8594d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66cdaaa0-cd22-42b1-87ee-968e96aa3ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 2192)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_ensembles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9148940e-835a-41e2-89d6-1daf1afa3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_chk_full = pd.read_csv(\"..//Data/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfa93801-4360-4403-a813-9a3f1867bf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1694445, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_chk_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd92965c-9cf5-4ee7-9769-a52a0de67784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   3.56194622,  -24.68021802],\n",
       "       [ -24.68021802, 1862.41330966]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(y_chk_full.values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65bf21e9-92c6-4824-93c3-87274b072afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Averaging Weight\n",
      "[0.5326151751382938, 0.18755218238845803]\n",
      "Weight Norms\n",
      "[39.7507074247841, 3.5662348820238243]\n",
      "standardized_scale_R_t\n",
      "[0.75421867 0.72119953] 0.002872827131236187\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 3.07797849 58.57155494] [1. 1.] [ 107.48844624 2607.40483423]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 3.29411388 58.19130343] [1. 1.] [ 106.11962977 2568.35170026]\n",
      "data_scale_R_t\n",
      "[[  1026.85395256    708.84384953]\n",
      " [   708.84384953 579046.80743386]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Averaging Weight\n",
      "[0.5398174341620299, 0.16945639528601447]\n",
      "Weight Norms\n",
      "[36.11701905231117, 3.6792231844926775]\n",
      "standardized_scale_R_t\n",
      "[0.74736182 0.72943055] -0.012045701905036198\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.46366315 50.98184485] [1. 1.] [  82.13640897 1871.67873875]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.76494676 50.19946162] [1. 1.] [  81.4920868  1849.94302779]\n",
      "data_scale_R_t\n",
      "[[   596.25043344   1213.10245046]\n",
      " [  1213.10245046 315740.29999096]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Averaging Weight\n",
      "[0.5311175448470311, 0.15781459186251365]\n",
      "Weight Norms\n",
      "[33.810003101000646, 3.6030153965440803]\n",
      "standardized_scale_R_t\n",
      "[0.7106852  0.74023041] -0.003924278821190241\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.91576066 47.7154056 ] [1. 1.] [  71.2653603  1543.31067972]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 3.17404218 46.44484929] [1. 1.] [  70.0269367  1527.04408447]\n",
      "data_scale_R_t\n",
      "[[   450.8858178     467.24954719]\n",
      " [   467.24954719 225096.08089921]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Averaging Weight\n",
      "[0.5459535352338075, 0.15057415290232123]\n",
      "Weight Norms\n",
      "[32.09683115942663, 3.246216414720801]\n",
      "standardized_scale_R_t\n",
      "[0.71537869 0.72666126] -0.0070169983601975254\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.61125879 46.06237815] [1. 1.] [  60.59777348 1323.7204911 ]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.86592049 45.2555708 ] [1. 1.] [  60.62564487 1313.88007696]\n",
      "data_scale_R_t\n",
      "[[   325.88134736    650.6590336 ]\n",
      " [   650.6590336  163850.0072933 ]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "Averaging Weight\n",
      "[0.5512347426052071, 0.14400818282997938]\n",
      "Weight Norms\n",
      "[30.74844482319052, 3.213419037876464]\n",
      "standardized_scale_R_t\n",
      "[0.73200608 0.69517788] -0.008133637466303583\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.50852898 47.3214047 ] [1. 1.] [  53.33990312 1166.8090307 ]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.768256   45.81779785] [1. 1.] [  52.62271903 1149.46443463]\n",
      "data_scale_R_t\n",
      "[[   249.47168366    299.43796493]\n",
      " [   299.43796493 126115.98009442]]\n",
      "patience\n",
      "1\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "Averaging Weight\n",
      "[0.5711229122447171, 0.1380605964545707]\n",
      "Weight Norms\n",
      "[29.64280198036731, 3.086822079968504]\n",
      "standardized_scale_R_t\n",
      "[0.72230435 0.67147117] -0.0049014929616218225\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.52800543 45.95629185] [1. 1.] [ 45.14350889 989.24237607]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.77687184 45.56954612] [1. 1.] [ 44.43938781 986.9142677 ]\n",
      "data_scale_R_t\n",
      "[[  183.09422111   261.39257331]\n",
      " [  261.39257331 91737.69506762]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "Averaging Weight\n",
      "[0.5655631188976638, 0.13269234025419296]\n",
      "Weight Norms\n",
      "[28.605567580336793, 3.1075125033111854]\n",
      "standardized_scale_R_t\n",
      "[0.73986069 0.65256576] -0.004089268693849165\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.48034612 47.54375737] [1. 1.] [ 41.925176   928.35697951]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.71919956 46.32487295] [1. 1.] [ 41.67837516 918.59910813]\n",
      "data_scale_R_t\n",
      "[[  157.01603658   135.05926028]\n",
      " [  135.05926028 78317.72960311]]\n",
      "patience\n",
      "1\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "Averaging Weight\n",
      "[0.5773415223490624, 0.12601280148939778]\n",
      "Weight Norms\n",
      "[27.697753365975746, 2.9601766370948215]\n",
      "standardized_scale_R_t\n",
      "[0.74697629 0.6760861 ] -0.0017746502022672195\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.51301113 48.91774503] [1. 1.] [ 37.89221507 818.13264098]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.75523058 48.31114074] [1. 1.] [ 37.80439943 813.10397345]\n",
      "data_scale_R_t\n",
      "[[  126.54752751    96.80667103]\n",
      " [   96.80667103 59554.77451738]]\n",
      "patience\n",
      "2\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "Averaging Weight\n",
      "[0.5834709896993368, 0.12125562798550235]\n",
      "Weight Norms\n",
      "[26.88987891070812, 2.790201567231584]\n",
      "standardized_scale_R_t\n",
      "[0.74916003 0.67892903] -0.010667612478932808\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.25171777 48.82420146] [1. 1.] [ 34.32369227 694.61844267]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.46416679 47.77225252] [1. 1.] [ 34.06156711 688.83281515]\n",
      "data_scale_R_t\n",
      "[[  104.32547321    60.3137166 ]\n",
      " [   60.3137166  44577.55861419]]\n",
      "patience\n",
      "3\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "Averaging Weight\n",
      "[0.5974931950696853, 0.11541794988331873]\n",
      "Weight Norms\n",
      "[26.07929219371288, 2.6214735856370086]\n",
      "standardized_scale_R_t\n",
      "[0.70809457 0.71234864] 0.0013453809378316697\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.21761259 47.44740087] [1.         0.99860918] [ 29.67209771 585.39838153]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.43502572 45.48011257] [1. 1.] [ 29.33761869 580.32625068]\n",
      "data_scale_R_t\n",
      "[[7.94134652e+01 2.45999559e+01]\n",
      " [2.45999559e+01 3.25252652e+04]]\n",
      "patience\n",
      "4\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "Averaging Weight\n",
      "[0.6185237952172942, 0.10915743459398239]\n",
      "Weight Norms\n",
      "[25.241933091967724, 2.406382515722209]\n",
      "standardized_scale_R_t\n",
      "[0.72004704 0.74213214] 0.021505075929775096\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.06988528 48.60820312] [0.99860918 0.99860918] [ 25.14467833 502.72641593]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.26911185 44.52777915] [1. 1.] [ 24.96569076 499.04394325]\n",
      "data_scale_R_t\n",
      "[[   59.30555071   -43.94866728]\n",
      " [  -43.94866728 24699.06446982]]\n",
      "patience\n",
      "5\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "Averaging Weight\n",
      "[0.6276175104759835, 0.10186869616567519]\n",
      "Weight Norms\n",
      "[24.260225056795406, 2.293942939893696]\n",
      "standardized_scale_R_t\n",
      "[0.76772736 0.76122538] 0.03912962070530762\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.03773644 53.90661158] [0.99443672 0.99721836] [ 21.48755718 428.22773926]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.24657485 47.72965839] [0.99583333 0.99583333] [ 21.3238547  426.62109693]\n",
      "data_scale_R_t\n",
      "[[   42.97412746   -47.20302458]\n",
      " [  -47.20302458 19519.83985042]]\n",
      "patience\n",
      "6\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "Averaging Weight\n",
      "[0.6396803133907777, 0.09110403379851309]\n",
      "Weight Norms\n",
      "[23.26398647327512, 2.0750525149174623]\n",
      "standardized_scale_R_t\n",
      "[0.81315043 0.72116705] 0.0287036658262003\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.02075564 54.63957574] [0.98748261 0.98748261] [ 18.59748071 360.40289529]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.16577037 47.16911736] [0.99583333 0.99166667] [ 18.50556535 356.58654088]\n",
      "data_scale_R_t\n",
      "[[   33.3920086    -45.99647652]\n",
      " [  -45.99647652 14101.88222619]]\n",
      "patience\n",
      "7\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "Averaging Weight\n",
      "[0.6393244115373161, 0.0843960149141855]\n",
      "Weight Norms\n",
      "[22.231952547043893, 1.8699210727165425]\n",
      "standardized_scale_R_t\n",
      "[0.81858997 0.70796983] 0.03834720421759553\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.03414558 58.51385611] [0.97913769 0.97635605] [ 15.41418514 316.36203661]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.1549215  50.67134203] [0.975  0.9875] [ 15.30919058 313.94538501]\n",
      "data_scale_R_t\n",
      "[[   23.76533572   -39.28587146]\n",
      " [  -39.28587146 12215.54582694]]\n",
      "patience\n",
      "8\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "Averaging Weight\n",
      "[0.6385953731833258, 0.07869171227237254]\n",
      "Weight Norms\n",
      "[21.323153935693025, 1.7333314475084787]\n",
      "standardized_scale_R_t\n",
      "[0.8033949  0.68687883] 0.024411792167584684\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.10319161 57.29197605] [0.95688456 0.96105702] [ 13.06260734 275.19343899]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.21710338 48.20722552] [0.95833333 0.975     ] [ 12.93428747 274.8426304 ]\n",
      "data_scale_R_t\n",
      "[[  17.80980215  -63.10723262]\n",
      " [ -63.10723262 9751.84543559]]\n",
      "patience\n",
      "9\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "Averaging Weight\n",
      "[0.6403240504807298, 0.07076496968346556]\n",
      "Weight Norms\n",
      "[20.476815064550603, 1.6061481088549976]\n",
      "standardized_scale_R_t\n",
      "[0.84588977 0.67500972] 0.03553325423741565\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.21423544 54.73087785] [0.92350487 0.92489569] [ 11.35707966 229.78998639]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.34612833 46.80229789] [0.92916667 0.95      ] [ 11.17487348 228.02370449]\n",
      "data_scale_R_t\n",
      "[[  14.19978265  -55.61570089]\n",
      " [ -55.61570089 7333.40477515]]\n",
      "patience\n",
      "10\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "Averaging Weight\n",
      "[0.641520316835511, 0.06304423687983403]\n",
      "Weight Norms\n",
      "[19.74080001715515, 1.4676024372142122]\n",
      "standardized_scale_R_t\n",
      "[0.89390604 0.64784981] 0.03756703646402762\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.19558862 49.04643705] [0.86926287 0.88038943] [  8.65078462 189.10442426]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.30826808 44.890846  ] [0.87916667 0.9       ] [  8.47085409 187.57662345]\n",
      "data_scale_R_t\n",
      "[[   9.57400092  -41.08211477]\n",
      " [ -41.08211477 4628.49202155]]\n",
      "patience\n",
      "11\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "Averaging Weight\n",
      "[0.650765930868586, 0.05533324659575957]\n",
      "Weight Norms\n",
      "[18.887388814568236, 1.3741497843332577]\n",
      "standardized_scale_R_t\n",
      "[0.89236849 0.62192957] 0.036736756227531954\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 2.07015377 46.18613382] [0.82058414 0.81223922] [  7.12842994 147.92847825]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.16961871 43.91647577] [0.825 0.85 ] [  7.06814744 145.25837577]\n",
      "data_scale_R_t\n",
      "[[   7.02581563  -32.59755982]\n",
      " [ -32.59755982 3006.92564791]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "Averaging Weight\n",
      "[0.6586097611722086, 0.04983288752978889]\n",
      "Weight Norms\n",
      "[18.112169543944503, 1.2549336144780308]\n",
      "standardized_scale_R_t\n",
      "[0.86368022 0.62224194] 0.048090357027229455\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 1.9521488  44.16570616] [0.74269819 0.70653686] [  5.3268511  110.60592139]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.11496463 43.49346694] [0.70416667 0.74583333] [  5.29190989 109.22560234]\n",
      "data_scale_R_t\n",
      "[[   3.94007591  -27.57660748]\n",
      " [ -27.57660748 2202.3483805 ]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "Averaging Weight\n",
      "[0.6500113884851524, 0.046841237726779276]\n",
      "Weight Norms\n",
      "[17.516433922407078, 1.1724394528128768]\n",
      "standardized_scale_R_t\n",
      "[0.83354259 0.62823597] 0.05231560024041026\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 1.92189855 42.26910753] [0.63282337 0.62586926] [ 3.86069999 87.43219803]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.07176066 42.88924044] [0.61666667 0.62083333] [ 3.83201415 86.31709765]\n",
      "data_scale_R_t\n",
      "[[   3.08774362  -18.77745743]\n",
      " [ -18.77745743 1268.34161342]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "Averaging Weight\n",
      "[0.6540812189015648, 0.04180619320465038]\n",
      "Weight Norms\n",
      "[17.052770254322063, 1.0942377217434192]\n",
      "standardized_scale_R_t\n",
      "[0.80823772 0.65969151] 0.061785813180842766\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 1.89818221 41.7704977 ] [0.54937413 0.54102921] [ 3.09483243 66.01484249]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.05169862 42.35394345] [0.49583333 0.49583333] [ 3.07510237 65.83995578]\n",
      "data_scale_R_t\n",
      "[[  2.60170942 -20.42784223]\n",
      " [-20.42784223 980.16610763]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "Averaging Weight\n",
      "[0.6595911458706255, 0.0381900658697206]\n",
      "Weight Norms\n",
      "[16.676043909719628, 1.0106800419158726]\n",
      "standardized_scale_R_t\n",
      "[0.75354149 0.66748631] 0.06636743406571381\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 1.83394039 41.39792255] [0.46314325 0.44506259] [ 2.37078266 48.98000447]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.02255015 42.01503517] [0.425      0.38333333] [ 2.32923981 48.4604273 ]\n",
      "data_scale_R_t\n",
      "[[  1.86640736 -17.18748234]\n",
      " [-17.18748234 876.84857053]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "Averaging Weight\n",
      "[0.6590497730547192, 0.03523804750420356]\n",
      "Weight Norms\n",
      "[16.38774468169258, 0.9110675142712894]\n",
      "standardized_scale_R_t\n",
      "[0.74729486 0.69700853] 0.07914613326065614\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 1.80449601 41.25506924] [0.36995828 0.32823366] [ 1.82318906 35.45534171]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.01938357 42.12237467] [0.3375 0.325 ] [ 1.81003738 35.54942941]\n",
      "data_scale_R_t\n",
      "[[  1.52126181 -13.93269107]\n",
      " [-13.93269107 781.79768683]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "Averaging Weight\n",
      "[0.6578374031380212, 0.032639306059672024]\n",
      "Weight Norms\n",
      "[16.108412729877426, 0.8163073014801688]\n",
      "standardized_scale_R_t\n",
      "[0.72554129 0.68665804] 0.07856177056389295\n",
      "Train RMSEs, Coverages, and Widths\n",
      "[ 1.78702921 41.23027069] [0.29485396 0.27399166] [ 1.325274   29.22773044]\n",
      "Test RMSEs, Coverages, and Widths\n",
      "[ 2.02293247 42.1342184 ] [0.27916667 0.25833333] [ 1.30494249 29.30704437]\n",
      "data_scale_R_t\n",
      "[[  1.1938779  -10.80398002]\n",
      " [-10.80398002 760.45690464]]\n",
      "patience\n",
      "0\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "Averaging Weight\n",
      "[0.6587260853866813, 0.03014319800293864]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m rmse_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(((y_train_cur \u001b[38;5;241m-\u001b[39maveraged_targets_train)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# print(rmse_train, coverage_train, avg_width_train)\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m G_u_test, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles_feats_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrdkit_feats_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_ensembles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m catch \u001b[38;5;241m=\u001b[39m Parallel(n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)(delayed(inverse_transform)(G_u_test, i)  \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(G_u_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     50\u001b[0m G_u_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(catch)\n",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(data1, data2, initial_ensembles)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_predictions\u001b[39m(data1, data2, initial_ensembles): \n\u001b[0;32m----> 2\u001b[0m     _,_, weighted_alogp, weighted_psa, w1, w2 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_operation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_ensembles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_ens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize_ens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     weighted_alogp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(weighted_alogp,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m     weighted_psa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(weighted_psa,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 23\u001b[0m, in \u001b[0;36mforward_operation\u001b[0;34m(data1, data2, combined_ensembles, size_ens)\u001b[0m\n\u001b[1;32m     19\u001b[0m model_1 \u001b[38;5;241m=\u001b[39m softmax_weights[:,:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m model_2 \u001b[38;5;241m=\u001b[39m softmax_weights[:,\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m data1_out1, data1_stack1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_weighted_targets_with_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_ensembles1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_ens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize_ens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mann\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msamp_ann\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m data1_out2, data1_stack2 \u001b[38;5;241m=\u001b[39m get_weighted_targets_with_weights(data1, initial_ensembles2, size_ens \u001b[38;5;241m=\u001b[39m size_ens,\n\u001b[1;32m     27\u001b[0m                                                              ann \u001b[38;5;241m=\u001b[39m samp_ann, weights\u001b[38;5;241m=\u001b[39mmodel_1)\n\u001b[1;32m     29\u001b[0m data2_out1, data2_stack1 \u001b[38;5;241m=\u001b[39m get_weighted_targets_with_weights(data2, initial_ensembles3, size_ens \u001b[38;5;241m=\u001b[39m size_ens,\n\u001b[1;32m     30\u001b[0m                                                              ann \u001b[38;5;241m=\u001b[39m samp_ann, weights\u001b[38;5;241m=\u001b[39mmodel_2)\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mget_weighted_targets_with_weights\u001b[0;34m(batch_data, initial_ensembles, size_ens, ann, weights)\u001b[0m\n\u001b[1;32m      9\u001b[0m n_hidden_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights_ann_1[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mravel())\n\u001b[1;32m     11\u001b[0m hidden_weights_1 \u001b[38;5;241m=\u001b[39m initial_ensembles[:,:n_hidden_1]\u001b[38;5;241m.\u001b[39mreshape( size_ens, batch_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], h1)\n\u001b[0;32m---> 14\u001b[0m hidden_output_1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij,kjl->kil\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_weights_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m hidden_layer_bias_1 \u001b[38;5;241m=\u001b[39m initial_ensembles[:,n_hidden_1:(n_hidden_1 \u001b[38;5;241m+\u001b[39m h1)]\u001b[38;5;241m.\u001b[39mreshape(size_ens, \u001b[38;5;241m1\u001b[39m,  h1)\n\u001b[1;32m     20\u001b[0m hidden_output_1 \u001b[38;5;241m=\u001b[39m hidden_output_1 \u001b[38;5;241m+\u001b[39m hidden_layer_bias_1\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/enkf/lib/python3.10/site-packages/numpy/core/einsumfunc.py:1359\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[1;32m   1358\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m-> 1359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mc_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m valid_einsum_kwargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w1_catch = []\n",
    "w2_catch = []\n",
    "w1_sd_catch = []\n",
    "w2_sd_catch = []\n",
    "weight_norm_mean = []\n",
    "weight_norm_sd = []\n",
    "\n",
    "best_rmse = 10000\n",
    "patience = 0\n",
    "for i in range(0,10000):\n",
    "    c = np.zeros((2,2))\n",
    "    initial_ensembles = get_updated_ensemble(smiles_feats_train, rdkit_feats_train, initial_ensembles)\n",
    "    G_u_train, w1, w2 = get_predictions(smiles_feats_train, rdkit_feats_train, initial_ensembles)\n",
    "    \n",
    "    w1_catch.append(w1.mean())\n",
    "    w1_sd_catch.append(w1.std())\n",
    "    \n",
    "    w2_catch.append(w2.mean())\n",
    "    w2_sd_catch.append(w2.std())  \n",
    "    \n",
    "    print(\"Epoch \" + str(i+1))\n",
    "    \n",
    "    \n",
    "    print(\"Averaging Weight\")\n",
    "    print([w1.mean(), w1.std()])\n",
    "    # print(w2.mean(), w2.std())\n",
    "    \n",
    "    # G_u_train = get_targets_with_weights(smiles_feats_train, rdkit_feats_train, initial_ensembles, size_ens = size_ens)\n",
    "    catch = Parallel(n_jobs = 15, verbose = 0)(delayed(inverse_transform)(G_u_train, i)  for i in range(G_u_train.shape[0]))\n",
    "    G_u_train = np.array(catch)\n",
    "    \n",
    "    y_train_cur = std_targets.inverse_transform(y_train)\n",
    "    \n",
    "    li_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[0,:,:]   \n",
    "    ui_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "    \n",
    "    width_train = ui_train - li_train\n",
    "    avg_width_train = width_train.mean(0)\n",
    "    \n",
    "    ind_train = (y_train_cur >= li_train) & (y_train_cur <= ui_train)\n",
    "    coverage_train= ind_train.mean(0)\n",
    "    \n",
    "    averaged_targets_train = G_u_train.mean(0)\n",
    "    rmse_train = np.sqrt(((y_train_cur -averaged_targets_train)**2).mean(0))\n",
    "    # print(rmse_train, coverage_train, avg_width_train)\n",
    "    \n",
    "    G_u_test, _, _ = get_predictions(smiles_feats_valid, rdkit_feats_valid, initial_ensembles)\n",
    "    \n",
    "    catch = Parallel(n_jobs = 15, verbose = 0)(delayed(inverse_transform)(G_u_test, i)  for i in range(G_u_test.shape[0]))\n",
    "    G_u_test = np.array(catch)\n",
    "    \n",
    "    y_valid_cur = std_targets.inverse_transform(y_valid)    \n",
    "    \n",
    "    li_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[0,:,:]   \n",
    "    ui_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "    \n",
    "    width_test = ui_test - li_test\n",
    "    avg_width_test = width_test.mean(0)\n",
    "    \n",
    "    ind_test = (y_valid_cur >= li_test) & (y_valid_cur <= ui_test)\n",
    "    coverage_test= ind_test.mean(0)\n",
    "    \n",
    "    averaged_targets_test = G_u_test.mean(0)\n",
    "    rmse_test = np.sqrt(((y_valid_cur -averaged_targets_test)**2).mean(0))    \n",
    "    \n",
    "    weight_norms = np.array(norm(initial_ensembles, ord = 2, axis = 1))\n",
    "    weight_norm_mean.append(weight_norms.mean())\n",
    "    weight_norm_sd.append(weight_norms.std())\n",
    "    \n",
    "   \n",
    "    print(\"Weight Norms\")\n",
    "    print([weight_norms.mean(), weight_norms.std()])\n",
    "    \n",
    "    \n",
    "    # if np.mean(coverage_train < 0.95) == 1:\n",
    "    #     break\n",
    "        \n",
    "    cov_mat_final, _ = create_cov(smiles_feats_train.shape[0],initial_ensembles)\n",
    "    \n",
    "    print(\"standardized_scale_R_t\")\n",
    "    print(np.diag(cov_mat_final), cov_mat_final[0,1])\n",
    "    \n",
    "    print(\"Train RMSEs, Coverages, and Widths\")\n",
    "    print(rmse_train, coverage_train, avg_width_train)\n",
    "    \n",
    "    print(\"Test RMSEs, Coverages, and Widths\")\n",
    "    print(rmse_test, coverage_test, avg_width_test)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    value = rmse_train.mean()\n",
    "    \n",
    "    for i in range(0, size_ens):\n",
    "        c+= np.cov(G_u_train[i,:,:].T)\n",
    "        \n",
    "    c = c/size_ens    \n",
    "    \n",
    "    if value < best_rmse: \n",
    "        best_rmse = value\n",
    "        best_coverage = coverage_train\n",
    "        patience = 0\n",
    "        best_rmse_test = rmse_test\n",
    "        best_coverage_test = coverage_test\n",
    "        best_R_t = c\n",
    "        # print(['i', 'best_rmse', 'best_coverage', 'best_rmse_test', 'best_coverage_test', 'best_R_t'])\n",
    "        # print([i, best_rmse, best_coverage, best_rmse_test, best_coverage_test, best_R_t])\n",
    "    else:\n",
    "        patience +=1 \n",
    "    \n",
    "    print(\"data_scale_R_t\")\n",
    "    print(c)\n",
    "        \n",
    "    print('patience')\n",
    "    print(patience)\n",
    "    \n",
    "    print(\"\\n\")    \n",
    "        \n",
    "    if patience >= 20:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd0232-864e-422d-810b-ada022f08acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81eb6c3-e084-4e43-aa5e-3f9b0f5ee016",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_u_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8df43-9ae3-47f6-9c69-ac77c38a6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47207771-df91-48f0-9963-9a78989bcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.sample(range(y_valid_cur.shape[0]), k = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f17ea-a47c-421d-bd32-7a0dc0f1af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8, 2,figsize=(15, 15))\n",
    "# axs = axs.ravel()\n",
    "# counter = 0\n",
    "for idx, i in enumerate(random_idx):\n",
    "    # print(counter)\n",
    "    truth = y_valid_cur[i,:]\n",
    "    preds = G_u_test[:, i,:]\n",
    "    percts = np.percentile(preds, axis = 0, q = (2.5, 97.5))\n",
    "    lis = percts[0,:]\n",
    "    uis = percts[1,:]\n",
    "    \n",
    "    \n",
    "    axs[idx, 0].hist(preds[:,0])\n",
    "    axs[idx, 0].axvline(truth[0], color='green', linewidth=2)\n",
    "    axs[idx, 0].axvline(lis[0], color='red', linewidth=2)\n",
    "    axs[idx, 0].axvline(uis[0], color='red', linewidth=2)\n",
    "    \n",
    "    axs[idx, 1].hist(preds[:,1])\n",
    "    axs[idx, 1].axvline(truth[1], color='green', linewidth=2)\n",
    "    axs[idx, 1].axvline(lis[1], color='red', linewidth=2)\n",
    "    axs[idx, 1].axvline(uis[1], color='red', linewidth=2)\n",
    "    \n",
    "    # counter+=2\n",
    "    # print(counter)\n",
    "    \n",
    "    # plt.show()\n",
    "plt.savefig('prediction_intervals.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230a62b-f36f-4ec0-9c7d-0ba75c9de369",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_valid_cur[:, 0], averaged_targets_test[:,0])\n",
    "plt.axline((0,0), slope = 1, c= \"black\")\n",
    "plt.show()\n",
    "plt.scatter(y_valid_cur[:,1], averaged_targets_test[:, 1])\n",
    "plt.axline((0,0), slope = 1, c= \"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8d1f1-5b52-48eb-997c-0a760a8729a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w1_catch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa904e3-6452-4443-93ef-3c54377ff059",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w1_sd_catch)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a36715c-20fa-406e-9ac6-55a56152d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(range(0, len(w1_catch)), w1_catch, w1_sd_catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a18b9e-6388-4b45-b46b-e7ecd3ba8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(range(0, len(weight_norm_mean)), weight_norm_mean, weight_norm_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a7783b-b135-41e1-8ae7-6a346965daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat_final, _ = create_cov(smiles_feats_train.shape[0],initial_ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d891012-f14b-40fa-b8e4-8bf97bac5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd1fd9-89cb-4a66-896c-62f8805c0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(y_chk_full.values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d0666-8d71-4b66-bc92-9d03cf19d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_R_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
